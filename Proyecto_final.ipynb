{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNerl-cTZI7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408f678c-6041-4b1f-984d-9b9baddf7c14"
      },
      "source": [
        "!pip install qgrid\n",
        "import qgrid\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler # Escalamiento estÃ¡ndar\n",
        "from sklearn.model_selection import train_test_split   \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix \n",
        "import scipy as sc\n",
        "import math\n",
        "from numpy import random\n",
        "from numpy import matlib\n",
        "from scipy.spatial import distance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qgrid in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from qgrid) (1.1.5)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from qgrid) (7.6.5)\n",
            "Requirement already satisfied: notebook>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from qgrid) (5.3.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->qgrid) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->qgrid) (3.5.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->qgrid) (5.1.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->qgrid) (4.10.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->qgrid) (1.0.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->qgrid) (5.1.3)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->qgrid) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->qgrid) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->qgrid) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->qgrid) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->qgrid) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->qgrid) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->qgrid) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->qgrid) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->qgrid) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->qgrid) (57.4.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->qgrid) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->qgrid) (4.8.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.0.0->qgrid) (2.11.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.0.0->qgrid) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.0.0->qgrid) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.0.0->qgrid) (0.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.0.0->qgrid) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.0.0->qgrid) (22.3.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->qgrid) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->qgrid) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.0.0->qgrid) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.0.0->qgrid) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.0.0->qgrid) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.0.0->qgrid) (2.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (4.1.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (1.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.0.0->qgrid) (21.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.0.0->qgrid) (0.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->notebook>=4.0.0->qgrid) (2.4.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucI5aPK3Z3bp"
      },
      "source": [
        "#Eliminamos las filas nulas\n",
        "DatosConNulls = pd.read_csv(\"water_potability.csv\", names = None, sep = \",\")\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIe0lyigH0LC",
        "outputId": "01d0572f-3961-4211-caec-dee5c8fa890c"
      },
      "source": [
        "DatosConNulls.isnull().sum()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ph                 491\n",
              "Hardness             0\n",
              "Solids               0\n",
              "Chloramines          0\n",
              "Sulfate            781\n",
              "Conductivity         0\n",
              "Organic_carbon       0\n",
              "Trihalomethanes    162\n",
              "Turbidity            0\n",
              "Potability           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8Oo8fTwH6u5",
        "outputId": "62191b3a-823b-4745-c33c-cef88b2cd1e2"
      },
      "source": [
        "#completar los valores en null con su media\n",
        "df1=DatosConNulls[DatosConNulls['Potability']==1].copy()\n",
        "df2=DatosConNulls[DatosConNulls['Potability']==0].copy()\n",
        "df1['ph']=df1['ph'].replace(np.nan, df1['ph'].median())\n",
        "df2['ph']=df2['ph'].replace(np.nan, df2['ph'].median())\n",
        "df1['Sulfate']=df1['Sulfate'].replace(np.nan, df1['Sulfate'].median())\n",
        "df2['Sulfate']=df2['Sulfate'].replace(np.nan, df2['Sulfate'].median())\n",
        "df1['Trihalomethanes']=df1['Trihalomethanes'].replace(np.nan, df1['Trihalomethanes'].median())\n",
        "df2['Trihalomethanes']=df2['Trihalomethanes'].replace(np.nan, df2['Trihalomethanes'].median())\n",
        "Datos=pd.concat([df1,df2], axis=0, ignore_index=True)\n",
        "Datos.isnull().sum()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ph                 0\n",
              "Hardness           0\n",
              "Solids             0\n",
              "Chloramines        0\n",
              "Sulfate            0\n",
              "Conductivity       0\n",
              "Organic_carbon     0\n",
              "Trihalomethanes    0\n",
              "Turbidity          0\n",
              "Potability         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgnPZGdklQ-B"
      },
      "source": [
        "X = Datos.iloc[:,0:9] #Caracteristicas\n",
        "Nombres = Datos.columns\n",
        "Y = Datos['Potability']#clases\n",
        "#Ya que la diferencia entre las escalas de los datos de X, se harÃ¡ una normalizaciÃ³n de los datos \n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X_escalado = scaler.transform(X)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h41D5NQkwX91"
      },
      "source": [
        "def completarTablaRedesNeuronales (i,tablaRedesNeuronales,EV,IC,F1,ICF1):\n",
        "  tablaRedesNeuronales.loc[i,\"Eficiencia en validacion\"] = str(EV) # reemplazar los valores\n",
        "  tablaRedesNeuronales.loc[i,\"Intervalo de confianza de eficiencia\"] = str(IC)\n",
        "  tablaRedesNeuronales.loc[i,\"f1-score\"] = str(F1)\n",
        "  tablaRedesNeuronales.loc[i,\"Intervalo de confianza de f1-score\"] = str(ICF1)\n",
        "  return(tablaRedesNeuronales)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk1Upv4C22-J"
      },
      "source": [
        "def completarTablaSoporteVectorial (i,tablaSoporteVectorial,EV,IC,PV,F1,ICF1):\n",
        "  tablaSoporteVectorial.loc[i,\"Eficiencia en validacion\"] = str(EV)\n",
        "  tablaSoporteVectorial.loc[i,\"Intervalo de confianza de eficiencia\"] = str(IC)\n",
        "  tablaSoporteVectorial.loc[i, \"% de Vectores de Soporte\"] = str(PV)\n",
        "  tablaSoporteVectorial.loc[i,\"f1-score\"] = str(F1)\n",
        "  tablaSoporteVectorial.loc[i,\"Intervalo de confianza de f1-score\"] = str(ICF1)\n",
        "  return(tablaSoporteVectorial)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmzGKrDLKEi5"
      },
      "source": [
        "def completarTablaBoostingTree (pos,tablaBoostingTree,EV,IC,n_estimators,learning_rate,max_depth,F1,ICF1):\n",
        "  tablaBoostingTree.loc[pos,\"n_estimators\"]=n_estimators\n",
        "  tablaBoostingTree.loc[pos,\"learning_rate\"]=learning_rate\n",
        "  tablaBoostingTree.loc[pos,\"max_depth\"]=max_depth\n",
        "  tablaBoostingTree.loc[pos,\"Eficiencia en validacion\"] = str(EV) \n",
        "  tablaBoostingTree.loc[pos,\"Intervalo de confianza\"] = str(IC)\n",
        "  tablaBoostingTree.loc[pos,\"f1-score\"] = str(F1)\n",
        "  tablaBoostingTree.loc[pos,\"Intervalo de confianza de f1-score\"] = str(ICF1)\n",
        "  return(tablaBoostingTree)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEgXGTHw3Rt8"
      },
      "source": [
        "def completarTablaAnalisisDiscriminanteCuadratico (i,tablaAnalisisDiscriminanteCuadratico,EV,IC,F1,ICF1):\n",
        "  tablaAnalisisDiscriminanteCuadratico.loc[i,\"Eficiencia en validacion\"] = str(EV) \n",
        "  tablaAnalisisDiscriminanteCuadratico.loc[i,\"Intervalo de confianza de eficiencia\"] = str(IC)\n",
        "  tablaAnalisisDiscriminanteCuadratico.loc[i,\"f1-score\"] = str(F1)\n",
        "  tablaAnalisisDiscriminanteCuadratico.loc[i,\"Intervalo de confianza de f1-score\"] = str(ICF1)\n",
        "  return(tablaAnalisisDiscriminanteCuadratico)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTRDXtOvrC9L"
      },
      "source": [
        "def completarTablaVentanaParzen (i,tablaVentanaParzen,EV,IC,F1,ICF1):\n",
        "  tablaVentanaParzen.loc[i,\"Eficiencia en validacion\"] = str(EV) \n",
        "  tablaVentanaParzen.loc[i,\"Intervalo de confianza de eficiencia\"] = str(IC)\n",
        "  tablaVentanaParzen.loc[i,\"f1-score\"] = str(F1)\n",
        "  tablaVentanaParzen.loc[i,\"Intervalo de confianza de f1-score\"] = str(ICF1)\n",
        "  return(tablaVentanaParzen)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_55nnKY1frn"
      },
      "source": [
        "# 1. Redes neuronales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUpBxaCcmaaf",
        "outputId": "62b94961-cf63-4997-e341-9fbae8a88338"
      },
      "source": [
        " #Integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integerY = label_encoder.fit_transform(Y)\n",
        "print(integerY)\n",
        "\n",
        "#Binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integerY = integerY.reshape(len(integerY), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integerY)\n",
        "print(onehot_encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 ... 0 0 0]\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hOQwWrbuGpT",
        "outputId": "ebe7b362-1450-4ac6-bf1f-3cfdd6210500"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import scipy as sc\n",
        "import math\n",
        "from numpy import random\n",
        "from numpy import matlib\n",
        "\n",
        "\n",
        "tablaRedesNeuronales = pd.DataFrame({\n",
        "    'N. de capas ocultas' : pd.Series([1,1,1,1,1,2,2,2,2,2,5,5,5,5,5,6,6,6,6,6]),\n",
        "    'Neuronas por capa' : pd.Series([2,4,10,28,32,2,4,10,28,32,2,4,10,28,32,2,4,10,28,32])})\n",
        "\n",
        "numerocapas = [1,1,1,1,1,2,2,2,2,2,5,5,5,5,5,6,6,6,6,6]\n",
        "neuronascapa = [2,4,10,28,32,2,4,10,28,32,2,4,10,28,32,2,4,10,28,32]\n",
        "\n",
        "for i in range (20):\n",
        "\n",
        "  Folds = 4\n",
        "  random.seed(19680801)\n",
        "  EficienciaTrain = np.zeros(Folds)\n",
        "  EficienciaVal = np.zeros(Folds)\n",
        "  f1 = np.zeros(Folds)\n",
        "  skf = StratifiedKFold(n_splits=Folds)\n",
        "  j = 0\n",
        "  for train, test in skf.split(X_escalado, integerY):\n",
        "      Xtrain = X_escalado[train,:]\n",
        "      Ytrain = integerY[train]\n",
        "      Xtest = X_escalado[test,:]\n",
        "      Ytest = integerY[test]\n",
        "      Ytest=Ytest.reshape(len(Ytest))\n",
        "      Ytrain=Ytrain.reshape(len(Ytrain))\n",
        "      \n",
        "    \n",
        "      if (numerocapas[i]==1):\n",
        "        vector=(neuronascapa[i])\n",
        "      if (numerocapas[i]==2):\n",
        "        vector=(neuronascapa[i],neuronascapa[i])\n",
        "      if (numerocapas[i]==5):\n",
        "        vector=(neuronascapa[i],neuronascapa[i],neuronascapa[i],neuronascapa[i],neuronascapa[i])\n",
        "      if (numerocapas[i]==6):\n",
        "        vector=(neuronascapa[i],neuronascapa[i],neuronascapa[i],neuronascapa[i],neuronascapa[i],neuronascapa[i])  \n",
        "\n",
        "      #Haga el llamado a la funciÃ³n para crear y entrenar el modelo usando los datos de entrenamiento\n",
        "      mlp = MLPClassifier(hidden_layer_sizes=vector, activation='identity', max_iter=400,verbose=True)\n",
        "      mlp.out_activation_='softmax'\n",
        "      mlp=mlp.fit(Xtrain,Ytrain)\n",
        "\n",
        "      Ytrain_pred = mlp.predict(Xtrain)\n",
        "      Yest = mlp.predict(Xtest)\n",
        "\n",
        "      \n",
        "      #Evaluamos las predicciones del modelo con los datos de test\n",
        "      EficienciaTrain[j] = metrics.accuracy_score(Ytrain, Ytrain_pred)\n",
        "      EficienciaVal[j] = metrics.accuracy_score(Ytest, Yest)\n",
        "      f1[j]=metrics.f1_score(Ytest, Yest,average='weighted')\n",
        "      j += 1\n",
        "          \n",
        "  print('Eficiencia durante el entrenamiento = ' + str(np.mean(EficienciaTrain)) + '+-' + str(np.std(EficienciaTrain)))\n",
        "  print('Eficiencia durante la validaciÃ³n = ' + str(np.mean(EficienciaVal)) + '+-' + str(np.std(EficienciaVal)))\n",
        "  print(\"f1-score \",str(np.mean(f1)),\" +- \",np.std(f1))\n",
        "  tablaRedesNeuronales = completarTablaRedesNeuronales(i,tablaRedesNeuronales,np.mean(EficienciaVal),np.std(EficienciaVal),np.mean(f1),np.std(f1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.73947586\n",
            "Iteration 2, loss = 0.73255183\n",
            "Iteration 3, loss = 0.72630518\n",
            "Iteration 4, loss = 0.72084060\n",
            "Iteration 5, loss = 0.71577958\n",
            "Iteration 6, loss = 0.71148337\n",
            "Iteration 7, loss = 0.70738935\n",
            "Iteration 8, loss = 0.70374855\n",
            "Iteration 9, loss = 0.70049213\n",
            "Iteration 10, loss = 0.69748388\n",
            "Iteration 11, loss = 0.69489640\n",
            "Iteration 12, loss = 0.69234934\n",
            "Iteration 13, loss = 0.69010342\n",
            "Iteration 14, loss = 0.68813002\n",
            "Iteration 15, loss = 0.68630076\n",
            "Iteration 16, loss = 0.68453447\n",
            "Iteration 17, loss = 0.68294305\n",
            "Iteration 18, loss = 0.68148567\n",
            "Iteration 19, loss = 0.68019911\n",
            "Iteration 20, loss = 0.67909370\n",
            "Iteration 21, loss = 0.67792273\n",
            "Iteration 22, loss = 0.67677033\n",
            "Iteration 23, loss = 0.67583675\n",
            "Iteration 24, loss = 0.67500774\n",
            "Iteration 25, loss = 0.67415089\n",
            "Iteration 26, loss = 0.67344947\n",
            "Iteration 27, loss = 0.67275346\n",
            "Iteration 28, loss = 0.67219157\n",
            "Iteration 29, loss = 0.67172219\n",
            "Iteration 30, loss = 0.67123879\n",
            "Iteration 31, loss = 0.67081891\n",
            "Iteration 32, loss = 0.67040418\n",
            "Iteration 33, loss = 0.67006991\n",
            "Iteration 34, loss = 0.66971832\n",
            "Iteration 35, loss = 0.66937766\n",
            "Iteration 36, loss = 0.66918331\n",
            "Iteration 37, loss = 0.66892792\n",
            "Iteration 38, loss = 0.66867617\n",
            "Iteration 39, loss = 0.66847491\n",
            "Iteration 40, loss = 0.66829770\n",
            "Iteration 41, loss = 0.66810348\n",
            "Iteration 42, loss = 0.66793240\n",
            "Iteration 43, loss = 0.66780864\n",
            "Iteration 44, loss = 0.66767908\n",
            "Iteration 45, loss = 0.66758795\n",
            "Iteration 46, loss = 0.66755470\n",
            "Iteration 47, loss = 0.66746804\n",
            "Iteration 48, loss = 0.66744036\n",
            "Iteration 49, loss = 0.66739974\n",
            "Iteration 50, loss = 0.66735482\n",
            "Iteration 51, loss = 0.66734569\n",
            "Iteration 52, loss = 0.66725573\n",
            "Iteration 53, loss = 0.66721791\n",
            "Iteration 54, loss = 0.66721446\n",
            "Iteration 55, loss = 0.66715360\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.91031114\n",
            "Iteration 2, loss = 0.89664947\n",
            "Iteration 3, loss = 0.88349129\n",
            "Iteration 4, loss = 0.87132486\n",
            "Iteration 5, loss = 0.85968473\n",
            "Iteration 6, loss = 0.84901810\n",
            "Iteration 7, loss = 0.83879045\n",
            "Iteration 8, loss = 0.82939419\n",
            "Iteration 9, loss = 0.82007879\n",
            "Iteration 10, loss = 0.81164150\n",
            "Iteration 11, loss = 0.80363107\n",
            "Iteration 12, loss = 0.79597182\n",
            "Iteration 13, loss = 0.78867540\n",
            "Iteration 14, loss = 0.78192090\n",
            "Iteration 15, loss = 0.77541390\n",
            "Iteration 16, loss = 0.76933990\n",
            "Iteration 17, loss = 0.76347444\n",
            "Iteration 18, loss = 0.75811078\n",
            "Iteration 19, loss = 0.75278370\n",
            "Iteration 20, loss = 0.74783075\n",
            "Iteration 21, loss = 0.74330214\n",
            "Iteration 22, loss = 0.73898903\n",
            "Iteration 23, loss = 0.73478271\n",
            "Iteration 24, loss = 0.73082088\n",
            "Iteration 25, loss = 0.72710995\n",
            "Iteration 26, loss = 0.72336557\n",
            "Iteration 27, loss = 0.72000702\n",
            "Iteration 28, loss = 0.71678924\n",
            "Iteration 29, loss = 0.71383147\n",
            "Iteration 30, loss = 0.71096497\n",
            "Iteration 31, loss = 0.70835583\n",
            "Iteration 32, loss = 0.70585966\n",
            "Iteration 33, loss = 0.70350111\n",
            "Iteration 34, loss = 0.70131233\n",
            "Iteration 35, loss = 0.69925237\n",
            "Iteration 36, loss = 0.69711470\n",
            "Iteration 37, loss = 0.69522683\n",
            "Iteration 38, loss = 0.69342300\n",
            "Iteration 39, loss = 0.69174705\n",
            "Iteration 40, loss = 0.69020809\n",
            "Iteration 41, loss = 0.68868859\n",
            "Iteration 42, loss = 0.68722284\n",
            "Iteration 43, loss = 0.68591047\n",
            "Iteration 44, loss = 0.68472985\n",
            "Iteration 45, loss = 0.68357466\n",
            "Iteration 46, loss = 0.68249678\n",
            "Iteration 47, loss = 0.68137457\n",
            "Iteration 48, loss = 0.68046791\n",
            "Iteration 49, loss = 0.67964608\n",
            "Iteration 50, loss = 0.67878152\n",
            "Iteration 51, loss = 0.67802203\n",
            "Iteration 52, loss = 0.67730717\n",
            "Iteration 53, loss = 0.67657598\n",
            "Iteration 54, loss = 0.67601840\n",
            "Iteration 55, loss = 0.67538552\n",
            "Iteration 56, loss = 0.67483977\n",
            "Iteration 57, loss = 0.67429023\n",
            "Iteration 58, loss = 0.67377102\n",
            "Iteration 59, loss = 0.67339390\n",
            "Iteration 60, loss = 0.67296621\n",
            "Iteration 61, loss = 0.67259200\n",
            "Iteration 62, loss = 0.67223445\n",
            "Iteration 63, loss = 0.67191206\n",
            "Iteration 64, loss = 0.67158825\n",
            "Iteration 65, loss = 0.67131379\n",
            "Iteration 66, loss = 0.67099655\n",
            "Iteration 67, loss = 0.67077292\n",
            "Iteration 68, loss = 0.67055266\n",
            "Iteration 69, loss = 0.67028239\n",
            "Iteration 70, loss = 0.67004152\n",
            "Iteration 71, loss = 0.66984141\n",
            "Iteration 72, loss = 0.66968225\n",
            "Iteration 73, loss = 0.66945800\n",
            "Iteration 74, loss = 0.66935258\n",
            "Iteration 75, loss = 0.66914169\n",
            "Iteration 76, loss = 0.66901426\n",
            "Iteration 77, loss = 0.66888756\n",
            "Iteration 78, loss = 0.66875967\n",
            "Iteration 79, loss = 0.66861027\n",
            "Iteration 80, loss = 0.66853301\n",
            "Iteration 81, loss = 0.66838544\n",
            "Iteration 82, loss = 0.66832715\n",
            "Iteration 83, loss = 0.66823528\n",
            "Iteration 84, loss = 0.66816074\n",
            "Iteration 85, loss = 0.66806004\n",
            "Iteration 86, loss = 0.66797297\n",
            "Iteration 87, loss = 0.66791580\n",
            "Iteration 88, loss = 0.66786730\n",
            "Iteration 89, loss = 0.66779530\n",
            "Iteration 90, loss = 0.66778259\n",
            "Iteration 91, loss = 0.66770917\n",
            "Iteration 92, loss = 0.66767393\n",
            "Iteration 93, loss = 0.66762847\n",
            "Iteration 94, loss = 0.66759085\n",
            "Iteration 95, loss = 0.66754499\n",
            "Iteration 96, loss = 0.66756010\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.97124439\n",
            "Iteration 2, loss = 0.95042458\n",
            "Iteration 3, loss = 0.93137089\n",
            "Iteration 4, loss = 0.91293026\n",
            "Iteration 5, loss = 0.89644890\n",
            "Iteration 6, loss = 0.88051319\n",
            "Iteration 7, loss = 0.86597902\n",
            "Iteration 8, loss = 0.85172975\n",
            "Iteration 9, loss = 0.83834287\n",
            "Iteration 10, loss = 0.82517315\n",
            "Iteration 11, loss = 0.81268362\n",
            "Iteration 12, loss = 0.80009591\n",
            "Iteration 13, loss = 0.78840780\n",
            "Iteration 14, loss = 0.77715808\n",
            "Iteration 15, loss = 0.76678429\n",
            "Iteration 16, loss = 0.75647513\n",
            "Iteration 17, loss = 0.74675052\n",
            "Iteration 18, loss = 0.73783745\n",
            "Iteration 19, loss = 0.72925795\n",
            "Iteration 20, loss = 0.72134561\n",
            "Iteration 21, loss = 0.71407706\n",
            "Iteration 22, loss = 0.70761452\n",
            "Iteration 23, loss = 0.70185719\n",
            "Iteration 24, loss = 0.69680049\n",
            "Iteration 25, loss = 0.69215432\n",
            "Iteration 26, loss = 0.68836046\n",
            "Iteration 27, loss = 0.68484139\n",
            "Iteration 28, loss = 0.68201563\n",
            "Iteration 29, loss = 0.67951393\n",
            "Iteration 30, loss = 0.67734353\n",
            "Iteration 31, loss = 0.67560829\n",
            "Iteration 32, loss = 0.67409301\n",
            "Iteration 33, loss = 0.67276325\n",
            "Iteration 34, loss = 0.67165488\n",
            "Iteration 35, loss = 0.67069260\n",
            "Iteration 36, loss = 0.66995821\n",
            "Iteration 37, loss = 0.66931349\n",
            "Iteration 38, loss = 0.66873727\n",
            "Iteration 39, loss = 0.66829746\n",
            "Iteration 40, loss = 0.66788187\n",
            "Iteration 41, loss = 0.66751773\n",
            "Iteration 42, loss = 0.66718966\n",
            "Iteration 43, loss = 0.66695489\n",
            "Iteration 44, loss = 0.66678845\n",
            "Iteration 45, loss = 0.66656159\n",
            "Iteration 46, loss = 0.66639022\n",
            "Iteration 47, loss = 0.66628311\n",
            "Iteration 48, loss = 0.66610638\n",
            "Iteration 49, loss = 0.66602823\n",
            "Iteration 50, loss = 0.66591627\n",
            "Iteration 51, loss = 0.66587081\n",
            "Iteration 52, loss = 0.66582892\n",
            "Iteration 53, loss = 0.66576779\n",
            "Iteration 54, loss = 0.66572850\n",
            "Iteration 55, loss = 0.66567160\n",
            "Iteration 56, loss = 0.66566547\n",
            "Iteration 57, loss = 0.66561886\n",
            "Iteration 58, loss = 0.66560432\n",
            "Iteration 59, loss = 0.66556375\n",
            "Iteration 60, loss = 0.66552428\n",
            "Iteration 61, loss = 0.66551761\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.97287258\n",
            "Iteration 2, loss = 0.95380045\n",
            "Iteration 3, loss = 0.93530106\n",
            "Iteration 4, loss = 0.91906746\n",
            "Iteration 5, loss = 0.90335936\n",
            "Iteration 6, loss = 0.88908992\n",
            "Iteration 7, loss = 0.87578037\n",
            "Iteration 8, loss = 0.86295819\n",
            "Iteration 9, loss = 0.85149404\n",
            "Iteration 10, loss = 0.84034344\n",
            "Iteration 11, loss = 0.83027442\n",
            "Iteration 12, loss = 0.82063277\n",
            "Iteration 13, loss = 0.81178096\n",
            "Iteration 14, loss = 0.80349316\n",
            "Iteration 15, loss = 0.79564282\n",
            "Iteration 16, loss = 0.78822889\n",
            "Iteration 17, loss = 0.78136122\n",
            "Iteration 18, loss = 0.77491719\n",
            "Iteration 19, loss = 0.76862532\n",
            "Iteration 20, loss = 0.76282042\n",
            "Iteration 21, loss = 0.75746989\n",
            "Iteration 22, loss = 0.75229111\n",
            "Iteration 23, loss = 0.74770063\n",
            "Iteration 24, loss = 0.74329058\n",
            "Iteration 25, loss = 0.73910309\n",
            "Iteration 26, loss = 0.73503497\n",
            "Iteration 27, loss = 0.73125771\n",
            "Iteration 28, loss = 0.72782673\n",
            "Iteration 29, loss = 0.72430210\n",
            "Iteration 30, loss = 0.72107954\n",
            "Iteration 31, loss = 0.71803413\n",
            "Iteration 32, loss = 0.71527069\n",
            "Iteration 33, loss = 0.71250226\n",
            "Iteration 34, loss = 0.70990460\n",
            "Iteration 35, loss = 0.70757507\n",
            "Iteration 36, loss = 0.70527552\n",
            "Iteration 37, loss = 0.70306287\n",
            "Iteration 38, loss = 0.70110177\n",
            "Iteration 39, loss = 0.69912753\n",
            "Iteration 40, loss = 0.69731258\n",
            "Iteration 41, loss = 0.69563290\n",
            "Iteration 42, loss = 0.69390924\n",
            "Iteration 43, loss = 0.69237375\n",
            "Iteration 44, loss = 0.69089084\n",
            "Iteration 45, loss = 0.68943724\n",
            "Iteration 46, loss = 0.68814054\n",
            "Iteration 47, loss = 0.68689609\n",
            "Iteration 48, loss = 0.68571783\n",
            "Iteration 49, loss = 0.68450241\n",
            "Iteration 50, loss = 0.68341617\n",
            "Iteration 51, loss = 0.68242609\n",
            "Iteration 52, loss = 0.68135435\n",
            "Iteration 53, loss = 0.68047032\n",
            "Iteration 54, loss = 0.67961620\n",
            "Iteration 55, loss = 0.67879644\n",
            "Iteration 56, loss = 0.67800884\n",
            "Iteration 57, loss = 0.67719570\n",
            "Iteration 58, loss = 0.67645591\n",
            "Iteration 59, loss = 0.67579250\n",
            "Iteration 60, loss = 0.67521918\n",
            "Iteration 61, loss = 0.67461150\n",
            "Iteration 62, loss = 0.67403311\n",
            "Iteration 63, loss = 0.67359103\n",
            "Iteration 64, loss = 0.67311542\n",
            "Iteration 65, loss = 0.67270640\n",
            "Iteration 66, loss = 0.67223797\n",
            "Iteration 67, loss = 0.67179931\n",
            "Iteration 68, loss = 0.67142279\n",
            "Iteration 69, loss = 0.67108128\n",
            "Iteration 70, loss = 0.67077997\n",
            "Iteration 71, loss = 0.67046851\n",
            "Iteration 72, loss = 0.67019393\n",
            "Iteration 73, loss = 0.66988349\n",
            "Iteration 74, loss = 0.66961740\n",
            "Iteration 75, loss = 0.66940710\n",
            "Iteration 76, loss = 0.66917582\n",
            "Iteration 77, loss = 0.66895942\n",
            "Iteration 78, loss = 0.66874780\n",
            "Iteration 79, loss = 0.66860555\n",
            "Iteration 80, loss = 0.66837293\n",
            "Iteration 81, loss = 0.66823095\n",
            "Iteration 82, loss = 0.66808969\n",
            "Iteration 83, loss = 0.66793198\n",
            "Iteration 84, loss = 0.66781237\n",
            "Iteration 85, loss = 0.66770020\n",
            "Iteration 86, loss = 0.66759917\n",
            "Iteration 87, loss = 0.66749571\n",
            "Iteration 88, loss = 0.66743879\n",
            "Iteration 89, loss = 0.66733700\n",
            "Iteration 90, loss = 0.66727864\n",
            "Iteration 91, loss = 0.66722596\n",
            "Iteration 92, loss = 0.66716152\n",
            "Iteration 93, loss = 0.66709868\n",
            "Iteration 94, loss = 0.66703221\n",
            "Iteration 95, loss = 0.66701735\n",
            "Iteration 96, loss = 0.66694504\n",
            "Iteration 97, loss = 0.66689765\n",
            "Iteration 98, loss = 0.66687251\n",
            "Iteration 99, loss = 0.66684101\n",
            "Iteration 100, loss = 0.66680843\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6125356125356125+-0.0013802055317715162\n",
            "Eficiencia durante la validaciÃ³n = 0.6111111111111112+-0.002024801459313474\n",
            "f1-score  0.4648758256351011  +-  0.004030014347053157\n",
            "Iteration 1, loss = 0.91228021\n",
            "Iteration 2, loss = 0.88386718\n",
            "Iteration 3, loss = 0.85834737\n",
            "Iteration 4, loss = 0.83513159\n",
            "Iteration 5, loss = 0.81487360\n",
            "Iteration 6, loss = 0.79691556\n",
            "Iteration 7, loss = 0.78056027\n",
            "Iteration 8, loss = 0.76662908\n",
            "Iteration 9, loss = 0.75418430\n",
            "Iteration 10, loss = 0.74323698\n",
            "Iteration 11, loss = 0.73291261\n",
            "Iteration 12, loss = 0.72428746\n",
            "Iteration 13, loss = 0.71605685\n",
            "Iteration 14, loss = 0.70913126\n",
            "Iteration 15, loss = 0.70291461\n",
            "Iteration 16, loss = 0.69769893\n",
            "Iteration 17, loss = 0.69330210\n",
            "Iteration 18, loss = 0.68883887\n",
            "Iteration 19, loss = 0.68505623\n",
            "Iteration 20, loss = 0.68217568\n",
            "Iteration 21, loss = 0.67945902\n",
            "Iteration 22, loss = 0.67739577\n",
            "Iteration 23, loss = 0.67555278\n",
            "Iteration 24, loss = 0.67397378\n",
            "Iteration 25, loss = 0.67279308\n",
            "Iteration 26, loss = 0.67150657\n",
            "Iteration 27, loss = 0.67066406\n",
            "Iteration 28, loss = 0.67008750\n",
            "Iteration 29, loss = 0.66931189\n",
            "Iteration 30, loss = 0.66874496\n",
            "Iteration 31, loss = 0.66840278\n",
            "Iteration 32, loss = 0.66822526\n",
            "Iteration 33, loss = 0.66791013\n",
            "Iteration 34, loss = 0.66776757\n",
            "Iteration 35, loss = 0.66755181\n",
            "Iteration 36, loss = 0.66767491\n",
            "Iteration 37, loss = 0.66743828\n",
            "Iteration 38, loss = 0.66739148\n",
            "Iteration 39, loss = 0.66741701\n",
            "Iteration 40, loss = 0.66724025\n",
            "Iteration 41, loss = 0.66721207\n",
            "Iteration 42, loss = 0.66736022\n",
            "Iteration 43, loss = 0.66717175\n",
            "Iteration 44, loss = 0.66714236\n",
            "Iteration 45, loss = 0.66718121\n",
            "Iteration 46, loss = 0.66712800\n",
            "Iteration 47, loss = 0.66709089\n",
            "Iteration 48, loss = 0.66717527\n",
            "Iteration 49, loss = 0.66709747\n",
            "Iteration 50, loss = 0.66710235\n",
            "Iteration 51, loss = 0.66712987\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.74091215\n",
            "Iteration 2, loss = 0.73008801\n",
            "Iteration 3, loss = 0.72146607\n",
            "Iteration 4, loss = 0.71376487\n",
            "Iteration 5, loss = 0.70698169\n",
            "Iteration 6, loss = 0.70150855\n",
            "Iteration 7, loss = 0.69679143\n",
            "Iteration 8, loss = 0.69286011\n",
            "Iteration 9, loss = 0.68938093\n",
            "Iteration 10, loss = 0.68627075\n",
            "Iteration 11, loss = 0.68380759\n",
            "Iteration 12, loss = 0.68157062\n",
            "Iteration 13, loss = 0.67968129\n",
            "Iteration 14, loss = 0.67809302\n",
            "Iteration 15, loss = 0.67659707\n",
            "Iteration 16, loss = 0.67551812\n",
            "Iteration 17, loss = 0.67444343\n",
            "Iteration 18, loss = 0.67345893\n",
            "Iteration 19, loss = 0.67261968\n",
            "Iteration 20, loss = 0.67179664\n",
            "Iteration 21, loss = 0.67124097\n",
            "Iteration 22, loss = 0.67068386\n",
            "Iteration 23, loss = 0.67022883\n",
            "Iteration 24, loss = 0.66986161\n",
            "Iteration 25, loss = 0.66947974\n",
            "Iteration 26, loss = 0.66920814\n",
            "Iteration 27, loss = 0.66889806\n",
            "Iteration 28, loss = 0.66867506\n",
            "Iteration 29, loss = 0.66841359\n",
            "Iteration 30, loss = 0.66823765\n",
            "Iteration 31, loss = 0.66810430\n",
            "Iteration 32, loss = 0.66799337\n",
            "Iteration 33, loss = 0.66785922\n",
            "Iteration 34, loss = 0.66773044\n",
            "Iteration 35, loss = 0.66762434\n",
            "Iteration 36, loss = 0.66758093\n",
            "Iteration 37, loss = 0.66748517\n",
            "Iteration 38, loss = 0.66742987\n",
            "Iteration 39, loss = 0.66740281\n",
            "Iteration 40, loss = 0.66738114\n",
            "Iteration 41, loss = 0.66729511\n",
            "Iteration 42, loss = 0.66725813\n",
            "Iteration 43, loss = 0.66716015\n",
            "Iteration 44, loss = 0.66718534\n",
            "Iteration 45, loss = 0.66714426\n",
            "Iteration 46, loss = 0.66723493\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.95649851\n",
            "Iteration 2, loss = 0.93222160\n",
            "Iteration 3, loss = 0.91058396\n",
            "Iteration 4, loss = 0.89039173\n",
            "Iteration 5, loss = 0.87271561\n",
            "Iteration 6, loss = 0.85638661\n",
            "Iteration 7, loss = 0.84138886\n",
            "Iteration 8, loss = 0.82812484\n",
            "Iteration 9, loss = 0.81592768\n",
            "Iteration 10, loss = 0.80450524\n",
            "Iteration 11, loss = 0.79424557\n",
            "Iteration 12, loss = 0.78454872\n",
            "Iteration 13, loss = 0.77591383\n",
            "Iteration 14, loss = 0.76775788\n",
            "Iteration 15, loss = 0.76012104\n",
            "Iteration 16, loss = 0.75301163\n",
            "Iteration 17, loss = 0.74630252\n",
            "Iteration 18, loss = 0.74024977\n",
            "Iteration 19, loss = 0.73449070\n",
            "Iteration 20, loss = 0.72920572\n",
            "Iteration 21, loss = 0.72411946\n",
            "Iteration 22, loss = 0.71948393\n",
            "Iteration 23, loss = 0.71489928\n",
            "Iteration 24, loss = 0.71056139\n",
            "Iteration 25, loss = 0.70677037\n",
            "Iteration 26, loss = 0.70295540\n",
            "Iteration 27, loss = 0.69946677\n",
            "Iteration 28, loss = 0.69609231\n",
            "Iteration 29, loss = 0.69319451\n",
            "Iteration 30, loss = 0.69053176\n",
            "Iteration 31, loss = 0.68794326\n",
            "Iteration 32, loss = 0.68565182\n",
            "Iteration 33, loss = 0.68337892\n",
            "Iteration 34, loss = 0.68139646\n",
            "Iteration 35, loss = 0.67950260\n",
            "Iteration 36, loss = 0.67784832\n",
            "Iteration 37, loss = 0.67636406\n",
            "Iteration 38, loss = 0.67495394\n",
            "Iteration 39, loss = 0.67394929\n",
            "Iteration 40, loss = 0.67276868\n",
            "Iteration 41, loss = 0.67190059\n",
            "Iteration 42, loss = 0.67096519\n",
            "Iteration 43, loss = 0.67022926\n",
            "Iteration 44, loss = 0.66956194\n",
            "Iteration 45, loss = 0.66894517\n",
            "Iteration 46, loss = 0.66840432\n",
            "Iteration 47, loss = 0.66804070\n",
            "Iteration 48, loss = 0.66753947\n",
            "Iteration 49, loss = 0.66731191\n",
            "Iteration 50, loss = 0.66693487\n",
            "Iteration 51, loss = 0.66672415\n",
            "Iteration 52, loss = 0.66645412\n",
            "Iteration 53, loss = 0.66634444\n",
            "Iteration 54, loss = 0.66609544\n",
            "Iteration 55, loss = 0.66599921\n",
            "Iteration 56, loss = 0.66588584\n",
            "Iteration 57, loss = 0.66581364\n",
            "Iteration 58, loss = 0.66573079\n",
            "Iteration 59, loss = 0.66571951\n",
            "Iteration 60, loss = 0.66564987\n",
            "Iteration 61, loss = 0.66558178\n",
            "Iteration 62, loss = 0.66553936\n",
            "Iteration 63, loss = 0.66553943\n",
            "Iteration 64, loss = 0.66549379\n",
            "Iteration 65, loss = 0.66548418\n",
            "Iteration 66, loss = 0.66545626\n",
            "Iteration 67, loss = 0.66546811\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.26778125\n",
            "Iteration 2, loss = 1.22409656\n",
            "Iteration 3, loss = 1.18352697\n",
            "Iteration 4, loss = 1.14463740\n",
            "Iteration 5, loss = 1.10819900\n",
            "Iteration 6, loss = 1.07358075\n",
            "Iteration 7, loss = 1.04111930\n",
            "Iteration 8, loss = 1.01023807\n",
            "Iteration 9, loss = 0.98028112\n",
            "Iteration 10, loss = 0.95280759\n",
            "Iteration 11, loss = 0.92697118\n",
            "Iteration 12, loss = 0.90252422\n",
            "Iteration 13, loss = 0.88025447\n",
            "Iteration 14, loss = 0.85927346\n",
            "Iteration 15, loss = 0.83883729\n",
            "Iteration 16, loss = 0.82036393\n",
            "Iteration 17, loss = 0.80326137\n",
            "Iteration 18, loss = 0.78749414\n",
            "Iteration 19, loss = 0.77282159\n",
            "Iteration 20, loss = 0.75955657\n",
            "Iteration 21, loss = 0.74736919\n",
            "Iteration 22, loss = 0.73679456\n",
            "Iteration 23, loss = 0.72675764\n",
            "Iteration 24, loss = 0.71775617\n",
            "Iteration 25, loss = 0.70994882\n",
            "Iteration 26, loss = 0.70253906\n",
            "Iteration 27, loss = 0.69680915\n",
            "Iteration 28, loss = 0.69160939\n",
            "Iteration 29, loss = 0.68707446\n",
            "Iteration 30, loss = 0.68347050\n",
            "Iteration 31, loss = 0.68043280\n",
            "Iteration 32, loss = 0.67788062\n",
            "Iteration 33, loss = 0.67559840\n",
            "Iteration 34, loss = 0.67355734\n",
            "Iteration 35, loss = 0.67215215\n",
            "Iteration 36, loss = 0.67086947\n",
            "Iteration 37, loss = 0.66988677\n",
            "Iteration 38, loss = 0.66909814\n",
            "Iteration 39, loss = 0.66847429\n",
            "Iteration 40, loss = 0.66811888\n",
            "Iteration 41, loss = 0.66760815\n",
            "Iteration 42, loss = 0.66739368\n",
            "Iteration 43, loss = 0.66710502\n",
            "Iteration 44, loss = 0.66690471\n",
            "Iteration 45, loss = 0.66685655\n",
            "Iteration 46, loss = 0.66675923\n",
            "Iteration 47, loss = 0.66664103\n",
            "Iteration 48, loss = 0.66663535\n",
            "Iteration 49, loss = 0.66660242\n",
            "Iteration 50, loss = 0.66654168\n",
            "Iteration 51, loss = 0.66654500\n",
            "Iteration 52, loss = 0.66647221\n",
            "Iteration 53, loss = 0.66647557\n",
            "Iteration 54, loss = 0.66653058\n",
            "Iteration 55, loss = 0.66650071\n",
            "Iteration 56, loss = 0.66653278\n",
            "Iteration 57, loss = 0.66647021\n",
            "Iteration 58, loss = 0.66647639\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6127391127391127+-0.002533557101747797\n",
            "Eficiencia durante la validaciÃ³n = 0.6117216117216118+-0.0025901347296210966\n",
            "f1-score  0.46679362571414984  +-  0.0055689576547940794\n",
            "Iteration 1, loss = 0.78132062\n",
            "Iteration 2, loss = 0.74710621\n",
            "Iteration 3, loss = 0.72341589\n",
            "Iteration 4, loss = 0.70391557\n",
            "Iteration 5, loss = 0.69193787\n",
            "Iteration 6, loss = 0.68341094\n",
            "Iteration 7, loss = 0.67751628\n",
            "Iteration 8, loss = 0.67366917\n",
            "Iteration 9, loss = 0.67129804\n",
            "Iteration 10, loss = 0.66946943\n",
            "Iteration 11, loss = 0.66864764\n",
            "Iteration 12, loss = 0.66830396\n",
            "Iteration 13, loss = 0.66766425\n",
            "Iteration 14, loss = 0.66745880\n",
            "Iteration 15, loss = 0.66734105\n",
            "Iteration 16, loss = 0.66730921\n",
            "Iteration 17, loss = 0.66727290\n",
            "Iteration 18, loss = 0.66723372\n",
            "Iteration 19, loss = 0.66714067\n",
            "Iteration 20, loss = 0.66723326\n",
            "Iteration 21, loss = 0.66730663\n",
            "Iteration 22, loss = 0.66722173\n",
            "Iteration 23, loss = 0.66727683\n",
            "Iteration 24, loss = 0.66732947\n",
            "Iteration 25, loss = 0.66723181\n",
            "Iteration 26, loss = 0.66735412\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80020936\n",
            "Iteration 2, loss = 0.76267546\n",
            "Iteration 3, loss = 0.73380420\n",
            "Iteration 4, loss = 0.71296740\n",
            "Iteration 5, loss = 0.69734699\n",
            "Iteration 6, loss = 0.68687929\n",
            "Iteration 7, loss = 0.67966701\n",
            "Iteration 8, loss = 0.67469478\n",
            "Iteration 9, loss = 0.67168448\n",
            "Iteration 10, loss = 0.66978259\n",
            "Iteration 11, loss = 0.66845108\n",
            "Iteration 12, loss = 0.66811917\n",
            "Iteration 13, loss = 0.66760416\n",
            "Iteration 14, loss = 0.66751896\n",
            "Iteration 15, loss = 0.66726876\n",
            "Iteration 16, loss = 0.66733955\n",
            "Iteration 17, loss = 0.66712858\n",
            "Iteration 18, loss = 0.66714502\n",
            "Iteration 19, loss = 0.66741836\n",
            "Iteration 20, loss = 0.66715825\n",
            "Iteration 21, loss = 0.66724085\n",
            "Iteration 22, loss = 0.66716806\n",
            "Iteration 23, loss = 0.66731886\n",
            "Iteration 24, loss = 0.66724827\n",
            "Iteration 25, loss = 0.66731706\n",
            "Iteration 26, loss = 0.66721806\n",
            "Iteration 27, loss = 0.66726939\n",
            "Iteration 28, loss = 0.66723886\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.83945949\n",
            "Iteration 2, loss = 0.80946330\n",
            "Iteration 3, loss = 0.78489056\n",
            "Iteration 4, loss = 0.76390143\n",
            "Iteration 5, loss = 0.74710704\n",
            "Iteration 6, loss = 0.73278829\n",
            "Iteration 7, loss = 0.72048662\n",
            "Iteration 8, loss = 0.71041937\n",
            "Iteration 9, loss = 0.70223488\n",
            "Iteration 10, loss = 0.69531170\n",
            "Iteration 11, loss = 0.68984652\n",
            "Iteration 12, loss = 0.68511832\n",
            "Iteration 13, loss = 0.68152649\n",
            "Iteration 14, loss = 0.67845485\n",
            "Iteration 15, loss = 0.67589514\n",
            "Iteration 16, loss = 0.67380932\n",
            "Iteration 17, loss = 0.67203270\n",
            "Iteration 18, loss = 0.67068547\n",
            "Iteration 19, loss = 0.66939478\n",
            "Iteration 20, loss = 0.66859102\n",
            "Iteration 21, loss = 0.66800540\n",
            "Iteration 22, loss = 0.66746312\n",
            "Iteration 23, loss = 0.66708417\n",
            "Iteration 24, loss = 0.66662888\n",
            "Iteration 25, loss = 0.66645003\n",
            "Iteration 26, loss = 0.66621829\n",
            "Iteration 27, loss = 0.66611470\n",
            "Iteration 28, loss = 0.66588857\n",
            "Iteration 29, loss = 0.66576636\n",
            "Iteration 30, loss = 0.66564903\n",
            "Iteration 31, loss = 0.66562518\n",
            "Iteration 32, loss = 0.66555703\n",
            "Iteration 33, loss = 0.66553834\n",
            "Iteration 34, loss = 0.66554911\n",
            "Iteration 35, loss = 0.66553935\n",
            "Iteration 36, loss = 0.66540998\n",
            "Iteration 37, loss = 0.66556181\n",
            "Iteration 38, loss = 0.66555242\n",
            "Iteration 39, loss = 0.66563825\n",
            "Iteration 40, loss = 0.66546770\n",
            "Iteration 41, loss = 0.66555930\n",
            "Iteration 42, loss = 0.66545332\n",
            "Iteration 43, loss = 0.66555028\n",
            "Iteration 44, loss = 0.66557867\n",
            "Iteration 45, loss = 0.66567116\n",
            "Iteration 46, loss = 0.66552389\n",
            "Iteration 47, loss = 0.66559874\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.03143903\n",
            "Iteration 2, loss = 0.96736504\n",
            "Iteration 3, loss = 0.91054225\n",
            "Iteration 4, loss = 0.86169679\n",
            "Iteration 5, loss = 0.81945918\n",
            "Iteration 6, loss = 0.78564014\n",
            "Iteration 7, loss = 0.75695937\n",
            "Iteration 8, loss = 0.73398766\n",
            "Iteration 9, loss = 0.71567995\n",
            "Iteration 10, loss = 0.70221647\n",
            "Iteration 11, loss = 0.69181653\n",
            "Iteration 12, loss = 0.68415120\n",
            "Iteration 13, loss = 0.67854805\n",
            "Iteration 14, loss = 0.67506478\n",
            "Iteration 15, loss = 0.67206799\n",
            "Iteration 16, loss = 0.67029287\n",
            "Iteration 17, loss = 0.66921362\n",
            "Iteration 18, loss = 0.66827920\n",
            "Iteration 19, loss = 0.66785517\n",
            "Iteration 20, loss = 0.66730997\n",
            "Iteration 21, loss = 0.66716214\n",
            "Iteration 22, loss = 0.66690772\n",
            "Iteration 23, loss = 0.66684716\n",
            "Iteration 24, loss = 0.66665960\n",
            "Iteration 25, loss = 0.66674741\n",
            "Iteration 26, loss = 0.66673410\n",
            "Iteration 27, loss = 0.66654259\n",
            "Iteration 28, loss = 0.66661252\n",
            "Iteration 29, loss = 0.66677533\n",
            "Iteration 30, loss = 0.66678682\n",
            "Iteration 31, loss = 0.66667252\n",
            "Iteration 32, loss = 0.66676996\n",
            "Iteration 33, loss = 0.66665651\n",
            "Iteration 34, loss = 0.66663208\n",
            "Iteration 35, loss = 0.66663095\n",
            "Iteration 36, loss = 0.66678091\n",
            "Iteration 37, loss = 0.66662283\n",
            "Iteration 38, loss = 0.66653171\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6132478632478633+-0.0027510186883589393\n",
            "Eficiencia durante la validaciÃ³n = 0.6117216117216118+-0.0025901347296210966\n",
            "f1-score  0.46679362571414984  +-  0.0055689576547940794\n",
            "Iteration 1, loss = 0.75975714\n",
            "Iteration 2, loss = 0.71041309\n",
            "Iteration 3, loss = 0.68404725\n",
            "Iteration 4, loss = 0.67342162\n",
            "Iteration 5, loss = 0.66905168\n",
            "Iteration 6, loss = 0.66797943\n",
            "Iteration 7, loss = 0.66762733\n",
            "Iteration 8, loss = 0.66747663\n",
            "Iteration 9, loss = 0.66741579\n",
            "Iteration 10, loss = 0.66737375\n",
            "Iteration 11, loss = 0.66709934\n",
            "Iteration 12, loss = 0.66756668\n",
            "Iteration 13, loss = 0.66732360\n",
            "Iteration 14, loss = 0.66766771\n",
            "Iteration 15, loss = 0.66745568\n",
            "Iteration 16, loss = 0.66719824\n",
            "Iteration 17, loss = 0.66743748\n",
            "Iteration 18, loss = 0.66725436\n",
            "Iteration 19, loss = 0.66743852\n",
            "Iteration 20, loss = 0.66742044\n",
            "Iteration 21, loss = 0.66775915\n",
            "Iteration 22, loss = 0.66741424\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.77033228\n",
            "Iteration 2, loss = 0.73441112\n",
            "Iteration 3, loss = 0.71111252\n",
            "Iteration 4, loss = 0.69414711\n",
            "Iteration 5, loss = 0.68305135\n",
            "Iteration 6, loss = 0.67648414\n",
            "Iteration 7, loss = 0.67204213\n",
            "Iteration 8, loss = 0.67002244\n",
            "Iteration 9, loss = 0.66861074\n",
            "Iteration 10, loss = 0.66791594\n",
            "Iteration 11, loss = 0.66762138\n",
            "Iteration 12, loss = 0.66757794\n",
            "Iteration 13, loss = 0.66732809\n",
            "Iteration 14, loss = 0.66741806\n",
            "Iteration 15, loss = 0.66741264\n",
            "Iteration 16, loss = 0.66743267\n",
            "Iteration 17, loss = 0.66739069\n",
            "Iteration 18, loss = 0.66752478\n",
            "Iteration 19, loss = 0.66714857\n",
            "Iteration 20, loss = 0.66738982\n",
            "Iteration 21, loss = 0.66760171\n",
            "Iteration 22, loss = 0.66748744\n",
            "Iteration 23, loss = 0.66741343\n",
            "Iteration 24, loss = 0.66736294\n",
            "Iteration 25, loss = 0.66743913\n",
            "Iteration 26, loss = 0.66749390\n",
            "Iteration 27, loss = 0.66756113\n",
            "Iteration 28, loss = 0.66721267\n",
            "Iteration 29, loss = 0.66725705\n",
            "Iteration 30, loss = 0.66733822\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75545153\n",
            "Iteration 2, loss = 0.72212021\n",
            "Iteration 3, loss = 0.69995648\n",
            "Iteration 4, loss = 0.68581517\n",
            "Iteration 5, loss = 0.67656120\n",
            "Iteration 6, loss = 0.67169288\n",
            "Iteration 7, loss = 0.66886407\n",
            "Iteration 8, loss = 0.66719940\n",
            "Iteration 9, loss = 0.66645672\n",
            "Iteration 10, loss = 0.66587399\n",
            "Iteration 11, loss = 0.66602877\n",
            "Iteration 12, loss = 0.66582719\n",
            "Iteration 13, loss = 0.66591588\n",
            "Iteration 14, loss = 0.66565494\n",
            "Iteration 15, loss = 0.66585499\n",
            "Iteration 16, loss = 0.66576830\n",
            "Iteration 17, loss = 0.66581171\n",
            "Iteration 18, loss = 0.66575244\n",
            "Iteration 19, loss = 0.66587304\n",
            "Iteration 20, loss = 0.66568481\n",
            "Iteration 21, loss = 0.66582872\n",
            "Iteration 22, loss = 0.66558931\n",
            "Iteration 23, loss = 0.66575652\n",
            "Iteration 24, loss = 0.66577960\n",
            "Iteration 25, loss = 0.66590884\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89918135\n",
            "Iteration 2, loss = 0.82192480\n",
            "Iteration 3, loss = 0.76060395\n",
            "Iteration 4, loss = 0.72001139\n",
            "Iteration 5, loss = 0.69301952\n",
            "Iteration 6, loss = 0.67897568\n",
            "Iteration 7, loss = 0.67203499\n",
            "Iteration 8, loss = 0.66888212\n",
            "Iteration 9, loss = 0.66719336\n",
            "Iteration 10, loss = 0.66683751\n",
            "Iteration 11, loss = 0.66688118\n",
            "Iteration 12, loss = 0.66676296\n",
            "Iteration 13, loss = 0.66679564\n",
            "Iteration 14, loss = 0.66687785\n",
            "Iteration 15, loss = 0.66694071\n",
            "Iteration 16, loss = 0.66691288\n",
            "Iteration 17, loss = 0.66697732\n",
            "Iteration 18, loss = 0.66704675\n",
            "Iteration 19, loss = 0.66692319\n",
            "Iteration 20, loss = 0.66696735\n",
            "Iteration 21, loss = 0.66672155\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6136548636548637+-0.004996126860883573\n",
            "Eficiencia durante la validaciÃ³n = 0.6111111111111112+-0.002024801459313474\n",
            "f1-score  0.4664560257508187  +-  0.00660342592653192\n",
            "Iteration 1, loss = 0.76269221\n",
            "Iteration 2, loss = 0.71485553\n",
            "Iteration 3, loss = 0.68494276\n",
            "Iteration 4, loss = 0.67278852\n",
            "Iteration 5, loss = 0.66842309\n",
            "Iteration 6, loss = 0.66745580\n",
            "Iteration 7, loss = 0.66735839\n",
            "Iteration 8, loss = 0.66751931\n",
            "Iteration 9, loss = 0.66731288\n",
            "Iteration 10, loss = 0.66745379\n",
            "Iteration 11, loss = 0.66720436\n",
            "Iteration 12, loss = 0.66720204\n",
            "Iteration 13, loss = 0.66763770\n",
            "Iteration 14, loss = 0.66766101\n",
            "Iteration 15, loss = 0.66736142\n",
            "Iteration 16, loss = 0.66748104\n",
            "Iteration 17, loss = 0.66732820\n",
            "Iteration 18, loss = 0.66758299\n",
            "Iteration 19, loss = 0.66776002\n",
            "Iteration 20, loss = 0.66735624\n",
            "Iteration 21, loss = 0.66751120\n",
            "Iteration 22, loss = 0.66770416\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.77220474\n",
            "Iteration 2, loss = 0.73054822\n",
            "Iteration 3, loss = 0.70172377\n",
            "Iteration 4, loss = 0.68461505\n",
            "Iteration 5, loss = 0.67503000\n",
            "Iteration 6, loss = 0.67069257\n",
            "Iteration 7, loss = 0.66876451\n",
            "Iteration 8, loss = 0.66784661\n",
            "Iteration 9, loss = 0.66757736\n",
            "Iteration 10, loss = 0.66744637\n",
            "Iteration 11, loss = 0.66745857\n",
            "Iteration 12, loss = 0.66758384\n",
            "Iteration 13, loss = 0.66742095\n",
            "Iteration 14, loss = 0.66767672\n",
            "Iteration 15, loss = 0.66776123\n",
            "Iteration 16, loss = 0.66768742\n",
            "Iteration 17, loss = 0.66714806\n",
            "Iteration 18, loss = 0.66749296\n",
            "Iteration 19, loss = 0.66727908\n",
            "Iteration 20, loss = 0.66756451\n",
            "Iteration 21, loss = 0.66753048\n",
            "Iteration 22, loss = 0.66747142\n",
            "Iteration 23, loss = 0.66743561\n",
            "Iteration 24, loss = 0.66737973\n",
            "Iteration 25, loss = 0.66787556\n",
            "Iteration 26, loss = 0.66748336\n",
            "Iteration 27, loss = 0.66719817\n",
            "Iteration 28, loss = 0.66792031\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.77435678\n",
            "Iteration 2, loss = 0.72454874\n",
            "Iteration 3, loss = 0.69410840\n",
            "Iteration 4, loss = 0.67717937\n",
            "Iteration 5, loss = 0.66980138\n",
            "Iteration 6, loss = 0.66714207\n",
            "Iteration 7, loss = 0.66587029\n",
            "Iteration 8, loss = 0.66583715\n",
            "Iteration 9, loss = 0.66587043\n",
            "Iteration 10, loss = 0.66616286\n",
            "Iteration 11, loss = 0.66580590\n",
            "Iteration 12, loss = 0.66572511\n",
            "Iteration 13, loss = 0.66566221\n",
            "Iteration 14, loss = 0.66574437\n",
            "Iteration 15, loss = 0.66583422\n",
            "Iteration 16, loss = 0.66584782\n",
            "Iteration 17, loss = 0.66562341\n",
            "Iteration 18, loss = 0.66576340\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.77656876\n",
            "Iteration 2, loss = 0.72971576\n",
            "Iteration 3, loss = 0.70044128\n",
            "Iteration 4, loss = 0.68327409\n",
            "Iteration 5, loss = 0.67451717\n",
            "Iteration 6, loss = 0.67051350\n",
            "Iteration 7, loss = 0.66808683\n",
            "Iteration 8, loss = 0.66732432\n",
            "Iteration 9, loss = 0.66684114\n",
            "Iteration 10, loss = 0.66727144\n",
            "Iteration 11, loss = 0.66688954\n",
            "Iteration 12, loss = 0.66692385\n",
            "Iteration 13, loss = 0.66692639\n",
            "Iteration 14, loss = 0.66718224\n",
            "Iteration 15, loss = 0.66663313\n",
            "Iteration 16, loss = 0.66691034\n",
            "Iteration 17, loss = 0.66673714\n",
            "Iteration 18, loss = 0.66679439\n",
            "Iteration 19, loss = 0.66682720\n",
            "Iteration 20, loss = 0.66662577\n",
            "Iteration 21, loss = 0.66707933\n",
            "Iteration 22, loss = 0.66722516\n",
            "Iteration 23, loss = 0.66679492\n",
            "Iteration 24, loss = 0.66684273\n",
            "Iteration 25, loss = 0.66667907\n",
            "Iteration 26, loss = 0.66692499\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6124338624338624+-0.0029119023242275355\n",
            "Eficiencia durante la validaciÃ³n = 0.6114163614163615+-0.002498581432195536\n",
            "f1-score  0.4666114218322488  +-  0.00686333242118209\n",
            "Iteration 1, loss = 0.80538643\n",
            "Iteration 2, loss = 0.78704078\n",
            "Iteration 3, loss = 0.77062743\n",
            "Iteration 4, loss = 0.75697939\n",
            "Iteration 5, loss = 0.74425835\n",
            "Iteration 6, loss = 0.73335253\n",
            "Iteration 7, loss = 0.72387092\n",
            "Iteration 8, loss = 0.71572241\n",
            "Iteration 9, loss = 0.70862406\n",
            "Iteration 10, loss = 0.70242299\n",
            "Iteration 11, loss = 0.69693822\n",
            "Iteration 12, loss = 0.69208338\n",
            "Iteration 13, loss = 0.68837731\n",
            "Iteration 14, loss = 0.68513568\n",
            "Iteration 15, loss = 0.68241566\n",
            "Iteration 16, loss = 0.68010437\n",
            "Iteration 17, loss = 0.67826808\n",
            "Iteration 18, loss = 0.67682317\n",
            "Iteration 19, loss = 0.67546628\n",
            "Iteration 20, loss = 0.67439459\n",
            "Iteration 21, loss = 0.67355662\n",
            "Iteration 22, loss = 0.67280809\n",
            "Iteration 23, loss = 0.67212907\n",
            "Iteration 24, loss = 0.67160298\n",
            "Iteration 25, loss = 0.67107893\n",
            "Iteration 26, loss = 0.67074340\n",
            "Iteration 27, loss = 0.67034816\n",
            "Iteration 28, loss = 0.67012880\n",
            "Iteration 29, loss = 0.66982265\n",
            "Iteration 30, loss = 0.66964990\n",
            "Iteration 31, loss = 0.66938387\n",
            "Iteration 32, loss = 0.66920718\n",
            "Iteration 33, loss = 0.66896477\n",
            "Iteration 34, loss = 0.66881055\n",
            "Iteration 35, loss = 0.66861064\n",
            "Iteration 36, loss = 0.66847560\n",
            "Iteration 37, loss = 0.66835652\n",
            "Iteration 38, loss = 0.66824842\n",
            "Iteration 39, loss = 0.66814612\n",
            "Iteration 40, loss = 0.66805032\n",
            "Iteration 41, loss = 0.66796587\n",
            "Iteration 42, loss = 0.66789512\n",
            "Iteration 43, loss = 0.66782682\n",
            "Iteration 44, loss = 0.66775716\n",
            "Iteration 45, loss = 0.66774338\n",
            "Iteration 46, loss = 0.66769357\n",
            "Iteration 47, loss = 0.66764138\n",
            "Iteration 48, loss = 0.66757409\n",
            "Iteration 49, loss = 0.66756100\n",
            "Iteration 50, loss = 0.66750717\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84243665\n",
            "Iteration 2, loss = 0.82883675\n",
            "Iteration 3, loss = 0.81588158\n",
            "Iteration 4, loss = 0.80354399\n",
            "Iteration 5, loss = 0.79172785\n",
            "Iteration 6, loss = 0.78076234\n",
            "Iteration 7, loss = 0.76991607\n",
            "Iteration 8, loss = 0.75997650\n",
            "Iteration 9, loss = 0.75012504\n",
            "Iteration 10, loss = 0.74113368\n",
            "Iteration 11, loss = 0.73279059\n",
            "Iteration 12, loss = 0.72500883\n",
            "Iteration 13, loss = 0.71797796\n",
            "Iteration 14, loss = 0.71179392\n",
            "Iteration 15, loss = 0.70577357\n",
            "Iteration 16, loss = 0.70098653\n",
            "Iteration 17, loss = 0.69635215\n",
            "Iteration 18, loss = 0.69227247\n",
            "Iteration 19, loss = 0.68875585\n",
            "Iteration 20, loss = 0.68605152\n",
            "Iteration 21, loss = 0.68338546\n",
            "Iteration 22, loss = 0.68125683\n",
            "Iteration 23, loss = 0.67928696\n",
            "Iteration 24, loss = 0.67769443\n",
            "Iteration 25, loss = 0.67639684\n",
            "Iteration 26, loss = 0.67517180\n",
            "Iteration 27, loss = 0.67407553\n",
            "Iteration 28, loss = 0.67329223\n",
            "Iteration 29, loss = 0.67250671\n",
            "Iteration 30, loss = 0.67193296\n",
            "Iteration 31, loss = 0.67119995\n",
            "Iteration 32, loss = 0.67073557\n",
            "Iteration 33, loss = 0.67033617\n",
            "Iteration 34, loss = 0.66990677\n",
            "Iteration 35, loss = 0.66958498\n",
            "Iteration 36, loss = 0.66934317\n",
            "Iteration 37, loss = 0.66907456\n",
            "Iteration 38, loss = 0.66887412\n",
            "Iteration 39, loss = 0.66860411\n",
            "Iteration 40, loss = 0.66845248\n",
            "Iteration 41, loss = 0.66831683\n",
            "Iteration 42, loss = 0.66818408\n",
            "Iteration 43, loss = 0.66801742\n",
            "Iteration 44, loss = 0.66796651\n",
            "Iteration 45, loss = 0.66779656\n",
            "Iteration 46, loss = 0.66773096\n",
            "Iteration 47, loss = 0.66767522\n",
            "Iteration 48, loss = 0.66759065\n",
            "Iteration 49, loss = 0.66750205\n",
            "Iteration 50, loss = 0.66743175\n",
            "Iteration 51, loss = 0.66735305\n",
            "Iteration 52, loss = 0.66735416\n",
            "Iteration 53, loss = 0.66730065\n",
            "Iteration 54, loss = 0.66725196\n",
            "Iteration 55, loss = 0.66726823\n",
            "Iteration 56, loss = 0.66725858\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.74715246\n",
            "Iteration 2, loss = 0.74257950\n",
            "Iteration 3, loss = 0.73818369\n",
            "Iteration 4, loss = 0.73403437\n",
            "Iteration 5, loss = 0.73000686\n",
            "Iteration 6, loss = 0.72616176\n",
            "Iteration 7, loss = 0.72212171\n",
            "Iteration 8, loss = 0.71822043\n",
            "Iteration 9, loss = 0.71446331\n",
            "Iteration 10, loss = 0.71073085\n",
            "Iteration 11, loss = 0.70716837\n",
            "Iteration 12, loss = 0.70386582\n",
            "Iteration 13, loss = 0.70042466\n",
            "Iteration 14, loss = 0.69726395\n",
            "Iteration 15, loss = 0.69401678\n",
            "Iteration 16, loss = 0.69093077\n",
            "Iteration 17, loss = 0.68837662\n",
            "Iteration 18, loss = 0.68570608\n",
            "Iteration 19, loss = 0.68339129\n",
            "Iteration 20, loss = 0.68127811\n",
            "Iteration 21, loss = 0.67957700\n",
            "Iteration 22, loss = 0.67787136\n",
            "Iteration 23, loss = 0.67636225\n",
            "Iteration 24, loss = 0.67518725\n",
            "Iteration 25, loss = 0.67412848\n",
            "Iteration 26, loss = 0.67315557\n",
            "Iteration 27, loss = 0.67231640\n",
            "Iteration 28, loss = 0.67159010\n",
            "Iteration 29, loss = 0.67109197\n",
            "Iteration 30, loss = 0.67060390\n",
            "Iteration 31, loss = 0.67007200\n",
            "Iteration 32, loss = 0.66971585\n",
            "Iteration 33, loss = 0.66935910\n",
            "Iteration 34, loss = 0.66903953\n",
            "Iteration 35, loss = 0.66875716\n",
            "Iteration 36, loss = 0.66845618\n",
            "Iteration 37, loss = 0.66823381\n",
            "Iteration 38, loss = 0.66804792\n",
            "Iteration 39, loss = 0.66789169\n",
            "Iteration 40, loss = 0.66776342\n",
            "Iteration 41, loss = 0.66760952\n",
            "Iteration 42, loss = 0.66747410\n",
            "Iteration 43, loss = 0.66731870\n",
            "Iteration 44, loss = 0.66722146\n",
            "Iteration 45, loss = 0.66712579\n",
            "Iteration 46, loss = 0.66702111\n",
            "Iteration 47, loss = 0.66692352\n",
            "Iteration 48, loss = 0.66682102\n",
            "Iteration 49, loss = 0.66674927\n",
            "Iteration 50, loss = 0.66665230\n",
            "Iteration 51, loss = 0.66657887\n",
            "Iteration 52, loss = 0.66650696\n",
            "Iteration 53, loss = 0.66644338\n",
            "Iteration 54, loss = 0.66638489\n",
            "Iteration 55, loss = 0.66634082\n",
            "Iteration 56, loss = 0.66632618\n",
            "Iteration 57, loss = 0.66625872\n",
            "Iteration 58, loss = 0.66622067\n",
            "Iteration 59, loss = 0.66614223\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.61194261\n",
            "Iteration 2, loss = 1.53827450\n",
            "Iteration 3, loss = 1.47033867\n",
            "Iteration 4, loss = 1.40793765\n",
            "Iteration 5, loss = 1.35021993\n",
            "Iteration 6, loss = 1.29629911\n",
            "Iteration 7, loss = 1.24860169\n",
            "Iteration 8, loss = 1.20274372\n",
            "Iteration 9, loss = 1.16137134\n",
            "Iteration 10, loss = 1.12340974\n",
            "Iteration 11, loss = 1.08797049\n",
            "Iteration 12, loss = 1.05573338\n",
            "Iteration 13, loss = 1.02566786\n",
            "Iteration 14, loss = 0.99823871\n",
            "Iteration 15, loss = 0.97317539\n",
            "Iteration 16, loss = 0.94948892\n",
            "Iteration 17, loss = 0.92849215\n",
            "Iteration 18, loss = 0.90869846\n",
            "Iteration 19, loss = 0.89098420\n",
            "Iteration 20, loss = 0.87451369\n",
            "Iteration 21, loss = 0.85902161\n",
            "Iteration 22, loss = 0.84502753\n",
            "Iteration 23, loss = 0.83191466\n",
            "Iteration 24, loss = 0.81948549\n",
            "Iteration 25, loss = 0.80856540\n",
            "Iteration 26, loss = 0.79818963\n",
            "Iteration 27, loss = 0.78856126\n",
            "Iteration 28, loss = 0.78000884\n",
            "Iteration 29, loss = 0.77167565\n",
            "Iteration 30, loss = 0.76432923\n",
            "Iteration 31, loss = 0.75740283\n",
            "Iteration 32, loss = 0.75089296\n",
            "Iteration 33, loss = 0.74508917\n",
            "Iteration 34, loss = 0.73949904\n",
            "Iteration 35, loss = 0.73435959\n",
            "Iteration 36, loss = 0.72970802\n",
            "Iteration 37, loss = 0.72527183\n",
            "Iteration 38, loss = 0.72120433\n",
            "Iteration 39, loss = 0.71745965\n",
            "Iteration 40, loss = 0.71405853\n",
            "Iteration 41, loss = 0.71079379\n",
            "Iteration 42, loss = 0.70788254\n",
            "Iteration 43, loss = 0.70511253\n",
            "Iteration 44, loss = 0.70252597\n",
            "Iteration 45, loss = 0.70020030\n",
            "Iteration 46, loss = 0.69800846\n",
            "Iteration 47, loss = 0.69604301\n",
            "Iteration 48, loss = 0.69426520\n",
            "Iteration 49, loss = 0.69246125\n",
            "Iteration 50, loss = 0.69093628\n",
            "Iteration 51, loss = 0.68938920\n",
            "Iteration 52, loss = 0.68803718\n",
            "Iteration 53, loss = 0.68672401\n",
            "Iteration 54, loss = 0.68557165\n",
            "Iteration 55, loss = 0.68438599\n",
            "Iteration 56, loss = 0.68338939\n",
            "Iteration 57, loss = 0.68237995\n",
            "Iteration 58, loss = 0.68145784\n",
            "Iteration 59, loss = 0.68063213\n",
            "Iteration 60, loss = 0.67987442\n",
            "Iteration 61, loss = 0.67918894\n",
            "Iteration 62, loss = 0.67852809\n",
            "Iteration 63, loss = 0.67792663\n",
            "Iteration 64, loss = 0.67734423\n",
            "Iteration 65, loss = 0.67671487\n",
            "Iteration 66, loss = 0.67623665\n",
            "Iteration 67, loss = 0.67573110\n",
            "Iteration 68, loss = 0.67528207\n",
            "Iteration 69, loss = 0.67483716\n",
            "Iteration 70, loss = 0.67444470\n",
            "Iteration 71, loss = 0.67409096\n",
            "Iteration 72, loss = 0.67379137\n",
            "Iteration 73, loss = 0.67345932\n",
            "Iteration 74, loss = 0.67319475\n",
            "Iteration 75, loss = 0.67293640\n",
            "Iteration 76, loss = 0.67263617\n",
            "Iteration 77, loss = 0.67239036\n",
            "Iteration 78, loss = 0.67214351\n",
            "Iteration 79, loss = 0.67192433\n",
            "Iteration 80, loss = 0.67170731\n",
            "Iteration 81, loss = 0.67151085\n",
            "Iteration 82, loss = 0.67131579\n",
            "Iteration 83, loss = 0.67114391\n",
            "Iteration 84, loss = 0.67095902\n",
            "Iteration 85, loss = 0.67080741\n",
            "Iteration 86, loss = 0.67065299\n",
            "Iteration 87, loss = 0.67050108\n",
            "Iteration 88, loss = 0.67031871\n",
            "Iteration 89, loss = 0.67017384\n",
            "Iteration 90, loss = 0.67007348\n",
            "Iteration 91, loss = 0.66989671\n",
            "Iteration 92, loss = 0.66981471\n",
            "Iteration 93, loss = 0.66966516\n",
            "Iteration 94, loss = 0.66952100\n",
            "Iteration 95, loss = 0.66941584\n",
            "Iteration 96, loss = 0.66930437\n",
            "Iteration 97, loss = 0.66923061\n",
            "Iteration 98, loss = 0.66912516\n",
            "Iteration 99, loss = 0.66905031\n",
            "Iteration 100, loss = 0.66893241\n",
            "Iteration 101, loss = 0.66885836\n",
            "Iteration 102, loss = 0.66878295\n",
            "Iteration 103, loss = 0.66871400\n",
            "Iteration 104, loss = 0.66863570\n",
            "Iteration 105, loss = 0.66857157\n",
            "Iteration 106, loss = 0.66849802\n",
            "Iteration 107, loss = 0.66841613\n",
            "Iteration 108, loss = 0.66835091\n",
            "Iteration 109, loss = 0.66827844\n",
            "Iteration 110, loss = 0.66819701\n",
            "Iteration 111, loss = 0.66814845\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6117216117216118+-0.0019730076749761003\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4632042271708273  +-  0.0014649035750146594\n",
            "Iteration 1, loss = 0.70439464\n",
            "Iteration 2, loss = 0.68973595\n",
            "Iteration 3, loss = 0.68062708\n",
            "Iteration 4, loss = 0.67558733\n",
            "Iteration 5, loss = 0.67262659\n",
            "Iteration 6, loss = 0.67097189\n",
            "Iteration 7, loss = 0.66977330\n",
            "Iteration 8, loss = 0.66910350\n",
            "Iteration 9, loss = 0.66847296\n",
            "Iteration 10, loss = 0.66814525\n",
            "Iteration 11, loss = 0.66776748\n",
            "Iteration 12, loss = 0.66773982\n",
            "Iteration 13, loss = 0.66746718\n",
            "Iteration 14, loss = 0.66760602\n",
            "Iteration 15, loss = 0.66725482\n",
            "Iteration 16, loss = 0.66722398\n",
            "Iteration 17, loss = 0.66734254\n",
            "Iteration 18, loss = 0.66727482\n",
            "Iteration 19, loss = 0.66721281\n",
            "Iteration 20, loss = 0.66707803\n",
            "Iteration 21, loss = 0.66713456\n",
            "Iteration 22, loss = 0.66727177\n",
            "Iteration 23, loss = 0.66716383\n",
            "Iteration 24, loss = 0.66712016\n",
            "Iteration 25, loss = 0.66704070\n",
            "Iteration 26, loss = 0.66720999\n",
            "Iteration 27, loss = 0.66701683\n",
            "Iteration 28, loss = 0.66702346\n",
            "Iteration 29, loss = 0.66713895\n",
            "Iteration 30, loss = 0.66711941\n",
            "Iteration 31, loss = 0.66722666\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.98651254\n",
            "Iteration 2, loss = 0.93869144\n",
            "Iteration 3, loss = 0.89568643\n",
            "Iteration 4, loss = 0.86082494\n",
            "Iteration 5, loss = 0.83078289\n",
            "Iteration 6, loss = 0.80564794\n",
            "Iteration 7, loss = 0.78464329\n",
            "Iteration 8, loss = 0.76763719\n",
            "Iteration 9, loss = 0.75236606\n",
            "Iteration 10, loss = 0.74025201\n",
            "Iteration 11, loss = 0.72947222\n",
            "Iteration 12, loss = 0.72099814\n",
            "Iteration 13, loss = 0.71340825\n",
            "Iteration 14, loss = 0.70714598\n",
            "Iteration 15, loss = 0.70180206\n",
            "Iteration 16, loss = 0.69733940\n",
            "Iteration 17, loss = 0.69322998\n",
            "Iteration 18, loss = 0.69018349\n",
            "Iteration 19, loss = 0.68725634\n",
            "Iteration 20, loss = 0.68507489\n",
            "Iteration 21, loss = 0.68293763\n",
            "Iteration 22, loss = 0.68104967\n",
            "Iteration 23, loss = 0.67957988\n",
            "Iteration 24, loss = 0.67825379\n",
            "Iteration 25, loss = 0.67707905\n",
            "Iteration 26, loss = 0.67605410\n",
            "Iteration 27, loss = 0.67506474\n",
            "Iteration 28, loss = 0.67447160\n",
            "Iteration 29, loss = 0.67355251\n",
            "Iteration 30, loss = 0.67286258\n",
            "Iteration 31, loss = 0.67238209\n",
            "Iteration 32, loss = 0.67186905\n",
            "Iteration 33, loss = 0.67147664\n",
            "Iteration 34, loss = 0.67106108\n",
            "Iteration 35, loss = 0.67061960\n",
            "Iteration 36, loss = 0.67031821\n",
            "Iteration 37, loss = 0.66996795\n",
            "Iteration 38, loss = 0.66962917\n",
            "Iteration 39, loss = 0.66949369\n",
            "Iteration 40, loss = 0.66912801\n",
            "Iteration 41, loss = 0.66907592\n",
            "Iteration 42, loss = 0.66891959\n",
            "Iteration 43, loss = 0.66863454\n",
            "Iteration 44, loss = 0.66851062\n",
            "Iteration 45, loss = 0.66844504\n",
            "Iteration 46, loss = 0.66830601\n",
            "Iteration 47, loss = 0.66822656\n",
            "Iteration 48, loss = 0.66806439\n",
            "Iteration 49, loss = 0.66805350\n",
            "Iteration 50, loss = 0.66791486\n",
            "Iteration 51, loss = 0.66785637\n",
            "Iteration 52, loss = 0.66773609\n",
            "Iteration 53, loss = 0.66773100\n",
            "Iteration 54, loss = 0.66764780\n",
            "Iteration 55, loss = 0.66759960\n",
            "Iteration 56, loss = 0.66751501\n",
            "Iteration 57, loss = 0.66745592\n",
            "Iteration 58, loss = 0.66744768\n",
            "Iteration 59, loss = 0.66738948\n",
            "Iteration 60, loss = 0.66735784\n",
            "Iteration 61, loss = 0.66735996\n",
            "Iteration 62, loss = 0.66730005\n",
            "Iteration 63, loss = 0.66735785\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70752406\n",
            "Iteration 2, loss = 0.69217751\n",
            "Iteration 3, loss = 0.68407399\n",
            "Iteration 4, loss = 0.67913147\n",
            "Iteration 5, loss = 0.67619362\n",
            "Iteration 6, loss = 0.67419607\n",
            "Iteration 7, loss = 0.67274571\n",
            "Iteration 8, loss = 0.67160785\n",
            "Iteration 9, loss = 0.67062821\n",
            "Iteration 10, loss = 0.66996172\n",
            "Iteration 11, loss = 0.66933046\n",
            "Iteration 12, loss = 0.66894083\n",
            "Iteration 13, loss = 0.66847567\n",
            "Iteration 14, loss = 0.66820371\n",
            "Iteration 15, loss = 0.66805729\n",
            "Iteration 16, loss = 0.66767225\n",
            "Iteration 17, loss = 0.66752843\n",
            "Iteration 18, loss = 0.66749801\n",
            "Iteration 19, loss = 0.66732270\n",
            "Iteration 20, loss = 0.66716176\n",
            "Iteration 21, loss = 0.66706524\n",
            "Iteration 22, loss = 0.66700606\n",
            "Iteration 23, loss = 0.66689893\n",
            "Iteration 24, loss = 0.66685563\n",
            "Iteration 25, loss = 0.66683731\n",
            "Iteration 26, loss = 0.66668991\n",
            "Iteration 27, loss = 0.66657790\n",
            "Iteration 28, loss = 0.66665043\n",
            "Iteration 29, loss = 0.66661947\n",
            "Iteration 30, loss = 0.66656109\n",
            "Iteration 31, loss = 0.66639143\n",
            "Iteration 32, loss = 0.66632826\n",
            "Iteration 33, loss = 0.66634631\n",
            "Iteration 34, loss = 0.66642507\n",
            "Iteration 35, loss = 0.66623106\n",
            "Iteration 36, loss = 0.66632757\n",
            "Iteration 37, loss = 0.66609753\n",
            "Iteration 38, loss = 0.66610685\n",
            "Iteration 39, loss = 0.66610338\n",
            "Iteration 40, loss = 0.66609949\n",
            "Iteration 41, loss = 0.66597311\n",
            "Iteration 42, loss = 0.66610697\n",
            "Iteration 43, loss = 0.66593577\n",
            "Iteration 44, loss = 0.66594021\n",
            "Iteration 45, loss = 0.66625059\n",
            "Iteration 46, loss = 0.66598840\n",
            "Iteration 47, loss = 0.66577046\n",
            "Iteration 48, loss = 0.66575169\n",
            "Iteration 49, loss = 0.66585398\n",
            "Iteration 50, loss = 0.66575669\n",
            "Iteration 51, loss = 0.66588730\n",
            "Iteration 52, loss = 0.66568116\n",
            "Iteration 53, loss = 0.66568992\n",
            "Iteration 54, loss = 0.66595714\n",
            "Iteration 55, loss = 0.66561337\n",
            "Iteration 56, loss = 0.66564072\n",
            "Iteration 57, loss = 0.66561592\n",
            "Iteration 58, loss = 0.66568784\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88665387\n",
            "Iteration 2, loss = 0.84783773\n",
            "Iteration 3, loss = 0.81550861\n",
            "Iteration 4, loss = 0.79158378\n",
            "Iteration 5, loss = 0.77199598\n",
            "Iteration 6, loss = 0.75650274\n",
            "Iteration 7, loss = 0.74419573\n",
            "Iteration 8, loss = 0.73435662\n",
            "Iteration 9, loss = 0.72600492\n",
            "Iteration 10, loss = 0.71919986\n",
            "Iteration 11, loss = 0.71328869\n",
            "Iteration 12, loss = 0.70840251\n",
            "Iteration 13, loss = 0.70404280\n",
            "Iteration 14, loss = 0.70021473\n",
            "Iteration 15, loss = 0.69695582\n",
            "Iteration 16, loss = 0.69416818\n",
            "Iteration 17, loss = 0.69176996\n",
            "Iteration 18, loss = 0.68953180\n",
            "Iteration 19, loss = 0.68767629\n",
            "Iteration 20, loss = 0.68591249\n",
            "Iteration 21, loss = 0.68450909\n",
            "Iteration 22, loss = 0.68310333\n",
            "Iteration 23, loss = 0.68187534\n",
            "Iteration 24, loss = 0.68076830\n",
            "Iteration 25, loss = 0.67962349\n",
            "Iteration 26, loss = 0.67870621\n",
            "Iteration 27, loss = 0.67791725\n",
            "Iteration 28, loss = 0.67710311\n",
            "Iteration 29, loss = 0.67640015\n",
            "Iteration 30, loss = 0.67578483\n",
            "Iteration 31, loss = 0.67515377\n",
            "Iteration 32, loss = 0.67456525\n",
            "Iteration 33, loss = 0.67422745\n",
            "Iteration 34, loss = 0.67366746\n",
            "Iteration 35, loss = 0.67327015\n",
            "Iteration 36, loss = 0.67288902\n",
            "Iteration 37, loss = 0.67246961\n",
            "Iteration 38, loss = 0.67210081\n",
            "Iteration 39, loss = 0.67187723\n",
            "Iteration 40, loss = 0.67155270\n",
            "Iteration 41, loss = 0.67136099\n",
            "Iteration 42, loss = 0.67111874\n",
            "Iteration 43, loss = 0.67086744\n",
            "Iteration 44, loss = 0.67069311\n",
            "Iteration 45, loss = 0.67040021\n",
            "Iteration 46, loss = 0.67022282\n",
            "Iteration 47, loss = 0.67000278\n",
            "Iteration 48, loss = 0.66979995\n",
            "Iteration 49, loss = 0.66968947\n",
            "Iteration 50, loss = 0.66947804\n",
            "Iteration 51, loss = 0.66936526\n",
            "Iteration 52, loss = 0.66924956\n",
            "Iteration 53, loss = 0.66912346\n",
            "Iteration 54, loss = 0.66905106\n",
            "Iteration 55, loss = 0.66889292\n",
            "Iteration 56, loss = 0.66877777\n",
            "Iteration 57, loss = 0.66862954\n",
            "Iteration 58, loss = 0.66854665\n",
            "Iteration 59, loss = 0.66849296\n",
            "Iteration 60, loss = 0.66840690\n",
            "Iteration 61, loss = 0.66840145\n",
            "Iteration 62, loss = 0.66833714\n",
            "Iteration 63, loss = 0.66818316\n",
            "Iteration 64, loss = 0.66812969\n",
            "Iteration 65, loss = 0.66812671\n",
            "Iteration 66, loss = 0.66804361\n",
            "Iteration 67, loss = 0.66794131\n",
            "Iteration 68, loss = 0.66790406\n",
            "Iteration 69, loss = 0.66777682\n",
            "Iteration 70, loss = 0.66777205\n",
            "Iteration 71, loss = 0.66774205\n",
            "Iteration 72, loss = 0.66768629\n",
            "Iteration 73, loss = 0.66766261\n",
            "Iteration 74, loss = 0.66758822\n",
            "Iteration 75, loss = 0.66752723\n",
            "Iteration 76, loss = 0.66756278\n",
            "Iteration 77, loss = 0.66747185\n",
            "Iteration 78, loss = 0.66743050\n",
            "Iteration 79, loss = 0.66733530\n",
            "Iteration 80, loss = 0.66734064\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6120268620268621+-0.0013305552330710188\n",
            "Eficiencia durante la validaciÃ³n = 0.6108058608058609+-0.002001660111203332\n",
            "f1-score  0.46417943862290845  +-  0.0040787518209848455\n",
            "Iteration 1, loss = 0.87137204\n",
            "Iteration 2, loss = 0.79908932\n",
            "Iteration 3, loss = 0.75290307\n",
            "Iteration 4, loss = 0.72509472\n",
            "Iteration 5, loss = 0.70863098\n",
            "Iteration 6, loss = 0.69695846\n",
            "Iteration 7, loss = 0.68905805\n",
            "Iteration 8, loss = 0.68343844\n",
            "Iteration 9, loss = 0.67935587\n",
            "Iteration 10, loss = 0.67638626\n",
            "Iteration 11, loss = 0.67401273\n",
            "Iteration 12, loss = 0.67215329\n",
            "Iteration 13, loss = 0.67085782\n",
            "Iteration 14, loss = 0.66979820\n",
            "Iteration 15, loss = 0.66893834\n",
            "Iteration 16, loss = 0.66846538\n",
            "Iteration 17, loss = 0.66829905\n",
            "Iteration 18, loss = 0.66779965\n",
            "Iteration 19, loss = 0.66763021\n",
            "Iteration 20, loss = 0.66746466\n",
            "Iteration 21, loss = 0.66734797\n",
            "Iteration 22, loss = 0.66754600\n",
            "Iteration 23, loss = 0.66754880\n",
            "Iteration 24, loss = 0.66733557\n",
            "Iteration 25, loss = 0.66748500\n",
            "Iteration 26, loss = 0.66732956\n",
            "Iteration 27, loss = 0.66735572\n",
            "Iteration 28, loss = 0.66725891\n",
            "Iteration 29, loss = 0.66726277\n",
            "Iteration 30, loss = 0.66748943\n",
            "Iteration 31, loss = 0.66733247\n",
            "Iteration 32, loss = 0.66717437\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.74088401\n",
            "Iteration 2, loss = 0.71125403\n",
            "Iteration 3, loss = 0.69319986\n",
            "Iteration 4, loss = 0.68363262\n",
            "Iteration 5, loss = 0.67824391\n",
            "Iteration 6, loss = 0.67503222\n",
            "Iteration 7, loss = 0.67286311\n",
            "Iteration 8, loss = 0.67164481\n",
            "Iteration 9, loss = 0.67065589\n",
            "Iteration 10, loss = 0.66980066\n",
            "Iteration 11, loss = 0.66926771\n",
            "Iteration 12, loss = 0.66868559\n",
            "Iteration 13, loss = 0.66842468\n",
            "Iteration 14, loss = 0.66825764\n",
            "Iteration 15, loss = 0.66799423\n",
            "Iteration 16, loss = 0.66781305\n",
            "Iteration 17, loss = 0.66749828\n",
            "Iteration 18, loss = 0.66752759\n",
            "Iteration 19, loss = 0.66746803\n",
            "Iteration 20, loss = 0.66747732\n",
            "Iteration 21, loss = 0.66735134\n",
            "Iteration 22, loss = 0.66722815\n",
            "Iteration 23, loss = 0.66740495\n",
            "Iteration 24, loss = 0.66730025\n",
            "Iteration 25, loss = 0.66732490\n",
            "Iteration 26, loss = 0.66717655\n",
            "Iteration 27, loss = 0.66718869\n",
            "Iteration 28, loss = 0.66731244\n",
            "Iteration 29, loss = 0.66728474\n",
            "Iteration 30, loss = 0.66711064\n",
            "Iteration 31, loss = 0.66726803\n",
            "Iteration 32, loss = 0.66721560\n",
            "Iteration 33, loss = 0.66719953\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.86018316\n",
            "Iteration 2, loss = 0.79166965\n",
            "Iteration 3, loss = 0.74911572\n",
            "Iteration 4, loss = 0.72305560\n",
            "Iteration 5, loss = 0.70284395\n",
            "Iteration 6, loss = 0.68896193\n",
            "Iteration 7, loss = 0.68048455\n",
            "Iteration 8, loss = 0.67368511\n",
            "Iteration 9, loss = 0.66994334\n",
            "Iteration 10, loss = 0.66793871\n",
            "Iteration 11, loss = 0.66661346\n",
            "Iteration 12, loss = 0.66663108\n",
            "Iteration 13, loss = 0.66605257\n",
            "Iteration 14, loss = 0.66603255\n",
            "Iteration 15, loss = 0.66582921\n",
            "Iteration 16, loss = 0.66567156\n",
            "Iteration 17, loss = 0.66555140\n",
            "Iteration 18, loss = 0.66598812\n",
            "Iteration 19, loss = 0.66579991\n",
            "Iteration 20, loss = 0.66578888\n",
            "Iteration 21, loss = 0.66606998\n",
            "Iteration 22, loss = 0.66578741\n",
            "Iteration 23, loss = 0.66572809\n",
            "Iteration 24, loss = 0.66570630\n",
            "Iteration 25, loss = 0.66604170\n",
            "Iteration 26, loss = 0.66562234\n",
            "Iteration 27, loss = 0.66576503\n",
            "Iteration 28, loss = 0.66595430\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89964535\n",
            "Iteration 2, loss = 0.82448010\n",
            "Iteration 3, loss = 0.77140748\n",
            "Iteration 4, loss = 0.73595102\n",
            "Iteration 5, loss = 0.71217659\n",
            "Iteration 6, loss = 0.69646978\n",
            "Iteration 7, loss = 0.68634795\n",
            "Iteration 8, loss = 0.67952958\n",
            "Iteration 9, loss = 0.67518218\n",
            "Iteration 10, loss = 0.67228773\n",
            "Iteration 11, loss = 0.67014971\n",
            "Iteration 12, loss = 0.66918895\n",
            "Iteration 13, loss = 0.66832879\n",
            "Iteration 14, loss = 0.66773587\n",
            "Iteration 15, loss = 0.66747750\n",
            "Iteration 16, loss = 0.66720036\n",
            "Iteration 17, loss = 0.66720591\n",
            "Iteration 18, loss = 0.66688616\n",
            "Iteration 19, loss = 0.66684183\n",
            "Iteration 20, loss = 0.66670733\n",
            "Iteration 21, loss = 0.66662294\n",
            "Iteration 22, loss = 0.66670212\n",
            "Iteration 23, loss = 0.66661507\n",
            "Iteration 24, loss = 0.66675957\n",
            "Iteration 25, loss = 0.66659792\n",
            "Iteration 26, loss = 0.66668452\n",
            "Iteration 27, loss = 0.66659102\n",
            "Iteration 28, loss = 0.66659557\n",
            "Iteration 29, loss = 0.66660654\n",
            "Iteration 30, loss = 0.66668240\n",
            "Iteration 31, loss = 0.66665903\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6127391127391127+-0.002841725690632654\n",
            "Eficiencia durante la validaciÃ³n = 0.6105006105006106+-0.0014954149833841537\n",
            "f1-score  0.4661680657254196  +-  0.00514995462331914\n",
            "Iteration 1, loss = 0.68408800\n",
            "Iteration 2, loss = 0.66901519\n",
            "Iteration 3, loss = 0.66851816\n",
            "Iteration 4, loss = 0.66866979\n",
            "Iteration 5, loss = 0.66820773\n",
            "Iteration 6, loss = 0.66849806\n",
            "Iteration 7, loss = 0.66853011\n",
            "Iteration 8, loss = 0.66801664\n",
            "Iteration 9, loss = 0.66795621\n",
            "Iteration 10, loss = 0.66848888\n",
            "Iteration 11, loss = 0.66750427\n",
            "Iteration 12, loss = 0.66781070\n",
            "Iteration 13, loss = 0.66916282\n",
            "Iteration 14, loss = 0.66807832\n",
            "Iteration 15, loss = 0.66850802\n",
            "Iteration 16, loss = 0.66783229\n",
            "Iteration 17, loss = 0.66820707\n",
            "Iteration 18, loss = 0.66862605\n",
            "Iteration 19, loss = 0.66788869\n",
            "Iteration 20, loss = 0.66794123\n",
            "Iteration 21, loss = 0.66864234\n",
            "Iteration 22, loss = 0.66806859\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68853433\n",
            "Iteration 2, loss = 0.67026219\n",
            "Iteration 3, loss = 0.66971602\n",
            "Iteration 4, loss = 0.66817725\n",
            "Iteration 5, loss = 0.66854349\n",
            "Iteration 6, loss = 0.66818150\n",
            "Iteration 7, loss = 0.66849195\n",
            "Iteration 8, loss = 0.67003243\n",
            "Iteration 9, loss = 0.66991904\n",
            "Iteration 10, loss = 0.66797086\n",
            "Iteration 11, loss = 0.66840179\n",
            "Iteration 12, loss = 0.66773396\n",
            "Iteration 13, loss = 0.66941917\n",
            "Iteration 14, loss = 0.66827249\n",
            "Iteration 15, loss = 0.66805508\n",
            "Iteration 16, loss = 0.66802730\n",
            "Iteration 17, loss = 0.66818683\n",
            "Iteration 18, loss = 0.66931154\n",
            "Iteration 19, loss = 0.66819341\n",
            "Iteration 20, loss = 0.66880499\n",
            "Iteration 21, loss = 0.66904479\n",
            "Iteration 22, loss = 0.66852529\n",
            "Iteration 23, loss = 0.66875067\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.86024757\n",
            "Iteration 2, loss = 0.72020901\n",
            "Iteration 3, loss = 0.67250783\n",
            "Iteration 4, loss = 0.66745786\n",
            "Iteration 5, loss = 0.66704995\n",
            "Iteration 6, loss = 0.66728975\n",
            "Iteration 7, loss = 0.66662650\n",
            "Iteration 8, loss = 0.66632471\n",
            "Iteration 9, loss = 0.66648380\n",
            "Iteration 10, loss = 0.66625568\n",
            "Iteration 11, loss = 0.66656034\n",
            "Iteration 12, loss = 0.66637512\n",
            "Iteration 13, loss = 0.66624362\n",
            "Iteration 14, loss = 0.66655801\n",
            "Iteration 15, loss = 0.66629692\n",
            "Iteration 16, loss = 0.66640849\n",
            "Iteration 17, loss = 0.66678920\n",
            "Iteration 18, loss = 0.66644616\n",
            "Iteration 19, loss = 0.66698529\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.79958831\n",
            "Iteration 2, loss = 0.69321304\n",
            "Iteration 3, loss = 0.67289220\n",
            "Iteration 4, loss = 0.66968434\n",
            "Iteration 5, loss = 0.66764237\n",
            "Iteration 6, loss = 0.66744571\n",
            "Iteration 7, loss = 0.66779491\n",
            "Iteration 8, loss = 0.66785430\n",
            "Iteration 9, loss = 0.66728597\n",
            "Iteration 10, loss = 0.66826178\n",
            "Iteration 11, loss = 0.66750218\n",
            "Iteration 12, loss = 0.66771495\n",
            "Iteration 13, loss = 0.66798387\n",
            "Iteration 14, loss = 0.66843815\n",
            "Iteration 15, loss = 0.66876634\n",
            "Iteration 16, loss = 0.66766654\n",
            "Iteration 17, loss = 0.66793656\n",
            "Iteration 18, loss = 0.66756335\n",
            "Iteration 19, loss = 0.66727931\n",
            "Iteration 20, loss = 0.66774447\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6125356125356125+-0.0033438495574185847\n",
            "Eficiencia durante la validaciÃ³n = 0.6120268620268621+-0.0027809626309964735\n",
            "f1-score  0.46800597327908106  +-  0.006723795777252328\n",
            "Iteration 1, loss = 0.72611735\n",
            "Iteration 2, loss = 0.67495324\n",
            "Iteration 3, loss = 0.66891842\n",
            "Iteration 4, loss = 0.66831116\n",
            "Iteration 5, loss = 0.66775286\n",
            "Iteration 6, loss = 0.66802469\n",
            "Iteration 7, loss = 0.66837382\n",
            "Iteration 8, loss = 0.66794952\n",
            "Iteration 9, loss = 0.66881156\n",
            "Iteration 10, loss = 0.66800411\n",
            "Iteration 11, loss = 0.66812531\n",
            "Iteration 12, loss = 0.66898394\n",
            "Iteration 13, loss = 0.66851901\n",
            "Iteration 14, loss = 0.66870517\n",
            "Iteration 15, loss = 0.66804425\n",
            "Iteration 16, loss = 0.66818985\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.73612554\n",
            "Iteration 2, loss = 0.67576841\n",
            "Iteration 3, loss = 0.67056142\n",
            "Iteration 4, loss = 0.66872882\n",
            "Iteration 5, loss = 0.66897703\n",
            "Iteration 6, loss = 0.66833437\n",
            "Iteration 7, loss = 0.66861827\n",
            "Iteration 8, loss = 0.66799326\n",
            "Iteration 9, loss = 0.66856347\n",
            "Iteration 10, loss = 0.66799859\n",
            "Iteration 11, loss = 0.66878376\n",
            "Iteration 12, loss = 0.66812989\n",
            "Iteration 13, loss = 0.66796574\n",
            "Iteration 14, loss = 0.66773124\n",
            "Iteration 15, loss = 0.66829277\n",
            "Iteration 16, loss = 0.66823963\n",
            "Iteration 17, loss = 0.66894861\n",
            "Iteration 18, loss = 0.66798074\n",
            "Iteration 19, loss = 0.66868545\n",
            "Iteration 20, loss = 0.66826014\n",
            "Iteration 21, loss = 0.66830647\n",
            "Iteration 22, loss = 0.66867487\n",
            "Iteration 23, loss = 0.66865515\n",
            "Iteration 24, loss = 0.66884456\n",
            "Iteration 25, loss = 0.66774205\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71734674\n",
            "Iteration 2, loss = 0.67195427\n",
            "Iteration 3, loss = 0.66860415\n",
            "Iteration 4, loss = 0.66769906\n",
            "Iteration 5, loss = 0.66664544\n",
            "Iteration 6, loss = 0.66623625\n",
            "Iteration 7, loss = 0.66624684\n",
            "Iteration 8, loss = 0.66641159\n",
            "Iteration 9, loss = 0.66662567\n",
            "Iteration 10, loss = 0.66687836\n",
            "Iteration 11, loss = 0.66683908\n",
            "Iteration 12, loss = 0.66640981\n",
            "Iteration 13, loss = 0.66619306\n",
            "Iteration 14, loss = 0.66689180\n",
            "Iteration 15, loss = 0.66704492\n",
            "Iteration 16, loss = 0.66657336\n",
            "Iteration 17, loss = 0.66672789\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.74064648\n",
            "Iteration 2, loss = 0.67372434\n",
            "Iteration 3, loss = 0.66867832\n",
            "Iteration 4, loss = 0.66795647\n",
            "Iteration 5, loss = 0.66808680\n",
            "Iteration 6, loss = 0.66755626\n",
            "Iteration 7, loss = 0.66818246\n",
            "Iteration 8, loss = 0.66826425\n",
            "Iteration 9, loss = 0.66788331\n",
            "Iteration 10, loss = 0.66785149\n",
            "Iteration 11, loss = 0.66822220\n",
            "Iteration 12, loss = 0.66810828\n",
            "Iteration 13, loss = 0.66754888\n",
            "Iteration 14, loss = 0.66749658\n",
            "Iteration 15, loss = 0.66782990\n",
            "Iteration 16, loss = 0.66797998\n",
            "Iteration 17, loss = 0.66777710\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6135531135531135+-0.0035656116133364383\n",
            "Eficiencia durante la validaciÃ³n = 0.6117216117216118+-0.002590134729621096\n",
            "f1-score  0.47040674334138866  +-  0.009376664185810379\n",
            "Iteration 1, loss = 0.68497264\n",
            "Iteration 2, loss = 0.67887391\n",
            "Iteration 3, loss = 0.67520929\n",
            "Iteration 4, loss = 0.67306909\n",
            "Iteration 5, loss = 0.67195502\n",
            "Iteration 6, loss = 0.67110024\n",
            "Iteration 7, loss = 0.67055567\n",
            "Iteration 8, loss = 0.67011742\n",
            "Iteration 9, loss = 0.66984871\n",
            "Iteration 10, loss = 0.66964857\n",
            "Iteration 11, loss = 0.66937919\n",
            "Iteration 12, loss = 0.66924477\n",
            "Iteration 13, loss = 0.66921367\n",
            "Iteration 14, loss = 0.66903425\n",
            "Iteration 15, loss = 0.66890150\n",
            "Iteration 16, loss = 0.66889297\n",
            "Iteration 17, loss = 0.66874392\n",
            "Iteration 18, loss = 0.66870511\n",
            "Iteration 19, loss = 0.66865581\n",
            "Iteration 20, loss = 0.66865341\n",
            "Iteration 21, loss = 0.66855187\n",
            "Iteration 22, loss = 0.66847323\n",
            "Iteration 23, loss = 0.66842964\n",
            "Iteration 24, loss = 0.66840101\n",
            "Iteration 25, loss = 0.66842631\n",
            "Iteration 26, loss = 0.66833382\n",
            "Iteration 27, loss = 0.66836261\n",
            "Iteration 28, loss = 0.66828646\n",
            "Iteration 29, loss = 0.66825514\n",
            "Iteration 30, loss = 0.66823865\n",
            "Iteration 31, loss = 0.66818311\n",
            "Iteration 32, loss = 0.66817664\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.01194251\n",
            "Iteration 2, loss = 1.75908034\n",
            "Iteration 3, loss = 1.54010866\n",
            "Iteration 4, loss = 1.34804454\n",
            "Iteration 5, loss = 1.18702039\n",
            "Iteration 6, loss = 1.05354281\n",
            "Iteration 7, loss = 0.94607778\n",
            "Iteration 8, loss = 0.86369423\n",
            "Iteration 9, loss = 0.80114997\n",
            "Iteration 10, loss = 0.75910184\n",
            "Iteration 11, loss = 0.72937963\n",
            "Iteration 12, loss = 0.71156873\n",
            "Iteration 13, loss = 0.70032750\n",
            "Iteration 14, loss = 0.69307091\n",
            "Iteration 15, loss = 0.68828170\n",
            "Iteration 16, loss = 0.68491844\n",
            "Iteration 17, loss = 0.68236051\n",
            "Iteration 18, loss = 0.68036233\n",
            "Iteration 19, loss = 0.67878268\n",
            "Iteration 20, loss = 0.67736382\n",
            "Iteration 21, loss = 0.67619506\n",
            "Iteration 22, loss = 0.67509480\n",
            "Iteration 23, loss = 0.67446270\n",
            "Iteration 24, loss = 0.67358303\n",
            "Iteration 25, loss = 0.67288849\n",
            "Iteration 26, loss = 0.67234055\n",
            "Iteration 27, loss = 0.67175291\n",
            "Iteration 28, loss = 0.67134337\n",
            "Iteration 29, loss = 0.67093017\n",
            "Iteration 30, loss = 0.67060705\n",
            "Iteration 31, loss = 0.67035054\n",
            "Iteration 32, loss = 0.67014898\n",
            "Iteration 33, loss = 0.66993985\n",
            "Iteration 34, loss = 0.66976054\n",
            "Iteration 35, loss = 0.66961921\n",
            "Iteration 36, loss = 0.66948097\n",
            "Iteration 37, loss = 0.66931138\n",
            "Iteration 38, loss = 0.66920157\n",
            "Iteration 39, loss = 0.66914366\n",
            "Iteration 40, loss = 0.66902908\n",
            "Iteration 41, loss = 0.66896875\n",
            "Iteration 42, loss = 0.66890445\n",
            "Iteration 43, loss = 0.66884251\n",
            "Iteration 44, loss = 0.66880946\n",
            "Iteration 45, loss = 0.66878901\n",
            "Iteration 46, loss = 0.66879552\n",
            "Iteration 47, loss = 0.66861887\n",
            "Iteration 48, loss = 0.66887035\n",
            "Iteration 49, loss = 0.66866546\n",
            "Iteration 50, loss = 0.66854356\n",
            "Iteration 51, loss = 0.66855215\n",
            "Iteration 52, loss = 0.66848232\n",
            "Iteration 53, loss = 0.66845989\n",
            "Iteration 54, loss = 0.66844039\n",
            "Iteration 55, loss = 0.66842688\n",
            "Iteration 56, loss = 0.66841210\n",
            "Iteration 57, loss = 0.66835175\n",
            "Iteration 58, loss = 0.66837502\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88341477\n",
            "Iteration 2, loss = 0.84567804\n",
            "Iteration 3, loss = 0.81563909\n",
            "Iteration 4, loss = 0.79262717\n",
            "Iteration 5, loss = 0.77420011\n",
            "Iteration 6, loss = 0.75945713\n",
            "Iteration 7, loss = 0.74749172\n",
            "Iteration 8, loss = 0.73763947\n",
            "Iteration 9, loss = 0.72932285\n",
            "Iteration 10, loss = 0.72210299\n",
            "Iteration 11, loss = 0.71625826\n",
            "Iteration 12, loss = 0.71100132\n",
            "Iteration 13, loss = 0.70655737\n",
            "Iteration 14, loss = 0.70275450\n",
            "Iteration 15, loss = 0.69936610\n",
            "Iteration 16, loss = 0.69643171\n",
            "Iteration 17, loss = 0.69369431\n",
            "Iteration 18, loss = 0.69145566\n",
            "Iteration 19, loss = 0.68955026\n",
            "Iteration 20, loss = 0.68756456\n",
            "Iteration 21, loss = 0.68589405\n",
            "Iteration 22, loss = 0.68439228\n",
            "Iteration 23, loss = 0.68311742\n",
            "Iteration 24, loss = 0.68196202\n",
            "Iteration 25, loss = 0.68085856\n",
            "Iteration 26, loss = 0.67981389\n",
            "Iteration 27, loss = 0.67902606\n",
            "Iteration 28, loss = 0.67825839\n",
            "Iteration 29, loss = 0.67743577\n",
            "Iteration 30, loss = 0.67677270\n",
            "Iteration 31, loss = 0.67612564\n",
            "Iteration 32, loss = 0.67555809\n",
            "Iteration 33, loss = 0.67509010\n",
            "Iteration 34, loss = 0.67456333\n",
            "Iteration 35, loss = 0.67415197\n",
            "Iteration 36, loss = 0.67368275\n",
            "Iteration 37, loss = 0.67330994\n",
            "Iteration 38, loss = 0.67303180\n",
            "Iteration 39, loss = 0.67265750\n",
            "Iteration 40, loss = 0.67237770\n",
            "Iteration 41, loss = 0.67205142\n",
            "Iteration 42, loss = 0.67183177\n",
            "Iteration 43, loss = 0.67153610\n",
            "Iteration 44, loss = 0.67130760\n",
            "Iteration 45, loss = 0.67103913\n",
            "Iteration 46, loss = 0.67088883\n",
            "Iteration 47, loss = 0.67065007\n",
            "Iteration 48, loss = 0.67044785\n",
            "Iteration 49, loss = 0.67026499\n",
            "Iteration 50, loss = 0.67016080\n",
            "Iteration 51, loss = 0.66992983\n",
            "Iteration 52, loss = 0.66980458\n",
            "Iteration 53, loss = 0.66975072\n",
            "Iteration 54, loss = 0.66946306\n",
            "Iteration 55, loss = 0.66931863\n",
            "Iteration 56, loss = 0.66916554\n",
            "Iteration 57, loss = 0.66913824\n",
            "Iteration 58, loss = 0.66899340\n",
            "Iteration 59, loss = 0.66887402\n",
            "Iteration 60, loss = 0.66877336\n",
            "Iteration 61, loss = 0.66876886\n",
            "Iteration 62, loss = 0.66861420\n",
            "Iteration 63, loss = 0.66851344\n",
            "Iteration 64, loss = 0.66860996\n",
            "Iteration 65, loss = 0.66849887\n",
            "Iteration 66, loss = 0.66832227\n",
            "Iteration 67, loss = 0.66825438\n",
            "Iteration 68, loss = 0.66824741\n",
            "Iteration 69, loss = 0.66810844\n",
            "Iteration 70, loss = 0.66806466\n",
            "Iteration 71, loss = 0.66800674\n",
            "Iteration 72, loss = 0.66789911\n",
            "Iteration 73, loss = 0.66785624\n",
            "Iteration 74, loss = 0.66784527\n",
            "Iteration 75, loss = 0.66771194\n",
            "Iteration 76, loss = 0.66758540\n",
            "Iteration 77, loss = 0.66773071\n",
            "Iteration 78, loss = 0.66761650\n",
            "Iteration 79, loss = 0.66753060\n",
            "Iteration 80, loss = 0.66745387\n",
            "Iteration 81, loss = 0.66750597\n",
            "Iteration 82, loss = 0.66748893\n",
            "Iteration 83, loss = 0.66751397\n",
            "Iteration 84, loss = 0.66734160\n",
            "Iteration 85, loss = 0.66724030\n",
            "Iteration 86, loss = 0.66721624\n",
            "Iteration 87, loss = 0.66716080\n",
            "Iteration 88, loss = 0.66714519\n",
            "Iteration 89, loss = 0.66710045\n",
            "Iteration 90, loss = 0.66702574\n",
            "Iteration 91, loss = 0.66702194\n",
            "Iteration 92, loss = 0.66709764\n",
            "Iteration 93, loss = 0.66690784\n",
            "Iteration 94, loss = 0.66693819\n",
            "Iteration 95, loss = 0.66700044\n",
            "Iteration 96, loss = 0.66691027\n",
            "Iteration 97, loss = 0.66685806\n",
            "Iteration 98, loss = 0.66679582\n",
            "Iteration 99, loss = 0.66669816\n",
            "Iteration 100, loss = 0.66669639\n",
            "Iteration 101, loss = 0.66667307\n",
            "Iteration 102, loss = 0.66659066\n",
            "Iteration 103, loss = 0.66659942\n",
            "Iteration 104, loss = 0.66656216\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70808588\n",
            "Iteration 2, loss = 0.68213773\n",
            "Iteration 3, loss = 0.67183957\n",
            "Iteration 4, loss = 0.66923604\n",
            "Iteration 5, loss = 0.66897157\n",
            "Iteration 6, loss = 0.66899211\n",
            "Iteration 7, loss = 0.66890306\n",
            "Iteration 8, loss = 0.66881076\n",
            "Iteration 9, loss = 0.66883766\n",
            "Iteration 10, loss = 0.66881192\n",
            "Iteration 11, loss = 0.66882939\n",
            "Iteration 12, loss = 0.66874256\n",
            "Iteration 13, loss = 0.66892816\n",
            "Iteration 14, loss = 0.66879245\n",
            "Iteration 15, loss = 0.66889464\n",
            "Iteration 16, loss = 0.66873634\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.60999185999186+-0.00017623634590646252\n",
            "Eficiencia durante la validaciÃ³n = 0.6101953601953602+-0.0010124007296567622\n",
            "f1-score  0.4633426727868033  +-  0.0026547837987329764\n",
            "Iteration 1, loss = 1.13424730\n",
            "Iteration 2, loss = 0.93655769\n",
            "Iteration 3, loss = 0.82005701\n",
            "Iteration 4, loss = 0.75060086\n",
            "Iteration 5, loss = 0.71271092\n",
            "Iteration 6, loss = 0.68927202\n",
            "Iteration 7, loss = 0.67709094\n",
            "Iteration 8, loss = 0.67166538\n",
            "Iteration 9, loss = 0.66920483\n",
            "Iteration 10, loss = 0.66885924\n",
            "Iteration 11, loss = 0.66821727\n",
            "Iteration 12, loss = 0.66819706\n",
            "Iteration 13, loss = 0.66816279\n",
            "Iteration 14, loss = 0.66813087\n",
            "Iteration 15, loss = 0.66822162\n",
            "Iteration 16, loss = 0.66804097\n",
            "Iteration 17, loss = 0.66812851\n",
            "Iteration 18, loss = 0.66809112\n",
            "Iteration 19, loss = 0.66828165\n",
            "Iteration 20, loss = 0.66825944\n",
            "Iteration 21, loss = 0.66806053\n",
            "Iteration 22, loss = 0.66794732\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.29316925\n",
            "Iteration 2, loss = 1.05229934\n",
            "Iteration 3, loss = 0.89054651\n",
            "Iteration 4, loss = 0.80801007\n",
            "Iteration 5, loss = 0.76590830\n",
            "Iteration 6, loss = 0.73817464\n",
            "Iteration 7, loss = 0.72125577\n",
            "Iteration 8, loss = 0.71059064\n",
            "Iteration 9, loss = 0.70215522\n",
            "Iteration 10, loss = 0.69655179\n",
            "Iteration 11, loss = 0.69178015\n",
            "Iteration 12, loss = 0.68811671\n",
            "Iteration 13, loss = 0.68532562\n",
            "Iteration 14, loss = 0.68287688\n",
            "Iteration 15, loss = 0.68078400\n",
            "Iteration 16, loss = 0.67931627\n",
            "Iteration 17, loss = 0.67774484\n",
            "Iteration 18, loss = 0.67647375\n",
            "Iteration 19, loss = 0.67546017\n",
            "Iteration 20, loss = 0.67465862\n",
            "Iteration 21, loss = 0.67392111\n",
            "Iteration 22, loss = 0.67312486\n",
            "Iteration 23, loss = 0.67253583\n",
            "Iteration 24, loss = 0.67212014\n",
            "Iteration 25, loss = 0.67150110\n",
            "Iteration 26, loss = 0.67108888\n",
            "Iteration 27, loss = 0.67077524\n",
            "Iteration 28, loss = 0.67050784\n",
            "Iteration 29, loss = 0.66997976\n",
            "Iteration 30, loss = 0.67006481\n",
            "Iteration 31, loss = 0.66978207\n",
            "Iteration 32, loss = 0.66954604\n",
            "Iteration 33, loss = 0.66916757\n",
            "Iteration 34, loss = 0.66896369\n",
            "Iteration 35, loss = 0.66873781\n",
            "Iteration 36, loss = 0.66859047\n",
            "Iteration 37, loss = 0.66883467\n",
            "Iteration 38, loss = 0.66831243\n",
            "Iteration 39, loss = 0.66810707\n",
            "Iteration 40, loss = 0.66798769\n",
            "Iteration 41, loss = 0.66812417\n",
            "Iteration 42, loss = 0.66816942\n",
            "Iteration 43, loss = 0.66780014\n",
            "Iteration 44, loss = 0.66784437\n",
            "Iteration 45, loss = 0.66845044\n",
            "Iteration 46, loss = 0.66814333\n",
            "Iteration 47, loss = 0.66792685\n",
            "Iteration 48, loss = 0.66744571\n",
            "Iteration 49, loss = 0.66764403\n",
            "Iteration 50, loss = 0.66738973\n",
            "Iteration 51, loss = 0.66767009\n",
            "Iteration 52, loss = 0.66737739\n",
            "Iteration 53, loss = 0.66728372\n",
            "Iteration 54, loss = 0.66733902\n",
            "Iteration 55, loss = 0.66727404\n",
            "Iteration 56, loss = 0.66730355\n",
            "Iteration 57, loss = 0.66726129\n",
            "Iteration 58, loss = 0.66732024\n",
            "Iteration 59, loss = 0.66787083\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.22245277\n",
            "Iteration 2, loss = 1.08053732\n",
            "Iteration 3, loss = 0.96882446\n",
            "Iteration 4, loss = 0.88552087\n",
            "Iteration 5, loss = 0.82098902\n",
            "Iteration 6, loss = 0.77414169\n",
            "Iteration 7, loss = 0.73869723\n",
            "Iteration 8, loss = 0.71378513\n",
            "Iteration 9, loss = 0.69774258\n",
            "Iteration 10, loss = 0.68675201\n",
            "Iteration 11, loss = 0.68033974\n",
            "Iteration 12, loss = 0.67615544\n",
            "Iteration 13, loss = 0.67379968\n",
            "Iteration 14, loss = 0.67207635\n",
            "Iteration 15, loss = 0.67119763\n",
            "Iteration 16, loss = 0.67021495\n",
            "Iteration 17, loss = 0.66966954\n",
            "Iteration 18, loss = 0.66942269\n",
            "Iteration 19, loss = 0.66906015\n",
            "Iteration 20, loss = 0.66882774\n",
            "Iteration 21, loss = 0.66864547\n",
            "Iteration 22, loss = 0.66847357\n",
            "Iteration 23, loss = 0.66840686\n",
            "Iteration 24, loss = 0.66824258\n",
            "Iteration 25, loss = 0.66813943\n",
            "Iteration 26, loss = 0.66800852\n",
            "Iteration 27, loss = 0.66788703\n",
            "Iteration 28, loss = 0.66780790\n",
            "Iteration 29, loss = 0.66777055\n",
            "Iteration 30, loss = 0.66783511\n",
            "Iteration 31, loss = 0.66764939\n",
            "Iteration 32, loss = 0.66760699\n",
            "Iteration 33, loss = 0.66747936\n",
            "Iteration 34, loss = 0.66744389\n",
            "Iteration 35, loss = 0.66732702\n",
            "Iteration 36, loss = 0.66730467\n",
            "Iteration 37, loss = 0.66732892\n",
            "Iteration 38, loss = 0.66723998\n",
            "Iteration 39, loss = 0.66707505\n",
            "Iteration 40, loss = 0.66706694\n",
            "Iteration 41, loss = 0.66697520\n",
            "Iteration 42, loss = 0.66695594\n",
            "Iteration 43, loss = 0.66696733\n",
            "Iteration 44, loss = 0.66674778\n",
            "Iteration 45, loss = 0.66686715\n",
            "Iteration 46, loss = 0.66702366\n",
            "Iteration 47, loss = 0.66673333\n",
            "Iteration 48, loss = 0.66661953\n",
            "Iteration 49, loss = 0.66657037\n",
            "Iteration 50, loss = 0.66656130\n",
            "Iteration 51, loss = 0.66639377\n",
            "Iteration 52, loss = 0.66642808\n",
            "Iteration 53, loss = 0.66664928\n",
            "Iteration 54, loss = 0.66646475\n",
            "Iteration 55, loss = 0.66628313\n",
            "Iteration 56, loss = 0.66632013\n",
            "Iteration 57, loss = 0.66622403\n",
            "Iteration 58, loss = 0.66614149\n",
            "Iteration 59, loss = 0.66620137\n",
            "Iteration 60, loss = 0.66604464\n",
            "Iteration 61, loss = 0.66624395\n",
            "Iteration 62, loss = 0.66598155\n",
            "Iteration 63, loss = 0.66618574\n",
            "Iteration 64, loss = 0.66586560\n",
            "Iteration 65, loss = 0.66605345\n",
            "Iteration 66, loss = 0.66591610\n",
            "Iteration 67, loss = 0.66577327\n",
            "Iteration 68, loss = 0.66584136\n",
            "Iteration 69, loss = 0.66601744\n",
            "Iteration 70, loss = 0.66585976\n",
            "Iteration 71, loss = 0.66574933\n",
            "Iteration 72, loss = 0.66583169\n",
            "Iteration 73, loss = 0.66574900\n",
            "Iteration 74, loss = 0.66590822\n",
            "Iteration 75, loss = 0.66570278\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.01403911\n",
            "Iteration 2, loss = 0.84829519\n",
            "Iteration 3, loss = 0.74562125\n",
            "Iteration 4, loss = 0.69411979\n",
            "Iteration 5, loss = 0.67651395\n",
            "Iteration 6, loss = 0.67093042\n",
            "Iteration 7, loss = 0.66954907\n",
            "Iteration 8, loss = 0.66915024\n",
            "Iteration 9, loss = 0.66873305\n",
            "Iteration 10, loss = 0.66856098\n",
            "Iteration 11, loss = 0.66840260\n",
            "Iteration 12, loss = 0.66836927\n",
            "Iteration 13, loss = 0.66830292\n",
            "Iteration 14, loss = 0.66827996\n",
            "Iteration 15, loss = 0.66849637\n",
            "Iteration 16, loss = 0.66831767\n",
            "Iteration 17, loss = 0.66825923\n",
            "Iteration 18, loss = 0.66814676\n",
            "Iteration 19, loss = 0.66811645\n",
            "Iteration 20, loss = 0.66826310\n",
            "Iteration 21, loss = 0.66819301\n",
            "Iteration 22, loss = 0.66815425\n",
            "Iteration 23, loss = 0.66818714\n",
            "Iteration 24, loss = 0.66814775\n",
            "Iteration 25, loss = 0.66836112\n",
            "Iteration 26, loss = 0.66813992\n",
            "Iteration 27, loss = 0.66828158\n",
            "Iteration 28, loss = 0.66813362\n",
            "Iteration 29, loss = 0.66815247\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6109076109076109+-0.0014389637386783604\n",
            "Eficiencia durante la validaciÃ³n = 0.6105006105006106+-0.0014954149833841537\n",
            "f1-score  0.46349033304069376  +-  0.002904242941163179\n",
            "Iteration 1, loss = 1.44978372\n",
            "Iteration 2, loss = 0.93288190\n",
            "Iteration 3, loss = 0.72147711\n",
            "Iteration 4, loss = 0.67673087\n",
            "Iteration 5, loss = 0.67273720\n",
            "Iteration 6, loss = 0.67091602\n",
            "Iteration 7, loss = 0.66949274\n",
            "Iteration 8, loss = 0.66905980\n",
            "Iteration 9, loss = 0.66866648\n",
            "Iteration 10, loss = 0.66867804\n",
            "Iteration 11, loss = 0.66849426\n",
            "Iteration 12, loss = 0.66856132\n",
            "Iteration 13, loss = 0.66812839\n",
            "Iteration 14, loss = 0.66813250\n",
            "Iteration 15, loss = 0.66769385\n",
            "Iteration 16, loss = 0.66797224\n",
            "Iteration 17, loss = 0.66804889\n",
            "Iteration 18, loss = 0.66785840\n",
            "Iteration 19, loss = 0.66758243\n",
            "Iteration 20, loss = 0.66752966\n",
            "Iteration 21, loss = 0.66810968\n",
            "Iteration 22, loss = 0.66775542\n",
            "Iteration 23, loss = 0.66765000\n",
            "Iteration 24, loss = 0.66784084\n",
            "Iteration 25, loss = 0.66759495\n",
            "Iteration 26, loss = 0.66783483\n",
            "Iteration 27, loss = 0.66793646\n",
            "Iteration 28, loss = 0.66772350\n",
            "Iteration 29, loss = 0.66755252\n",
            "Iteration 30, loss = 0.66778453\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70018375\n",
            "Iteration 2, loss = 0.67506374\n",
            "Iteration 3, loss = 0.67114076\n",
            "Iteration 4, loss = 0.66886031\n",
            "Iteration 5, loss = 0.66854341\n",
            "Iteration 6, loss = 0.66849723\n",
            "Iteration 7, loss = 0.66821936\n",
            "Iteration 8, loss = 0.66772918\n",
            "Iteration 9, loss = 0.66792957\n",
            "Iteration 10, loss = 0.66842713\n",
            "Iteration 11, loss = 0.66734571\n",
            "Iteration 12, loss = 0.66832802\n",
            "Iteration 13, loss = 0.66843492\n",
            "Iteration 14, loss = 0.66744780\n",
            "Iteration 15, loss = 0.66759024\n",
            "Iteration 16, loss = 0.66762636\n",
            "Iteration 17, loss = 0.66726943\n",
            "Iteration 18, loss = 0.66710940\n",
            "Iteration 19, loss = 0.66735560\n",
            "Iteration 20, loss = 0.66808549\n",
            "Iteration 21, loss = 0.66795177\n",
            "Iteration 22, loss = 0.66739218\n",
            "Iteration 23, loss = 0.66768246\n",
            "Iteration 24, loss = 0.66737149\n",
            "Iteration 25, loss = 0.66798144\n",
            "Iteration 26, loss = 0.66827851\n",
            "Iteration 27, loss = 0.66733572\n",
            "Iteration 28, loss = 0.66799879\n",
            "Iteration 29, loss = 0.66767794\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.91509449\n",
            "Iteration 2, loss = 0.74400945\n",
            "Iteration 3, loss = 0.70171912\n",
            "Iteration 4, loss = 0.68474052\n",
            "Iteration 5, loss = 0.67543410\n",
            "Iteration 6, loss = 0.67239444\n",
            "Iteration 7, loss = 0.67003259\n",
            "Iteration 8, loss = 0.66865965\n",
            "Iteration 9, loss = 0.66732299\n",
            "Iteration 10, loss = 0.66725983\n",
            "Iteration 11, loss = 0.66651679\n",
            "Iteration 12, loss = 0.66693252\n",
            "Iteration 13, loss = 0.66630842\n",
            "Iteration 14, loss = 0.66628486\n",
            "Iteration 15, loss = 0.66588533\n",
            "Iteration 16, loss = 0.66600145\n",
            "Iteration 17, loss = 0.66582737\n",
            "Iteration 18, loss = 0.66611227\n",
            "Iteration 19, loss = 0.66595605\n",
            "Iteration 20, loss = 0.66608928\n",
            "Iteration 21, loss = 0.66616270\n",
            "Iteration 22, loss = 0.66593702\n",
            "Iteration 23, loss = 0.66577025\n",
            "Iteration 24, loss = 0.66588319\n",
            "Iteration 25, loss = 0.66608908\n",
            "Iteration 26, loss = 0.66666349\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71624271\n",
            "Iteration 2, loss = 0.68008956\n",
            "Iteration 3, loss = 0.67241353\n",
            "Iteration 4, loss = 0.66956751\n",
            "Iteration 5, loss = 0.66893426\n",
            "Iteration 6, loss = 0.66803962\n",
            "Iteration 7, loss = 0.66796333\n",
            "Iteration 8, loss = 0.66724942\n",
            "Iteration 9, loss = 0.66764646\n",
            "Iteration 10, loss = 0.66783397\n",
            "Iteration 11, loss = 0.66700185\n",
            "Iteration 12, loss = 0.66766360\n",
            "Iteration 13, loss = 0.66786822\n",
            "Iteration 14, loss = 0.66806946\n",
            "Iteration 15, loss = 0.66685184\n",
            "Iteration 16, loss = 0.66790419\n",
            "Iteration 17, loss = 0.66819512\n",
            "Iteration 18, loss = 0.66731639\n",
            "Iteration 19, loss = 0.66732878\n",
            "Iteration 20, loss = 0.66777093\n",
            "Iteration 21, loss = 0.66738769\n",
            "Iteration 22, loss = 0.66705018\n",
            "Iteration 23, loss = 0.66788651\n",
            "Iteration 24, loss = 0.66692182\n",
            "Iteration 25, loss = 0.66733150\n",
            "Iteration 26, loss = 0.66736673\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6115181115181115+-0.0014815038439724114\n",
            "Eficiencia durante la validaciÃ³n = 0.6105006105006104+-0.002114836150877791\n",
            "f1-score  0.4640356316969368  +-  0.004132832467887253\n",
            "Iteration 1, loss = 0.75971320\n",
            "Iteration 2, loss = 0.67892686\n",
            "Iteration 3, loss = 0.66999729\n",
            "Iteration 4, loss = 0.66946444\n",
            "Iteration 5, loss = 0.67019896\n",
            "Iteration 6, loss = 0.66852137\n",
            "Iteration 7, loss = 0.66783432\n",
            "Iteration 8, loss = 0.66955307\n",
            "Iteration 9, loss = 0.66968880\n",
            "Iteration 10, loss = 0.66900772\n",
            "Iteration 11, loss = 0.66827066\n",
            "Iteration 12, loss = 0.66861061\n",
            "Iteration 13, loss = 0.66898406\n",
            "Iteration 14, loss = 0.66992059\n",
            "Iteration 15, loss = 0.67178556\n",
            "Iteration 16, loss = 0.67023955\n",
            "Iteration 17, loss = 0.66905169\n",
            "Iteration 18, loss = 0.67072808\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70513034\n",
            "Iteration 2, loss = 0.67372450\n",
            "Iteration 3, loss = 0.67066736\n",
            "Iteration 4, loss = 0.67068355\n",
            "Iteration 5, loss = 0.66993667\n",
            "Iteration 6, loss = 0.66928448\n",
            "Iteration 7, loss = 0.66946667\n",
            "Iteration 8, loss = 0.66847751\n",
            "Iteration 9, loss = 0.66935487\n",
            "Iteration 10, loss = 0.67066599\n",
            "Iteration 11, loss = 0.67089660\n",
            "Iteration 12, loss = 0.67071210\n",
            "Iteration 13, loss = 0.66962402\n",
            "Iteration 14, loss = 0.67005182\n",
            "Iteration 15, loss = 0.67047020\n",
            "Iteration 16, loss = 0.67115852\n",
            "Iteration 17, loss = 0.67025136\n",
            "Iteration 18, loss = 0.67047211\n",
            "Iteration 19, loss = 0.66775964\n",
            "Iteration 20, loss = 0.66838821\n",
            "Iteration 21, loss = 0.66929118\n",
            "Iteration 22, loss = 0.66955656\n",
            "Iteration 23, loss = 0.66962587\n",
            "Iteration 24, loss = 0.66990955\n",
            "Iteration 25, loss = 0.66884549\n",
            "Iteration 26, loss = 0.66780135\n",
            "Iteration 27, loss = 0.66799397\n",
            "Iteration 28, loss = 0.67069873\n",
            "Iteration 29, loss = 0.66926850\n",
            "Iteration 30, loss = 0.67040689\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68807378\n",
            "Iteration 2, loss = 0.66988687\n",
            "Iteration 3, loss = 0.67009146\n",
            "Iteration 4, loss = 0.66818940\n",
            "Iteration 5, loss = 0.66757261\n",
            "Iteration 6, loss = 0.66883618\n",
            "Iteration 7, loss = 0.66832854\n",
            "Iteration 8, loss = 0.66848448\n",
            "Iteration 9, loss = 0.66856632\n",
            "Iteration 10, loss = 0.66834703\n",
            "Iteration 11, loss = 0.66797692\n",
            "Iteration 12, loss = 0.67083254\n",
            "Iteration 13, loss = 0.66683338\n",
            "Iteration 14, loss = 0.66928846\n",
            "Iteration 15, loss = 0.66742563\n",
            "Iteration 16, loss = 0.66653052\n",
            "Iteration 17, loss = 0.66792733\n",
            "Iteration 18, loss = 0.66724021\n",
            "Iteration 19, loss = 0.66859063\n",
            "Iteration 20, loss = 0.66927711\n",
            "Iteration 21, loss = 0.66768298\n",
            "Iteration 22, loss = 0.66673607\n",
            "Iteration 23, loss = 0.66682092\n",
            "Iteration 24, loss = 0.66666055\n",
            "Iteration 25, loss = 0.66830069\n",
            "Iteration 26, loss = 0.66759523\n",
            "Iteration 27, loss = 0.66732332\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71856805\n",
            "Iteration 2, loss = 0.67208675\n",
            "Iteration 3, loss = 0.66902335\n",
            "Iteration 4, loss = 0.67015646\n",
            "Iteration 5, loss = 0.66707112\n",
            "Iteration 6, loss = 0.66918060\n",
            "Iteration 7, loss = 0.66741880\n",
            "Iteration 8, loss = 0.66845958\n",
            "Iteration 9, loss = 0.66825570\n",
            "Iteration 10, loss = 0.66893899\n",
            "Iteration 11, loss = 0.66834473\n",
            "Iteration 12, loss = 0.66855429\n",
            "Iteration 13, loss = 0.66734021\n",
            "Iteration 14, loss = 0.66824975\n",
            "Iteration 15, loss = 0.66722141\n",
            "Iteration 16, loss = 0.66827195\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.613044363044363+-0.0028105875695834588\n",
            "Eficiencia durante la validaciÃ³n = 0.6117216117216118+-0.0025901347296210966\n",
            "f1-score  0.46732232407131274  +-  0.006274752834710863\n",
            "Iteration 1, loss = 0.71212543\n",
            "Iteration 2, loss = 0.67964509\n",
            "Iteration 3, loss = 0.67090617\n",
            "Iteration 4, loss = 0.67108951\n",
            "Iteration 5, loss = 0.66892529\n",
            "Iteration 6, loss = 0.66872670\n",
            "Iteration 7, loss = 0.67045324\n",
            "Iteration 8, loss = 0.67537048\n",
            "Iteration 9, loss = 0.67179082\n",
            "Iteration 10, loss = 0.66875633\n",
            "Iteration 11, loss = 0.66996884\n",
            "Iteration 12, loss = 0.66990481\n",
            "Iteration 13, loss = 0.67017029\n",
            "Iteration 14, loss = 0.66964639\n",
            "Iteration 15, loss = 0.66980368\n",
            "Iteration 16, loss = 0.66865431\n",
            "Iteration 17, loss = 0.66843711\n",
            "Iteration 18, loss = 0.66827413\n",
            "Iteration 19, loss = 0.66838784\n",
            "Iteration 20, loss = 0.67033257\n",
            "Iteration 21, loss = 0.67027223\n",
            "Iteration 22, loss = 0.67096816\n",
            "Iteration 23, loss = 0.67062091\n",
            "Iteration 24, loss = 0.66956166\n",
            "Iteration 25, loss = 0.66921248\n",
            "Iteration 26, loss = 0.67055770\n",
            "Iteration 27, loss = 0.66922434\n",
            "Iteration 28, loss = 0.67023693\n",
            "Iteration 29, loss = 0.66914993\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.76083427\n",
            "Iteration 2, loss = 0.67622618\n",
            "Iteration 3, loss = 0.67104540\n",
            "Iteration 4, loss = 0.67004723\n",
            "Iteration 5, loss = 0.66911380\n",
            "Iteration 6, loss = 0.66934328\n",
            "Iteration 7, loss = 0.66937123\n",
            "Iteration 8, loss = 0.67030774\n",
            "Iteration 9, loss = 0.67168162\n",
            "Iteration 10, loss = 0.66926105\n",
            "Iteration 11, loss = 0.66892432\n",
            "Iteration 12, loss = 0.66835321\n",
            "Iteration 13, loss = 0.66970971\n",
            "Iteration 14, loss = 0.66925636\n",
            "Iteration 15, loss = 0.67041389\n",
            "Iteration 16, loss = 0.66926755\n",
            "Iteration 17, loss = 0.66860714\n",
            "Iteration 18, loss = 0.66914194\n",
            "Iteration 19, loss = 0.66990501\n",
            "Iteration 20, loss = 0.66848331\n",
            "Iteration 21, loss = 0.66844118\n",
            "Iteration 22, loss = 0.66953545\n",
            "Iteration 23, loss = 0.66872817\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70222717\n",
            "Iteration 2, loss = 0.67361607\n",
            "Iteration 3, loss = 0.67374092\n",
            "Iteration 4, loss = 0.67023440\n",
            "Iteration 5, loss = 0.67016910\n",
            "Iteration 6, loss = 0.66730815\n",
            "Iteration 7, loss = 0.66747204\n",
            "Iteration 8, loss = 0.66756652\n",
            "Iteration 9, loss = 0.66912479\n",
            "Iteration 10, loss = 0.66816219\n",
            "Iteration 11, loss = 0.66710184\n",
            "Iteration 12, loss = 0.66695702\n",
            "Iteration 13, loss = 0.66815229\n",
            "Iteration 14, loss = 0.66900975\n",
            "Iteration 15, loss = 0.66687168\n",
            "Iteration 16, loss = 0.66707905\n",
            "Iteration 17, loss = 0.66754880\n",
            "Iteration 18, loss = 0.66695133\n",
            "Iteration 19, loss = 0.67092074\n",
            "Iteration 20, loss = 0.66806325\n",
            "Iteration 21, loss = 0.66730136\n",
            "Iteration 22, loss = 0.66758174\n",
            "Iteration 23, loss = 0.66671001\n",
            "Iteration 24, loss = 0.66756482\n",
            "Iteration 25, loss = 0.66657519\n",
            "Iteration 26, loss = 0.66807574\n",
            "Iteration 27, loss = 0.66793892\n",
            "Iteration 28, loss = 0.66722598\n",
            "Iteration 29, loss = 0.66726167\n",
            "Iteration 30, loss = 0.66669363\n",
            "Iteration 31, loss = 0.66652570\n",
            "Iteration 32, loss = 0.66673838\n",
            "Iteration 33, loss = 0.66660708\n",
            "Iteration 34, loss = 0.66590479\n",
            "Iteration 35, loss = 0.66665112\n",
            "Iteration 36, loss = 0.66655932\n",
            "Iteration 37, loss = 0.66816830\n",
            "Iteration 38, loss = 0.66731908\n",
            "Iteration 39, loss = 0.66608350\n",
            "Iteration 40, loss = 0.66684216\n",
            "Iteration 41, loss = 0.66670476\n",
            "Iteration 42, loss = 0.66656619\n",
            "Iteration 43, loss = 0.66611481\n",
            "Iteration 44, loss = 0.66637195\n",
            "Iteration 45, loss = 0.66679486\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69870813\n",
            "Iteration 2, loss = 0.67396059\n",
            "Iteration 3, loss = 0.67016410\n",
            "Iteration 4, loss = 0.66920843\n",
            "Iteration 5, loss = 0.66855425\n",
            "Iteration 6, loss = 0.67031828\n",
            "Iteration 7, loss = 0.66971188\n",
            "Iteration 8, loss = 0.66753760\n",
            "Iteration 9, loss = 0.66906042\n",
            "Iteration 10, loss = 0.66829099\n",
            "Iteration 11, loss = 0.67121399\n",
            "Iteration 12, loss = 0.66720618\n",
            "Iteration 13, loss = 0.66846674\n",
            "Iteration 14, loss = 0.66885887\n",
            "Iteration 15, loss = 0.67035837\n",
            "Iteration 16, loss = 0.66890216\n",
            "Iteration 17, loss = 0.66903116\n",
            "Iteration 18, loss = 0.66832336\n",
            "Iteration 19, loss = 0.66848014\n",
            "Iteration 20, loss = 0.66763034\n",
            "Iteration 21, loss = 0.66837629\n",
            "Iteration 22, loss = 0.66843626\n",
            "Iteration 23, loss = 0.67164605\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6121286121286121+-0.0023905861059158864\n",
            "Eficiencia durante la validaciÃ³n = 0.6105006105006106+-0.0014954149833841537\n",
            "f1-score  0.4656628062573869  +-  0.004236176401888313\n",
            "Iteration 1, loss = 0.82862332\n",
            "Iteration 2, loss = 0.78688557\n",
            "Iteration 3, loss = 0.75201873\n",
            "Iteration 4, loss = 0.72465828\n",
            "Iteration 5, loss = 0.70486685\n",
            "Iteration 6, loss = 0.69214565\n",
            "Iteration 7, loss = 0.68535532\n",
            "Iteration 8, loss = 0.68030327\n",
            "Iteration 9, loss = 0.67778288\n",
            "Iteration 10, loss = 0.67585248\n",
            "Iteration 11, loss = 0.67439509\n",
            "Iteration 12, loss = 0.67343534\n",
            "Iteration 13, loss = 0.67264226\n",
            "Iteration 14, loss = 0.67195376\n",
            "Iteration 15, loss = 0.67137671\n",
            "Iteration 16, loss = 0.67084585\n",
            "Iteration 17, loss = 0.67040577\n",
            "Iteration 18, loss = 0.67008985\n",
            "Iteration 19, loss = 0.66999288\n",
            "Iteration 20, loss = 0.66964434\n",
            "Iteration 21, loss = 0.66936284\n",
            "Iteration 22, loss = 0.66923944\n",
            "Iteration 23, loss = 0.66905651\n",
            "Iteration 24, loss = 0.66888855\n",
            "Iteration 25, loss = 0.66870581\n",
            "Iteration 26, loss = 0.66858585\n",
            "Iteration 27, loss = 0.66850487\n",
            "Iteration 28, loss = 0.66844953\n",
            "Iteration 29, loss = 0.66827715\n",
            "Iteration 30, loss = 0.66824274\n",
            "Iteration 31, loss = 0.66819765\n",
            "Iteration 32, loss = 0.66810387\n",
            "Iteration 33, loss = 0.66798483\n",
            "Iteration 34, loss = 0.66801720\n",
            "Iteration 35, loss = 0.66794793\n",
            "Iteration 36, loss = 0.66780307\n",
            "Iteration 37, loss = 0.66776057\n",
            "Iteration 38, loss = 0.66771262\n",
            "Iteration 39, loss = 0.66763510\n",
            "Iteration 40, loss = 0.66767999\n",
            "Iteration 41, loss = 0.66754281\n",
            "Iteration 42, loss = 0.66753048\n",
            "Iteration 43, loss = 0.66748892\n",
            "Iteration 44, loss = 0.66757663\n",
            "Iteration 45, loss = 0.66744482\n",
            "Iteration 46, loss = 0.66739598\n",
            "Iteration 47, loss = 0.66738331\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.15837460\n",
            "Iteration 2, loss = 1.10217219\n",
            "Iteration 3, loss = 1.05564583\n",
            "Iteration 4, loss = 1.01572930\n",
            "Iteration 5, loss = 0.98069152\n",
            "Iteration 6, loss = 0.94922480\n",
            "Iteration 7, loss = 0.92038377\n",
            "Iteration 8, loss = 0.89380276\n",
            "Iteration 9, loss = 0.86877472\n",
            "Iteration 10, loss = 0.84522157\n",
            "Iteration 11, loss = 0.82296400\n",
            "Iteration 12, loss = 0.80135426\n",
            "Iteration 13, loss = 0.78105353\n",
            "Iteration 14, loss = 0.76162077\n",
            "Iteration 15, loss = 0.74363006\n",
            "Iteration 16, loss = 0.72685870\n",
            "Iteration 17, loss = 0.71282859\n",
            "Iteration 18, loss = 0.70000264\n",
            "Iteration 19, loss = 0.69003562\n",
            "Iteration 20, loss = 0.68219286\n",
            "Iteration 21, loss = 0.67695612\n",
            "Iteration 22, loss = 0.67347049\n",
            "Iteration 23, loss = 0.67138230\n",
            "Iteration 24, loss = 0.67025237\n",
            "Iteration 25, loss = 0.66941409\n",
            "Iteration 26, loss = 0.66918454\n",
            "Iteration 27, loss = 0.66874514\n",
            "Iteration 28, loss = 0.66862629\n",
            "Iteration 29, loss = 0.66852894\n",
            "Iteration 30, loss = 0.66842433\n",
            "Iteration 31, loss = 0.66835655\n",
            "Iteration 32, loss = 0.66829101\n",
            "Iteration 33, loss = 0.66823970\n",
            "Iteration 34, loss = 0.66823426\n",
            "Iteration 35, loss = 0.66816137\n",
            "Iteration 36, loss = 0.66814545\n",
            "Iteration 37, loss = 0.66804309\n",
            "Iteration 38, loss = 0.66818259\n",
            "Iteration 39, loss = 0.66796083\n",
            "Iteration 40, loss = 0.66795055\n",
            "Iteration 41, loss = 0.66791795\n",
            "Iteration 42, loss = 0.66796070\n",
            "Iteration 43, loss = 0.66786329\n",
            "Iteration 44, loss = 0.66787296\n",
            "Iteration 45, loss = 0.66781690\n",
            "Iteration 46, loss = 0.66777332\n",
            "Iteration 47, loss = 0.66779822\n",
            "Iteration 48, loss = 0.66778492\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66980444\n",
            "Iteration 2, loss = 0.66942647\n",
            "Iteration 3, loss = 0.66923661\n",
            "Iteration 4, loss = 0.66916074\n",
            "Iteration 5, loss = 0.66908596\n",
            "Iteration 6, loss = 0.66902360\n",
            "Iteration 7, loss = 0.66896873\n",
            "Iteration 8, loss = 0.66895477\n",
            "Iteration 9, loss = 0.66888867\n",
            "Iteration 10, loss = 0.66888481\n",
            "Iteration 11, loss = 0.66885187\n",
            "Iteration 12, loss = 0.66883986\n",
            "Iteration 13, loss = 0.66884394\n",
            "Iteration 14, loss = 0.66881495\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.72379089\n",
            "Iteration 2, loss = 0.70707170\n",
            "Iteration 3, loss = 0.69544655\n",
            "Iteration 4, loss = 0.68775792\n",
            "Iteration 5, loss = 0.68245240\n",
            "Iteration 6, loss = 0.67862532\n",
            "Iteration 7, loss = 0.67641305\n",
            "Iteration 8, loss = 0.67444621\n",
            "Iteration 9, loss = 0.67359035\n",
            "Iteration 10, loss = 0.67280671\n",
            "Iteration 11, loss = 0.67225794\n",
            "Iteration 12, loss = 0.67188006\n",
            "Iteration 13, loss = 0.67150011\n",
            "Iteration 14, loss = 0.67132188\n",
            "Iteration 15, loss = 0.67102085\n",
            "Iteration 16, loss = 0.67082733\n",
            "Iteration 17, loss = 0.67062651\n",
            "Iteration 18, loss = 0.67053699\n",
            "Iteration 19, loss = 0.67046451\n",
            "Iteration 20, loss = 0.67027221\n",
            "Iteration 21, loss = 0.67018343\n",
            "Iteration 22, loss = 0.67009351\n",
            "Iteration 23, loss = 0.67000104\n",
            "Iteration 24, loss = 0.66991572\n",
            "Iteration 25, loss = 0.66984972\n",
            "Iteration 26, loss = 0.66980083\n",
            "Iteration 27, loss = 0.66974012\n",
            "Iteration 28, loss = 0.66971917\n",
            "Iteration 29, loss = 0.66964150\n",
            "Iteration 30, loss = 0.66961316\n",
            "Iteration 31, loss = 0.66954732\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.0003524726918129731\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Iteration 1, loss = 1.41392879\n",
            "Iteration 2, loss = 1.17146454\n",
            "Iteration 3, loss = 1.00632442\n",
            "Iteration 4, loss = 0.89845712\n",
            "Iteration 5, loss = 0.82234394\n",
            "Iteration 6, loss = 0.77193324\n",
            "Iteration 7, loss = 0.73738361\n",
            "Iteration 8, loss = 0.71554341\n",
            "Iteration 9, loss = 0.70145326\n",
            "Iteration 10, loss = 0.69131007\n",
            "Iteration 11, loss = 0.68536462\n",
            "Iteration 12, loss = 0.68095515\n",
            "Iteration 13, loss = 0.67746348\n",
            "Iteration 14, loss = 0.67523371\n",
            "Iteration 15, loss = 0.67362640\n",
            "Iteration 16, loss = 0.67237365\n",
            "Iteration 17, loss = 0.67118086\n",
            "Iteration 18, loss = 0.67043826\n",
            "Iteration 19, loss = 0.66996852\n",
            "Iteration 20, loss = 0.66931742\n",
            "Iteration 21, loss = 0.66897363\n",
            "Iteration 22, loss = 0.66866724\n",
            "Iteration 23, loss = 0.66858674\n",
            "Iteration 24, loss = 0.66814590\n",
            "Iteration 25, loss = 0.66800572\n",
            "Iteration 26, loss = 0.66786148\n",
            "Iteration 27, loss = 0.66776508\n",
            "Iteration 28, loss = 0.66759102\n",
            "Iteration 29, loss = 0.66742909\n",
            "Iteration 30, loss = 0.66742714\n",
            "Iteration 31, loss = 0.66739398\n",
            "Iteration 32, loss = 0.66725233\n",
            "Iteration 33, loss = 0.66716748\n",
            "Iteration 34, loss = 0.66725187\n",
            "Iteration 35, loss = 0.66720661\n",
            "Iteration 36, loss = 0.66724705\n",
            "Iteration 37, loss = 0.66744635\n",
            "Iteration 38, loss = 0.66717788\n",
            "Iteration 39, loss = 0.66720711\n",
            "Iteration 40, loss = 0.66726691\n",
            "Iteration 41, loss = 0.66714917\n",
            "Iteration 42, loss = 0.66712833\n",
            "Iteration 43, loss = 0.66713030\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37239298\n",
            "Iteration 2, loss = 1.12563464\n",
            "Iteration 3, loss = 0.95273293\n",
            "Iteration 4, loss = 0.84693233\n",
            "Iteration 5, loss = 0.78367258\n",
            "Iteration 6, loss = 0.74354318\n",
            "Iteration 7, loss = 0.72033307\n",
            "Iteration 8, loss = 0.70507047\n",
            "Iteration 9, loss = 0.69565806\n",
            "Iteration 10, loss = 0.68907107\n",
            "Iteration 11, loss = 0.68430341\n",
            "Iteration 12, loss = 0.68097902\n",
            "Iteration 13, loss = 0.67858464\n",
            "Iteration 14, loss = 0.67637309\n",
            "Iteration 15, loss = 0.67493492\n",
            "Iteration 16, loss = 0.67369183\n",
            "Iteration 17, loss = 0.67278007\n",
            "Iteration 18, loss = 0.67201995\n",
            "Iteration 19, loss = 0.67130701\n",
            "Iteration 20, loss = 0.67078666\n",
            "Iteration 21, loss = 0.67030447\n",
            "Iteration 22, loss = 0.66977696\n",
            "Iteration 23, loss = 0.66948091\n",
            "Iteration 24, loss = 0.66929514\n",
            "Iteration 25, loss = 0.66904465\n",
            "Iteration 26, loss = 0.66877767\n",
            "Iteration 27, loss = 0.66867204\n",
            "Iteration 28, loss = 0.66849905\n",
            "Iteration 29, loss = 0.66836081\n",
            "Iteration 30, loss = 0.66824238\n",
            "Iteration 31, loss = 0.66814329\n",
            "Iteration 32, loss = 0.66800291\n",
            "Iteration 33, loss = 0.66789815\n",
            "Iteration 34, loss = 0.66782526\n",
            "Iteration 35, loss = 0.66775443\n",
            "Iteration 36, loss = 0.66767539\n",
            "Iteration 37, loss = 0.66764072\n",
            "Iteration 38, loss = 0.66759463\n",
            "Iteration 39, loss = 0.66750942\n",
            "Iteration 40, loss = 0.66751983\n",
            "Iteration 41, loss = 0.66752892\n",
            "Iteration 42, loss = 0.66745427\n",
            "Iteration 43, loss = 0.66734986\n",
            "Iteration 44, loss = 0.66737392\n",
            "Iteration 45, loss = 0.66743432\n",
            "Iteration 46, loss = 0.66734103\n",
            "Iteration 47, loss = 0.66731753\n",
            "Iteration 48, loss = 0.66728805\n",
            "Iteration 49, loss = 0.66726815\n",
            "Iteration 50, loss = 0.66733666\n",
            "Iteration 51, loss = 0.66721325\n",
            "Iteration 52, loss = 0.66717865\n",
            "Iteration 53, loss = 0.66719630\n",
            "Iteration 54, loss = 0.66724349\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.10441382\n",
            "Iteration 2, loss = 0.91738013\n",
            "Iteration 3, loss = 0.79815230\n",
            "Iteration 4, loss = 0.73550991\n",
            "Iteration 5, loss = 0.70470239\n",
            "Iteration 6, loss = 0.69269167\n",
            "Iteration 7, loss = 0.68785978\n",
            "Iteration 8, loss = 0.68536920\n",
            "Iteration 9, loss = 0.68311915\n",
            "Iteration 10, loss = 0.68142710\n",
            "Iteration 11, loss = 0.67998310\n",
            "Iteration 12, loss = 0.67868426\n",
            "Iteration 13, loss = 0.67764841\n",
            "Iteration 14, loss = 0.67682430\n",
            "Iteration 15, loss = 0.67602849\n",
            "Iteration 16, loss = 0.67509006\n",
            "Iteration 17, loss = 0.67459859\n",
            "Iteration 18, loss = 0.67394248\n",
            "Iteration 19, loss = 0.67332526\n",
            "Iteration 20, loss = 0.67278893\n",
            "Iteration 21, loss = 0.67237173\n",
            "Iteration 22, loss = 0.67210148\n",
            "Iteration 23, loss = 0.67176471\n",
            "Iteration 24, loss = 0.67145150\n",
            "Iteration 25, loss = 0.67125086\n",
            "Iteration 26, loss = 0.67101327\n",
            "Iteration 27, loss = 0.67083635\n",
            "Iteration 28, loss = 0.67057226\n",
            "Iteration 29, loss = 0.67056295\n",
            "Iteration 30, loss = 0.67027517\n",
            "Iteration 31, loss = 0.67024965\n",
            "Iteration 32, loss = 0.66972211\n",
            "Iteration 33, loss = 0.66962918\n",
            "Iteration 34, loss = 0.66958454\n",
            "Iteration 35, loss = 0.66939900\n",
            "Iteration 36, loss = 0.66952646\n",
            "Iteration 37, loss = 0.66918628\n",
            "Iteration 38, loss = 0.66909796\n",
            "Iteration 39, loss = 0.66899515\n",
            "Iteration 40, loss = 0.66919287\n",
            "Iteration 41, loss = 0.66903550\n",
            "Iteration 42, loss = 0.66887957\n",
            "Iteration 43, loss = 0.66874923\n",
            "Iteration 44, loss = 0.66866578\n",
            "Iteration 45, loss = 0.66850090\n",
            "Iteration 46, loss = 0.66871339\n",
            "Iteration 47, loss = 0.66855175\n",
            "Iteration 48, loss = 0.66837860\n",
            "Iteration 49, loss = 0.66829934\n",
            "Iteration 50, loss = 0.66847779\n",
            "Iteration 51, loss = 0.66812309\n",
            "Iteration 52, loss = 0.66824563\n",
            "Iteration 53, loss = 0.66823854\n",
            "Iteration 54, loss = 0.66826907\n",
            "Iteration 55, loss = 0.66810326\n",
            "Iteration 56, loss = 0.66813779\n",
            "Iteration 57, loss = 0.66802895\n",
            "Iteration 58, loss = 0.66798955\n",
            "Iteration 59, loss = 0.66781967\n",
            "Iteration 60, loss = 0.66864325\n",
            "Iteration 61, loss = 0.66804933\n",
            "Iteration 62, loss = 0.66798351\n",
            "Iteration 63, loss = 0.66776897\n",
            "Iteration 64, loss = 0.66772254\n",
            "Iteration 65, loss = 0.66768633\n",
            "Iteration 66, loss = 0.66763218\n",
            "Iteration 67, loss = 0.66781597\n",
            "Iteration 68, loss = 0.66774617\n",
            "Iteration 69, loss = 0.66778075\n",
            "Iteration 70, loss = 0.66764530\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.96793967\n",
            "Iteration 2, loss = 0.75207475\n",
            "Iteration 3, loss = 0.68784555\n",
            "Iteration 4, loss = 0.67954872\n",
            "Iteration 5, loss = 0.67441347\n",
            "Iteration 6, loss = 0.67188203\n",
            "Iteration 7, loss = 0.67049823\n",
            "Iteration 8, loss = 0.66995198\n",
            "Iteration 9, loss = 0.66932672\n",
            "Iteration 10, loss = 0.66915065\n",
            "Iteration 11, loss = 0.66922845\n",
            "Iteration 12, loss = 0.66891720\n",
            "Iteration 13, loss = 0.66895921\n",
            "Iteration 14, loss = 0.66873645\n",
            "Iteration 15, loss = 0.66861009\n",
            "Iteration 16, loss = 0.66857136\n",
            "Iteration 17, loss = 0.66861519\n",
            "Iteration 18, loss = 0.66855099\n",
            "Iteration 19, loss = 0.66919923\n",
            "Iteration 20, loss = 0.66854094\n",
            "Iteration 21, loss = 0.66877469\n",
            "Iteration 22, loss = 0.66868611\n",
            "Iteration 23, loss = 0.66865865\n",
            "Iteration 24, loss = 0.66878312\n",
            "Iteration 25, loss = 0.66856989\n",
            "Iteration 26, loss = 0.66840458\n",
            "Iteration 27, loss = 0.66837823\n",
            "Iteration 28, loss = 0.66845256\n",
            "Iteration 29, loss = 0.66826274\n",
            "Iteration 30, loss = 0.66833965\n",
            "Iteration 31, loss = 0.66828216\n",
            "Iteration 32, loss = 0.66876083\n",
            "Iteration 33, loss = 0.66905500\n",
            "Iteration 34, loss = 0.66783403\n",
            "Iteration 35, loss = 0.66874862\n",
            "Iteration 36, loss = 0.66858778\n",
            "Iteration 37, loss = 0.66869545\n",
            "Iteration 38, loss = 0.66801023\n",
            "Iteration 39, loss = 0.66849587\n",
            "Iteration 40, loss = 0.66814368\n",
            "Iteration 41, loss = 0.66830180\n",
            "Iteration 42, loss = 0.66846274\n",
            "Iteration 43, loss = 0.66803753\n",
            "Iteration 44, loss = 0.66869187\n",
            "Iteration 45, loss = 0.66812162\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6100936100936101+-0.0004984716611280361\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Iteration 1, loss = 0.75644061\n",
            "Iteration 2, loss = 0.68417042\n",
            "Iteration 3, loss = 0.67030280\n",
            "Iteration 4, loss = 0.67058586\n",
            "Iteration 5, loss = 0.67040006\n",
            "Iteration 6, loss = 0.66972404\n",
            "Iteration 7, loss = 0.66857686\n",
            "Iteration 8, loss = 0.66881124\n",
            "Iteration 9, loss = 0.66844774\n",
            "Iteration 10, loss = 0.66905167\n",
            "Iteration 11, loss = 0.66937185\n",
            "Iteration 12, loss = 0.66756807\n",
            "Iteration 13, loss = 0.66924734\n",
            "Iteration 14, loss = 0.66910320\n",
            "Iteration 15, loss = 0.66869187\n",
            "Iteration 16, loss = 0.66771415\n",
            "Iteration 17, loss = 0.66780955\n",
            "Iteration 18, loss = 0.66790653\n",
            "Iteration 19, loss = 0.66794572\n",
            "Iteration 20, loss = 0.66790114\n",
            "Iteration 21, loss = 0.66812048\n",
            "Iteration 22, loss = 0.66770758\n",
            "Iteration 23, loss = 0.66864502\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.79737907\n",
            "Iteration 2, loss = 0.68792706\n",
            "Iteration 3, loss = 0.67647443\n",
            "Iteration 4, loss = 0.67194731\n",
            "Iteration 5, loss = 0.67028906\n",
            "Iteration 6, loss = 0.66925844\n",
            "Iteration 7, loss = 0.66882694\n",
            "Iteration 8, loss = 0.66905866\n",
            "Iteration 9, loss = 0.66818588\n",
            "Iteration 10, loss = 0.66811669\n",
            "Iteration 11, loss = 0.66834260\n",
            "Iteration 12, loss = 0.66907700\n",
            "Iteration 13, loss = 0.66761276\n",
            "Iteration 14, loss = 0.66783221\n",
            "Iteration 15, loss = 0.66805387\n",
            "Iteration 16, loss = 0.66827653\n",
            "Iteration 17, loss = 0.66759568\n",
            "Iteration 18, loss = 0.66752866\n",
            "Iteration 19, loss = 0.66774309\n",
            "Iteration 20, loss = 0.66882943\n",
            "Iteration 21, loss = 0.66757938\n",
            "Iteration 22, loss = 0.66776181\n",
            "Iteration 23, loss = 0.66775759\n",
            "Iteration 24, loss = 0.66808691\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.74365464\n",
            "Iteration 2, loss = 0.68391261\n",
            "Iteration 3, loss = 0.67049867\n",
            "Iteration 4, loss = 0.66839761\n",
            "Iteration 5, loss = 0.66753008\n",
            "Iteration 6, loss = 0.66694819\n",
            "Iteration 7, loss = 0.66696923\n",
            "Iteration 8, loss = 0.66676280\n",
            "Iteration 9, loss = 0.66720614\n",
            "Iteration 10, loss = 0.66596800\n",
            "Iteration 11, loss = 0.66656191\n",
            "Iteration 12, loss = 0.66614615\n",
            "Iteration 13, loss = 0.66640141\n",
            "Iteration 14, loss = 0.66736957\n",
            "Iteration 15, loss = 0.66638380\n",
            "Iteration 16, loss = 0.66597735\n",
            "Iteration 17, loss = 0.66637107\n",
            "Iteration 18, loss = 0.66624179\n",
            "Iteration 19, loss = 0.66611549\n",
            "Iteration 20, loss = 0.66742615\n",
            "Iteration 21, loss = 0.66701811\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71595251\n",
            "Iteration 2, loss = 0.68047467\n",
            "Iteration 3, loss = 0.67390313\n",
            "Iteration 4, loss = 0.67064259\n",
            "Iteration 5, loss = 0.67007316\n",
            "Iteration 6, loss = 0.66946533\n",
            "Iteration 7, loss = 0.66895597\n",
            "Iteration 8, loss = 0.66899583\n",
            "Iteration 9, loss = 0.66833565\n",
            "Iteration 10, loss = 0.66833023\n",
            "Iteration 11, loss = 0.66859835\n",
            "Iteration 12, loss = 0.66763028\n",
            "Iteration 13, loss = 0.66822773\n",
            "Iteration 14, loss = 0.66758470\n",
            "Iteration 15, loss = 0.66768255\n",
            "Iteration 16, loss = 0.66762629\n",
            "Iteration 17, loss = 0.66751250\n",
            "Iteration 18, loss = 0.66719209\n",
            "Iteration 19, loss = 0.66738161\n",
            "Iteration 20, loss = 0.66725027\n",
            "Iteration 21, loss = 0.66695415\n",
            "Iteration 22, loss = 0.66722042\n",
            "Iteration 23, loss = 0.66684286\n",
            "Iteration 24, loss = 0.66724003\n",
            "Iteration 25, loss = 0.66746391\n",
            "Iteration 26, loss = 0.66752198\n",
            "Iteration 27, loss = 0.66685652\n",
            "Iteration 28, loss = 0.66676397\n",
            "Iteration 29, loss = 0.66696201\n",
            "Iteration 30, loss = 0.66719741\n",
            "Iteration 31, loss = 0.66715193\n",
            "Iteration 32, loss = 0.66716107\n",
            "Iteration 33, loss = 0.66673616\n",
            "Iteration 34, loss = 0.66678026\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6129426129426129+-0.0038180022464985126\n",
            "Eficiencia durante la validaciÃ³n = 0.6111111111111112+-0.002517158501598115\n",
            "f1-score  0.46591503482005614  +-  0.007065254843499386\n",
            "Iteration 1, loss = 0.75093105\n",
            "Iteration 2, loss = 0.67810956\n",
            "Iteration 3, loss = 0.67288884\n",
            "Iteration 4, loss = 0.66938438\n",
            "Iteration 5, loss = 0.67209211\n",
            "Iteration 6, loss = 0.66987600\n",
            "Iteration 7, loss = 0.67345449\n",
            "Iteration 8, loss = 0.67019819\n",
            "Iteration 9, loss = 0.67025051\n",
            "Iteration 10, loss = 0.67182165\n",
            "Iteration 11, loss = 0.67033414\n",
            "Iteration 12, loss = 0.67121412\n",
            "Iteration 13, loss = 0.67046682\n",
            "Iteration 14, loss = 0.67144774\n",
            "Iteration 15, loss = 0.66876261\n",
            "Iteration 16, loss = 0.67028669\n",
            "Iteration 17, loss = 0.67060645\n",
            "Iteration 18, loss = 0.67087821\n",
            "Iteration 19, loss = 0.66992242\n",
            "Iteration 20, loss = 0.66962053\n",
            "Iteration 21, loss = 0.67020077\n",
            "Iteration 22, loss = 0.66876239\n",
            "Iteration 23, loss = 0.66853637\n",
            "Iteration 24, loss = 0.66908597\n",
            "Iteration 25, loss = 0.66891691\n",
            "Iteration 26, loss = 0.66829151\n",
            "Iteration 27, loss = 0.66863980\n",
            "Iteration 28, loss = 0.66858523\n",
            "Iteration 29, loss = 0.66819464\n",
            "Iteration 30, loss = 0.66882914\n",
            "Iteration 31, loss = 0.66839656\n",
            "Iteration 32, loss = 0.66912378\n",
            "Iteration 33, loss = 0.66981009\n",
            "Iteration 34, loss = 0.66818629\n",
            "Iteration 35, loss = 0.66856741\n",
            "Iteration 36, loss = 0.66865378\n",
            "Iteration 37, loss = 0.66791260\n",
            "Iteration 38, loss = 0.66936094\n",
            "Iteration 39, loss = 0.67045947\n",
            "Iteration 40, loss = 0.66795385\n",
            "Iteration 41, loss = 0.66914796\n",
            "Iteration 42, loss = 0.66906111\n",
            "Iteration 43, loss = 0.66793072\n",
            "Iteration 44, loss = 0.66934720\n",
            "Iteration 45, loss = 0.66895322\n",
            "Iteration 46, loss = 0.66862117\n",
            "Iteration 47, loss = 0.66784169\n",
            "Iteration 48, loss = 0.66834878\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69771409\n",
            "Iteration 2, loss = 0.67543925\n",
            "Iteration 3, loss = 0.67106193\n",
            "Iteration 4, loss = 0.67046761\n",
            "Iteration 5, loss = 0.66953794\n",
            "Iteration 6, loss = 0.66921016\n",
            "Iteration 7, loss = 0.66982134\n",
            "Iteration 8, loss = 0.66984709\n",
            "Iteration 9, loss = 0.67025716\n",
            "Iteration 10, loss = 0.66920286\n",
            "Iteration 11, loss = 0.67039387\n",
            "Iteration 12, loss = 0.66982202\n",
            "Iteration 13, loss = 0.66858714\n",
            "Iteration 14, loss = 0.66897692\n",
            "Iteration 15, loss = 0.66832235\n",
            "Iteration 16, loss = 0.66842957\n",
            "Iteration 17, loss = 0.67066921\n",
            "Iteration 18, loss = 0.66842293\n",
            "Iteration 19, loss = 0.66895666\n",
            "Iteration 20, loss = 0.66889050\n",
            "Iteration 21, loss = 0.66991189\n",
            "Iteration 22, loss = 0.66939772\n",
            "Iteration 23, loss = 0.66943800\n",
            "Iteration 24, loss = 0.66829001\n",
            "Iteration 25, loss = 0.66895757\n",
            "Iteration 26, loss = 0.66965404\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70603868\n",
            "Iteration 2, loss = 0.67607948\n",
            "Iteration 3, loss = 0.66914794\n",
            "Iteration 4, loss = 0.66945699\n",
            "Iteration 5, loss = 0.66873624\n",
            "Iteration 6, loss = 0.66872660\n",
            "Iteration 7, loss = 0.67224927\n",
            "Iteration 8, loss = 0.67105772\n",
            "Iteration 9, loss = 0.66826668\n",
            "Iteration 10, loss = 0.66837742\n",
            "Iteration 11, loss = 0.66746563\n",
            "Iteration 12, loss = 0.66765119\n",
            "Iteration 13, loss = 0.66805304\n",
            "Iteration 14, loss = 0.67116523\n",
            "Iteration 15, loss = 0.66790362\n",
            "Iteration 16, loss = 0.66902499\n",
            "Iteration 17, loss = 0.67031615\n",
            "Iteration 18, loss = 0.66832963\n",
            "Iteration 19, loss = 0.66867135\n",
            "Iteration 20, loss = 0.66692863\n",
            "Iteration 21, loss = 0.66687221\n",
            "Iteration 22, loss = 0.66784553\n",
            "Iteration 23, loss = 0.66722776\n",
            "Iteration 24, loss = 0.66805638\n",
            "Iteration 25, loss = 0.66814607\n",
            "Iteration 26, loss = 0.66656290\n",
            "Iteration 27, loss = 0.66772015\n",
            "Iteration 28, loss = 0.66785450\n",
            "Iteration 29, loss = 0.66711517\n",
            "Iteration 30, loss = 0.66745458\n",
            "Iteration 31, loss = 0.66837268\n",
            "Iteration 32, loss = 0.67262048\n",
            "Iteration 33, loss = 0.66915135\n",
            "Iteration 34, loss = 0.66737384\n",
            "Iteration 35, loss = 0.66946266\n",
            "Iteration 36, loss = 0.66671464\n",
            "Iteration 37, loss = 0.66704962\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68634409\n",
            "Iteration 2, loss = 0.67606468\n",
            "Iteration 3, loss = 0.66942826\n",
            "Iteration 4, loss = 0.66902028\n",
            "Iteration 5, loss = 0.67150669\n",
            "Iteration 6, loss = 0.66873243\n",
            "Iteration 7, loss = 0.66882690\n",
            "Iteration 8, loss = 0.66980402\n",
            "Iteration 9, loss = 0.66946725\n",
            "Iteration 10, loss = 0.66955381\n",
            "Iteration 11, loss = 0.66809684\n",
            "Iteration 12, loss = 0.66883628\n",
            "Iteration 13, loss = 0.66921636\n",
            "Iteration 14, loss = 0.67004939\n",
            "Iteration 15, loss = 0.66912359\n",
            "Iteration 16, loss = 0.66802176\n",
            "Iteration 17, loss = 0.66920431\n",
            "Iteration 18, loss = 0.66885435\n",
            "Iteration 19, loss = 0.66883819\n",
            "Iteration 20, loss = 0.66812041\n",
            "Iteration 21, loss = 0.66975069\n",
            "Iteration 22, loss = 0.66835424\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6124338624338624+-0.004535589965002601\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4647789502319116  +-  0.005107749023790897\n",
            "Iteration 1, loss = 0.69077934\n",
            "Iteration 2, loss = 0.67115641\n",
            "Iteration 3, loss = 0.66916969\n",
            "Iteration 4, loss = 0.66835540\n",
            "Iteration 5, loss = 0.66809366\n",
            "Iteration 6, loss = 0.66884619\n",
            "Iteration 7, loss = 0.67105570\n",
            "Iteration 8, loss = 0.66892516\n",
            "Iteration 9, loss = 0.66817515\n",
            "Iteration 10, loss = 0.66844848\n",
            "Iteration 11, loss = 0.66873200\n",
            "Iteration 12, loss = 0.66924109\n",
            "Iteration 13, loss = 0.67148525\n",
            "Iteration 14, loss = 0.67411613\n",
            "Iteration 15, loss = 0.66892160\n",
            "Iteration 16, loss = 0.66866950\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70831133\n",
            "Iteration 2, loss = 0.67562652\n",
            "Iteration 3, loss = 0.67391722\n",
            "Iteration 4, loss = 0.67167668\n",
            "Iteration 5, loss = 0.67009092\n",
            "Iteration 6, loss = 0.66929112\n",
            "Iteration 7, loss = 0.66901120\n",
            "Iteration 8, loss = 0.66826728\n",
            "Iteration 9, loss = 0.66927884\n",
            "Iteration 10, loss = 0.66908023\n",
            "Iteration 11, loss = 0.66939617\n",
            "Iteration 12, loss = 0.66861633\n",
            "Iteration 13, loss = 0.66980047\n",
            "Iteration 14, loss = 0.66825935\n",
            "Iteration 15, loss = 0.67063834\n",
            "Iteration 16, loss = 0.67018517\n",
            "Iteration 17, loss = 0.66966960\n",
            "Iteration 18, loss = 0.66839646\n",
            "Iteration 19, loss = 0.66893205\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69120621\n",
            "Iteration 2, loss = 0.67273433\n",
            "Iteration 3, loss = 0.67084311\n",
            "Iteration 4, loss = 0.66918274\n",
            "Iteration 5, loss = 0.67006744\n",
            "Iteration 6, loss = 0.67065560\n",
            "Iteration 7, loss = 0.66843141\n",
            "Iteration 8, loss = 0.66767738\n",
            "Iteration 9, loss = 0.66731849\n",
            "Iteration 10, loss = 0.66730140\n",
            "Iteration 11, loss = 0.66846668\n",
            "Iteration 12, loss = 0.67026883\n",
            "Iteration 13, loss = 0.66725818\n",
            "Iteration 14, loss = 0.66754831\n",
            "Iteration 15, loss = 0.66760232\n",
            "Iteration 16, loss = 0.66937199\n",
            "Iteration 17, loss = 0.66701462\n",
            "Iteration 18, loss = 0.66892338\n",
            "Iteration 19, loss = 0.66709242\n",
            "Iteration 20, loss = 0.66696712\n",
            "Iteration 21, loss = 0.66810211\n",
            "Iteration 22, loss = 0.66918655\n",
            "Iteration 23, loss = 0.66782401\n",
            "Iteration 24, loss = 0.66688273\n",
            "Iteration 25, loss = 0.66606153\n",
            "Iteration 26, loss = 0.66715957\n",
            "Iteration 27, loss = 0.66851641\n",
            "Iteration 28, loss = 0.66871100\n",
            "Iteration 29, loss = 0.66691660\n",
            "Iteration 30, loss = 0.66638045\n",
            "Iteration 31, loss = 0.66689894\n",
            "Iteration 32, loss = 0.66641815\n",
            "Iteration 33, loss = 0.66721384\n",
            "Iteration 34, loss = 0.66690084\n",
            "Iteration 35, loss = 0.66646048\n",
            "Iteration 36, loss = 0.66774085\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.85366206\n",
            "Iteration 2, loss = 0.68193373\n",
            "Iteration 3, loss = 0.67342883\n",
            "Iteration 4, loss = 0.66923975\n",
            "Iteration 5, loss = 0.67062957\n",
            "Iteration 6, loss = 0.66949245\n",
            "Iteration 7, loss = 0.66962748\n",
            "Iteration 8, loss = 0.66955889\n",
            "Iteration 9, loss = 0.66919208\n",
            "Iteration 10, loss = 0.66916874\n",
            "Iteration 11, loss = 0.66799580\n",
            "Iteration 12, loss = 0.66785976\n",
            "Iteration 13, loss = 0.66918348\n",
            "Iteration 14, loss = 0.66925614\n",
            "Iteration 15, loss = 0.66866084\n",
            "Iteration 16, loss = 0.66787554\n",
            "Iteration 17, loss = 0.66910587\n",
            "Iteration 18, loss = 0.66801156\n",
            "Iteration 19, loss = 0.66937879\n",
            "Iteration 20, loss = 0.66789404\n",
            "Iteration 21, loss = 0.66935559\n",
            "Iteration 22, loss = 0.66820141\n",
            "Iteration 23, loss = 0.66822720\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Eficiencia durante el entrenamiento = 0.6126373626373627+-0.002362268369198978\n",
            "Eficiencia durante la validaciÃ³n = 0.6105006105006106+-0.0014954149833841537\n",
            "f1-score  0.46458393251249686  +-  0.0032595080248605758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "tfsMaMvK0XAZ",
        "outputId": "359400c9-2cba-4d67-dfad-6c0f2ac606c8"
      },
      "source": [
        "tablaRedesNeuronales.set_index(['N. de capas ocultas','Neuronas por capa'], inplace=True)\n",
        "qgrid_widget = qgrid.show_grid(tablaRedesNeuronales, show_toolbar=False)\n",
        "qgrid_widget\n",
        "qgrid_widget.get_changed_df()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Eficiencia en validacion</th>\n",
              "      <th>Intervalo de confianza de eficiencia</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>Intervalo de confianza de f1-score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>N. de capas ocultas</th>\n",
              "      <th>Neuronas por capa</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
              "      <th>2</th>\n",
              "      <td>0.6111111111111112</td>\n",
              "      <td>0.002024801459313474</td>\n",
              "      <td>0.4648758256351011</td>\n",
              "      <td>0.004030014347053157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.6117216117216118</td>\n",
              "      <td>0.0025901347296210966</td>\n",
              "      <td>0.46679362571414984</td>\n",
              "      <td>0.0055689576547940794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.6117216117216118</td>\n",
              "      <td>0.0025901347296210966</td>\n",
              "      <td>0.46679362571414984</td>\n",
              "      <td>0.0055689576547940794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.6111111111111112</td>\n",
              "      <td>0.002024801459313474</td>\n",
              "      <td>0.4664560257508187</td>\n",
              "      <td>0.00660342592653192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.6114163614163615</td>\n",
              "      <td>0.002498581432195536</td>\n",
              "      <td>0.4666114218322488</td>\n",
              "      <td>0.00686333242118209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
              "      <th>2</th>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.4632042271708273</td>\n",
              "      <td>0.0014649035750146594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.6108058608058609</td>\n",
              "      <td>0.002001660111203332</td>\n",
              "      <td>0.46417943862290845</td>\n",
              "      <td>0.0040787518209848455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.6105006105006106</td>\n",
              "      <td>0.0014954149833841537</td>\n",
              "      <td>0.4661680657254196</td>\n",
              "      <td>0.00514995462331914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.6120268620268621</td>\n",
              "      <td>0.0027809626309964735</td>\n",
              "      <td>0.46800597327908106</td>\n",
              "      <td>0.006723795777252328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.6117216117216118</td>\n",
              "      <td>0.002590134729621096</td>\n",
              "      <td>0.47040674334138866</td>\n",
              "      <td>0.009376664185810379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
              "      <th>2</th>\n",
              "      <td>0.6101953601953602</td>\n",
              "      <td>0.0010124007296567622</td>\n",
              "      <td>0.4633426727868033</td>\n",
              "      <td>0.0026547837987329764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.6105006105006106</td>\n",
              "      <td>0.0014954149833841537</td>\n",
              "      <td>0.46349033304069376</td>\n",
              "      <td>0.002904242941163179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.6105006105006104</td>\n",
              "      <td>0.002114836150877791</td>\n",
              "      <td>0.4640356316969368</td>\n",
              "      <td>0.004132832467887253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.6117216117216118</td>\n",
              "      <td>0.0025901347296210966</td>\n",
              "      <td>0.46732232407131274</td>\n",
              "      <td>0.006274752834710863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.6105006105006106</td>\n",
              "      <td>0.0014954149833841537</td>\n",
              "      <td>0.4656628062573869</td>\n",
              "      <td>0.004236176401888313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">6</th>\n",
              "      <th>2</th>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.6111111111111112</td>\n",
              "      <td>0.002517158501598115</td>\n",
              "      <td>0.46591503482005614</td>\n",
              "      <td>0.007065254843499386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.4647789502319116</td>\n",
              "      <td>0.005107749023790897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.6105006105006106</td>\n",
              "      <td>0.0014954149833841537</td>\n",
              "      <td>0.46458393251249686</td>\n",
              "      <td>0.0032595080248605758</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      Eficiencia en validacion  ... Intervalo de confianza de f1-score\n",
              "N. de capas ocultas Neuronas por capa                           ...                                   \n",
              "1                   2                       0.6111111111111112  ...               0.004030014347053157\n",
              "                    4                       0.6117216117216118  ...              0.0055689576547940794\n",
              "                    10                      0.6117216117216118  ...              0.0055689576547940794\n",
              "                    28                      0.6111111111111112  ...                0.00660342592653192\n",
              "                    32                      0.6114163614163615  ...                0.00686333242118209\n",
              "2                   2                       0.6098901098901099  ...              0.0014649035750146594\n",
              "                    4                       0.6108058608058609  ...              0.0040787518209848455\n",
              "                    10                      0.6105006105006106  ...                0.00514995462331914\n",
              "                    28                      0.6120268620268621  ...               0.006723795777252328\n",
              "                    32                      0.6117216117216118  ...               0.009376664185810379\n",
              "5                   2                       0.6101953601953602  ...              0.0026547837987329764\n",
              "                    4                       0.6105006105006106  ...               0.002904242941163179\n",
              "                    10                      0.6105006105006104  ...               0.004132832467887253\n",
              "                    28                      0.6117216117216118  ...               0.006274752834710863\n",
              "                    32                      0.6105006105006106  ...               0.004236176401888313\n",
              "6                   2                       0.6098901098901099  ...              0.0007498897315181274\n",
              "                    4                       0.6098901098901099  ...              0.0007498897315181274\n",
              "                    10                      0.6111111111111112  ...               0.007065254843499386\n",
              "                    28                      0.6098901098901099  ...               0.005107749023790897\n",
              "                    32                      0.6105006105006106  ...              0.0032595080248605758\n",
              "\n",
              "[20 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuTEtxRY1MLB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbto04E812Nd"
      },
      "source": [
        "# 2. MÃ¡quinas de soporte vectorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl6LYeIW3f7E"
      },
      "source": [
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "\n",
        "#Se modifica cada etiqueta para que corresponda a un numero\n",
        "\n",
        "dataSet=[]\n",
        "label = np.unique(Y)\n",
        "encoder=preprocessing.LabelEncoder()\n",
        "encoder.fit(label)\n",
        "data=encoder.transform(Y)\n",
        "dataSet.append(data)\n",
        "dataFrameY=pd.DataFrame(dataSet).values\n",
        "dataFrameY=dataFrameY.T\n",
        "dataFrameY = dataFrameY.astype(np.float)\n",
        "dataFrameY=dataFrameY.ravel() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv9_N-Aq19vw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b1HBftx1-rY",
        "outputId": "e9bd96a1-7edc-44a5-beb4-fa00fc694213"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pandas as pd\n",
        "import qgrid\n",
        "\n",
        "tablaSoporteVectorial = pd.DataFrame({\n",
        "    'Kernel' : pd.Series(['lineal','lineal','lineal','lineal','lineal','lineal','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf']),\n",
        "    'C' : pd.Series([0.001,0.01,0.1,1,10,100,0.001,0.001,0.001,0.01,0.01,0.01,0.1,0.1,0.1,1,1,1,10,10,10,100,100,100]),\n",
        "    'gamma' : pd.Series([0,0,0,0,0,0,0.01,0.1,1,0.01,0.1,1,0.01,0.1,1,0.01,0.1,1,0.01,0.1,1,0.01,0.1,1])})\n",
        "\n",
        "k = ['linear','linear','linear','linear','linear','linear','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf']\n",
        "c = [0.001,0.01,0.1,1,10,100,0.001,0.001,0.001,0.01,0.01,0.01,0.1,0.1,0.1,1,1,1,10,10,10,100,100,100]\n",
        "g =  [0,0,0,0,0,0,0.01,0.1,1,0.01,0.1,1,0.01,0.1,1,0.01,0.1,1,0.01,0.1,1,0.01,0.1,1]\n",
        "\n",
        "\n",
        "for i in range (24):\n",
        "\n",
        "  #Validamos el modelo\n",
        "  Folds = 4\n",
        "  random.seed(24524)\n",
        "  EficienciaTrain = np.zeros(Folds)\n",
        "  EficienciaVal = np.zeros(Folds)\n",
        "  skf = StratifiedKFold(n_splits=Folds)\n",
        "  f1 = np.zeros(Folds)\n",
        "  j = 0\n",
        "  porcentaje = np.zeros(4)\n",
        "\n",
        "  for train, test in skf.split(X_escalado, dataFrameY):\n",
        "      Xtrain = X_escalado[train,:]\n",
        "      Ytrain = dataFrameY[train]\n",
        "      Xtest = X_escalado[test,:]\n",
        "      Ytest = dataFrameY[test]\n",
        "      \n",
        "      #Normalizamos los datos\n",
        "      scaler = preprocessing.StandardScaler().fit(Xtrain)\n",
        "      Xtrain = scaler.transform(Xtrain)\n",
        "      Xtest = scaler.transform(Xtest)\n",
        "      \n",
        "      if( k[i] == 'linear'):\n",
        "        #Haga el llamado a la funciÃ³n para crear y entrenar el modelo usando los datos de entrenamiento\n",
        "        modelo = SVC(kernel= k[i], C=c[i], decision_function_shape='ovr')#Si es Lineal\n",
        "      else:\n",
        "        #Haga el llamado a la funciÃ³n para crear y entrenar el modelo usando los datos de entrenamiento\n",
        "        modelo = SVC(kernel= k[i], C=c[i], gamma=g[i],decision_function_shape='ovr') #Si es rbf\n",
        "      \n",
        "      modelo = modelo.fit(Xtrain,Ytrain)\n",
        "      #ValidaciÃ³n\n",
        "      Ytrain_pred = modelo.predict(Xtrain)\n",
        "      Yest = modelo.predict(Xtest)\n",
        "\n",
        "      \n",
        "      #Evaluamos las predicciones del modelo con los datos de test\n",
        "      EficienciaTrain[j] = metrics.accuracy_score(Ytrain, Ytrain_pred)\n",
        "      EficienciaVal[j] = metrics.accuracy_score(Ytest, Yest)\n",
        "      f1[j]=metrics.f1_score(Ytest, Yest,average='weighted')\n",
        "      porcentaje[j] =  len(modelo.support_vectors_) / len(Xtrain)\n",
        "      \n",
        "      j += 1\n",
        "          \n",
        "  print('Eficiencia durante el entrenamiento = ' + str(np.mean(EficienciaTrain)) + '+-' + str(np.std(EficienciaTrain)))\n",
        "  print('Eficiencia durante la validaciÃ³n = ' + str(np.mean(EficienciaVal)) + '+-' + str(np.std(EficienciaVal)))\n",
        "  print(\"f1-score \",str(np.mean(f1)),\" +- \",np.std(f1))\n",
        "  tablaSoporteVectorial=completarTablaSoporteVectorial(i,tablaSoporteVectorial,np.mean(EficienciaVal),np.std(EficienciaVal),np.mean(porcentaje),np.mean(f1),np.std(f1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6292226292226292+-0.013660306428032887\n",
            "Eficiencia durante la validaciÃ³n = 0.6114163614163615+-0.0021799232077359594\n",
            "f1-score  0.47228210056958236  +-  0.008337354614942357\n",
            "Eficiencia durante el entrenamiento = 0.6098901098901098+-0.00020350020350018427\n",
            "Eficiencia durante la validaciÃ³n = 0.6098901098901099+-0.0006105006105006638\n",
            "f1-score  0.4621012175471858  +-  0.0007498897315181274\n",
            "Eficiencia durante el entrenamiento = 0.6277981277981278+-0.010168903336303613\n",
            "Eficiencia durante la validaciÃ³n = 0.6138583638583639+-0.007242863563922286\n",
            "f1-score  0.4803551186378619  +-  0.006477205883602256\n",
            "Eficiencia durante el entrenamiento = 0.7311762311762311+-0.014969373163697989\n",
            "Eficiencia durante la validaciÃ³n = 0.6184371184371185+-0.04322499531577269\n",
            "f1-score  0.5705224470559672  +-  0.03460752551821376\n",
            "Eficiencia durante el entrenamiento = 0.9818884818884819+-0.002085256566129375\n",
            "Eficiencia durante la validaciÃ³n = 0.5946275946275946+-0.007818222512127992\n",
            "f1-score  0.48642601590641565  +-  0.008671667019266572\n",
            "Eficiencia durante el entrenamiento = 0.6786731786731788+-0.010560464587162477\n",
            "Eficiencia durante la validaciÃ³n = 0.6205738705738706+-0.03367590208619703\n",
            "f1-score  0.5486536256578253  +-  0.026306903273775745\n",
            "Eficiencia durante el entrenamiento = 0.8263125763125763+-0.008172376628797304\n",
            "Eficiencia durante la validaciÃ³n = 0.605006105006105+-0.036898689221639976\n",
            "f1-score  0.5859246133455577  +-  0.035034982603876705\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.572039072039072+-0.015931609707814258\n",
            "f1-score  0.5154347216635147  +-  0.02186470140964175\n",
            "Eficiencia durante el entrenamiento = 0.7033984533984534+-0.012591960502728749\n",
            "Eficiencia durante la validaciÃ³n = 0.6156898656898657+-0.0466094326779865\n",
            "f1-score  0.5657180418253246  +-  0.03986029965733967\n",
            "Eficiencia durante el entrenamiento = 0.9121896621896622+-0.003050806746146723\n",
            "Eficiencia durante la validaciÃ³n = 0.5735653235653235+-0.03784488311061909\n",
            "f1-score  0.5653248531044482  +-  0.03677733298521877\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.572039072039072+-0.015931609707814258\n",
            "f1-score  0.5154347216635147  +-  0.02186470140964175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "2oHn6qJBqU0v",
        "outputId": "4fa00a23-e2e1-4d28-dc50-aaddb15b3002"
      },
      "source": [
        "tablaSoporteVectorial"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel</th>\n",
              "      <th>C</th>\n",
              "      <th>gamma</th>\n",
              "      <th>Eficiencia en validacion</th>\n",
              "      <th>Intervalo de confianza de eficiencia</th>\n",
              "      <th>% de Vectores de Soporte</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>Intervalo de confianza de f1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lineal</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.7818477818477818</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lineal</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.7831705331705332</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lineal</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.78998778998779</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lineal</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.8421855921855922</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lineal</td>\n",
              "      <td>10.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.8810541310541311</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>lineal</td>\n",
              "      <td>100.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.8569393569393569</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>rbf</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.7802197802197803</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>rbf</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.7842897842897842</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>rbf</td>\n",
              "      <td>0.001</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.7894790394790395</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>rbf</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.7888685388685389</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>rbf</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.8000610500610501</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>rbf</td>\n",
              "      <td>0.010</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.9833129833129833</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>rbf</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.7900895400895401</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>rbf</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.6114163614163615</td>\n",
              "      <td>0.0021799232077359594</td>\n",
              "      <td>0.8005698005698005</td>\n",
              "      <td>0.47228210056958236</td>\n",
              "      <td>0.008337354614942357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>rbf</td>\n",
              "      <td>0.100</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.6098901098901099</td>\n",
              "      <td>0.0006105006105006638</td>\n",
              "      <td>0.9876882376882377</td>\n",
              "      <td>0.4621012175471858</td>\n",
              "      <td>0.0007498897315181274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>rbf</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.6138583638583639</td>\n",
              "      <td>0.007242863563922286</td>\n",
              "      <td>0.7849002849002849</td>\n",
              "      <td>0.4803551186378619</td>\n",
              "      <td>0.006477205883602256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>rbf</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.6184371184371185</td>\n",
              "      <td>0.04322499531577269</td>\n",
              "      <td>0.7400284900284899</td>\n",
              "      <td>0.5705224470559672</td>\n",
              "      <td>0.03460752551821376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>rbf</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5946275946275946</td>\n",
              "      <td>0.007818222512127992</td>\n",
              "      <td>0.9878917378917379</td>\n",
              "      <td>0.48642601590641565</td>\n",
              "      <td>0.008671667019266572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>rbf</td>\n",
              "      <td>10.000</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.6205738705738706</td>\n",
              "      <td>0.03367590208619703</td>\n",
              "      <td>0.7302604802604802</td>\n",
              "      <td>0.5486536256578253</td>\n",
              "      <td>0.026306903273775745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>rbf</td>\n",
              "      <td>10.000</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.605006105006105</td>\n",
              "      <td>0.036898689221639976</td>\n",
              "      <td>0.6903744403744403</td>\n",
              "      <td>0.5859246133455577</td>\n",
              "      <td>0.035034982603876705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>rbf</td>\n",
              "      <td>10.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.572039072039072</td>\n",
              "      <td>0.015931609707814258</td>\n",
              "      <td>0.9863654863654864</td>\n",
              "      <td>0.5154347216635147</td>\n",
              "      <td>0.02186470140964175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>rbf</td>\n",
              "      <td>100.000</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.6156898656898657</td>\n",
              "      <td>0.0466094326779865</td>\n",
              "      <td>0.6934269434269434</td>\n",
              "      <td>0.5657180418253246</td>\n",
              "      <td>0.03986029965733967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>rbf</td>\n",
              "      <td>100.000</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.5735653235653235</td>\n",
              "      <td>0.03784488311061909</td>\n",
              "      <td>0.6283068783068784</td>\n",
              "      <td>0.5653248531044482</td>\n",
              "      <td>0.03677733298521877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>rbf</td>\n",
              "      <td>100.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.572039072039072</td>\n",
              "      <td>0.015931609707814258</td>\n",
              "      <td>0.9863654863654864</td>\n",
              "      <td>0.5154347216635147</td>\n",
              "      <td>0.02186470140964175</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Kernel        C  ...             f1-score Intervalo de confianza de f1-score\n",
              "0   lineal    0.001  ...   0.4621012175471858              0.0007498897315181274\n",
              "1   lineal    0.010  ...   0.4621012175471858              0.0007498897315181274\n",
              "2   lineal    0.100  ...   0.4621012175471858              0.0007498897315181274\n",
              "3   lineal    1.000  ...   0.4621012175471858              0.0007498897315181274\n",
              "4   lineal   10.000  ...   0.4621012175471858              0.0007498897315181274\n",
              "5   lineal  100.000  ...   0.4621012175471858              0.0007498897315181274\n",
              "6      rbf    0.001  ...   0.4621012175471858              0.0007498897315181274\n",
              "7      rbf    0.001  ...   0.4621012175471858              0.0007498897315181274\n",
              "8      rbf    0.001  ...   0.4621012175471858              0.0007498897315181274\n",
              "9      rbf    0.010  ...   0.4621012175471858              0.0007498897315181274\n",
              "10     rbf    0.010  ...   0.4621012175471858              0.0007498897315181274\n",
              "11     rbf    0.010  ...   0.4621012175471858              0.0007498897315181274\n",
              "12     rbf    0.100  ...   0.4621012175471858              0.0007498897315181274\n",
              "13     rbf    0.100  ...  0.47228210056958236               0.008337354614942357\n",
              "14     rbf    0.100  ...   0.4621012175471858              0.0007498897315181274\n",
              "15     rbf    1.000  ...   0.4803551186378619               0.006477205883602256\n",
              "16     rbf    1.000  ...   0.5705224470559672                0.03460752551821376\n",
              "17     rbf    1.000  ...  0.48642601590641565               0.008671667019266572\n",
              "18     rbf   10.000  ...   0.5486536256578253               0.026306903273775745\n",
              "19     rbf   10.000  ...   0.5859246133455577               0.035034982603876705\n",
              "20     rbf   10.000  ...   0.5154347216635147                0.02186470140964175\n",
              "21     rbf  100.000  ...   0.5657180418253246                0.03986029965733967\n",
              "22     rbf  100.000  ...   0.5653248531044482                0.03677733298521877\n",
              "23     rbf  100.000  ...   0.5154347216635147                0.02186470140964175\n",
              "\n",
              "[24 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll4n0dyzmwZG"
      },
      "source": [
        "# 3. Ventana de parzen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSysPhrunF6w"
      },
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "class KDEClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"Bayesian generative classification based on KDE\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    bandwidth : float\n",
        "        the kernel bandwidth within each class\n",
        "    kernel : str\n",
        "        the kernel name, passed to KernelDensity\n",
        "    \"\"\"\n",
        "    def __init__(self, bandwidth=1.0, kernel='gaussian'):\n",
        "        self.bandwidth = bandwidth\n",
        "        self.kernel = kernel\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.sort(np.unique(y))\n",
        "        training_sets = [X[y == yi] for yi in self.classes_]\n",
        "        self.models_ = [KernelDensity(bandwidth=self.bandwidth,\n",
        "                                      kernel=self.kernel).fit(Xi)\n",
        "                        for Xi in training_sets]\n",
        "        self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])\n",
        "                           for Xi in training_sets]\n",
        "        return self\n",
        "        \n",
        "    def predict_proba(self, X):\n",
        "        logprobs = np.array([model.score_samples(X)\n",
        "                             for model in self.models_]).T\n",
        "        result = np.exp(logprobs + self.logpriors_)\n",
        "        return result / result.sum(1, keepdims=True)\n",
        "        \n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), 1)]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRIa3IMaoElO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0be655-dcc7-4640-c819-41190e403c03"
      },
      "source": [
        "tablaVentanaParzen = pd.DataFrame({\n",
        "    'bandwidth' : pd.Series([0.2,0.5,0.7,1, 2, 5, 10])})\n",
        "\n",
        "\n",
        "bandwidth = [0.2,0.5,0.7,1, 2, 5, 10]\n",
        "for i in range(7):\n",
        "  clf = KDEClassifier(bandwidth[i])\n",
        "  Folds = 4\n",
        "  EficienciaTrain = np.zeros(Folds)\n",
        "  EficienciaVal = np.zeros(Folds)\n",
        "  f1 = np.zeros(Folds)\n",
        "  skf = StratifiedKFold(n_splits=Folds)\n",
        "  j = 0\n",
        "  for train, test in skf.split(X_escalado, integerY):\n",
        "      Xtrain = X_escalado[train,:]\n",
        "      Ytrain = integerY[train]\n",
        "      Xtest = X_escalado[test,:]\n",
        "      Ytest = integerY[test]\n",
        "      Ytest=Ytest.reshape(len(Ytest))\n",
        "      Ytrain=Ytrain.reshape(len(Ytrain))\n",
        "      clf.fit(Xtrain, Ytrain)\n",
        "      Ytrain_pred = clf.predict(Xtrain)\n",
        "      Yest = clf.predict(Xtest)\n",
        "      #Evaluamos las predicciones del modelo con los datos de test\n",
        "      EficienciaTrain[j] = metrics.accuracy_score(Ytrain, Ytrain_pred)\n",
        "      EficienciaVal[j] = metrics.accuracy_score(Ytest, Yest)\n",
        "      f1[j]=metrics.f1_score(Ytest, Yest,average='weighted')   \n",
        "  print('Eficiencia durante el entrenamiento = ' + str(np.mean(EficienciaTrain)) + '+-' + str(np.std(EficienciaTrain)))\n",
        "  print('Eficiencia durante la validaciÃ³n = ' + str(np.mean(EficienciaVal)) + '+-' + str(np.std(EficienciaVal)))\n",
        "  print(\"f1-score \",str(np.mean(f1)),\" +- \",np.std(f1))\n",
        "  tablaVentanaParzen = completarTablaVentanaParzen(i,tablaVentanaParzen,np.mean(EficienciaVal),np.std(EficienciaVal),np.mean(f1),np.std(f1))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eficiencia durante el entrenamiento = 0.25+-0.4330127018922193\n",
            "Eficiencia durante la validaciÃ³n = 0.13766788766788768+-0.23844777601146633\n",
            "f1-score  0.1375372968401485  +-  0.23822158606281962\n",
            "Eficiencia durante el entrenamiento = 0.2498982498982499+-0.4328364655463129\n",
            "Eficiencia durante la validaciÃ³n = 0.15567765567765568+-0.26964160923691316\n",
            "f1-score  0.15139190182873427  +-  0.2622184658218474\n",
            "Eficiencia durante el entrenamiento = 0.2271062271062271+-0.39335952406326147\n",
            "Eficiencia durante la validaciÃ³n = 0.16056166056166057+-0.27810095384042416\n",
            "f1-score  0.14650600754819118  +-  0.25375584868753653\n",
            "Eficiencia durante el entrenamiento = 0.17928367928367928+-0.31052844148721626\n",
            "Eficiencia durante la validaciÃ³n = 0.15964590964590963+-0.2765148267272658\n",
            "f1-score  0.13298671045560626  +-  0.23033973924056125\n",
            "Eficiencia durante el entrenamiento = 0.15282865282865282+-0.2647069915515317\n",
            "Eficiencia durante la validaciÃ³n = 0.15262515262515264+-0.2643545188597188\n",
            "f1-score  0.11571277681967598  +-  0.20042040853655704\n",
            "Eficiencia durante el entrenamiento = 0.15242165242165243+-0.26400204616790585\n",
            "Eficiencia durante la validaciÃ³n = 0.15262515262515264+-0.2643545188597188\n",
            "f1-score  0.11571277681967598  +-  0.20042040853655704\n",
            "Eficiencia durante el entrenamiento = 0.15242165242165243+-0.26400204616790585\n",
            "Eficiencia durante la validaciÃ³n = 0.15262515262515264+-0.2643545188597188\n",
            "f1-score  0.11571277681967598  +-  0.20042040853655704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "skfoTS0vsMxp",
        "outputId": "0ffce58b-f8d0-4036-f290-a9e14daaa347"
      },
      "source": [
        "tablaVentanaParzen"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bandwidth</th>\n",
              "      <th>Eficiencia en validacion</th>\n",
              "      <th>Intervalo de confianza de eficiencia</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>Intervalo de confianza de f1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.13766788766788768</td>\n",
              "      <td>0.23844777601146633</td>\n",
              "      <td>0.1375372968401485</td>\n",
              "      <td>0.23822158606281962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.15567765567765568</td>\n",
              "      <td>0.26964160923691316</td>\n",
              "      <td>0.15139190182873427</td>\n",
              "      <td>0.2622184658218474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.7</td>\n",
              "      <td>0.16056166056166057</td>\n",
              "      <td>0.27810095384042416</td>\n",
              "      <td>0.14650600754819118</td>\n",
              "      <td>0.25375584868753653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.15964590964590963</td>\n",
              "      <td>0.2765148267272658</td>\n",
              "      <td>0.13298671045560626</td>\n",
              "      <td>0.23033973924056125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.15262515262515264</td>\n",
              "      <td>0.2643545188597188</td>\n",
              "      <td>0.11571277681967598</td>\n",
              "      <td>0.20042040853655704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.15262515262515264</td>\n",
              "      <td>0.2643545188597188</td>\n",
              "      <td>0.11571277681967598</td>\n",
              "      <td>0.20042040853655704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.15262515262515264</td>\n",
              "      <td>0.2643545188597188</td>\n",
              "      <td>0.11571277681967598</td>\n",
              "      <td>0.20042040853655704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bandwidth  ... Intervalo de confianza de f1-score\n",
              "0        0.2  ...                0.23822158606281962\n",
              "1        0.5  ...                 0.2622184658218474\n",
              "2        0.7  ...                0.25375584868753653\n",
              "3        1.0  ...                0.23033973924056125\n",
              "4        2.0  ...                0.20042040853655704\n",
              "5        5.0  ...                0.20042040853655704\n",
              "6       10.0  ...                0.20042040853655704\n",
              "\n",
              "[7 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMNq1wtNovnU"
      },
      "source": [
        "# 4. AnÃ¡lisis discriminante cuadrÃ¡tico"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWlFppmQo9OF",
        "outputId": "c18ee102-262f-4e76-88b7-9eaf809be84b"
      },
      "source": [
        "tablaAnalisisDiscriminanteCuadratico = pd.DataFrame({\n",
        "    'regParam' : pd.Series([0.0,0.3,0.6,0.9])})\n",
        "tablaAnalisisDiscriminanteCuadratico[\"Eficiencia en validacion\"] = \"\"\n",
        "tablaAnalisisDiscriminanteCuadratico[\"Intervalo de confianza\"] = \"\"\n",
        "regParam=[0.0,0.3,0.6,0.9]\n",
        "for i in range(4):\n",
        "  clf = QuadraticDiscriminantAnalysis(reg_param=regParam[i])\n",
        "  Folds = 4\n",
        "  EficienciaTrain = np.zeros(Folds)\n",
        "  EficienciaVal = np.zeros(Folds)\n",
        "  skf = StratifiedKFold(n_splits=Folds)\n",
        "  f1 = np.zeros(Folds)\n",
        "  j = 0\n",
        "  for train, test in skf.split(X_escalado, integerY):\n",
        "      Xtrain = X_escalado[train,:]\n",
        "      Ytrain = integerY[train]\n",
        "      Xtest = X_escalado[test,:]\n",
        "      Ytest = integerY[test]\n",
        "      Ytest=Ytest.reshape(len(Ytest))\n",
        "      Ytrain=Ytrain.reshape(len(Ytrain))\n",
        "      clf.fit(Xtrain, Ytrain)\n",
        "      Ytrain_pred = clf.predict(Xtrain)\n",
        "      Yest = clf.predict(Xtest)\n",
        "      EficienciaTrain[j] = metrics.accuracy_score(Ytrain, Ytrain_pred)\n",
        "      EficienciaVal[j] = metrics.accuracy_score(Ytest, Yest)\n",
        "      f1[j]=metrics.f1_score(Ytest, Yest,average='weighted')   \n",
        "  print('Eficiencia durante el entrenamiento = ' + str(np.mean(EficienciaTrain)) + '+-' + str(np.std(EficienciaTrain)))\n",
        "  print('Eficiencia durante la validaciÃ³n = ' + str(np.mean(EficienciaVal)) + '+-' + str(np.std(EficienciaVal)))\n",
        "  print(\"f1-score \",str(np.mean(f1)),\" +- \",np.std(f1))\n",
        "  tablaAnalisisDiscriminanteCuadratico = completarTablaAnalisisDiscriminanteCuadratico(i,tablaAnalisisDiscriminanteCuadratico,np.mean(EficienciaVal),np.std(EficienciaVal),np.mean(f1),np.std(f1))\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eficiencia durante el entrenamiento = 0.17073667073667073+-0.295724588431072\n",
            "Eficiencia durante la validaciÃ³n = 0.1681929181929182+-0.2913186797834101\n",
            "f1-score  0.15945546406053548  +-  0.27618496529732056\n",
            "Eficiencia durante el entrenamiento = 0.17104192104192104+-0.2962532974687914\n",
            "Eficiencia durante la validaciÃ³n = 0.16697191697191696+-0.2892038436325323\n",
            "f1-score  0.15449234495960465  +-  0.26758859085049286\n",
            "Eficiencia durante el entrenamiento = 0.16778591778591778+-0.29061373439978416\n",
            "Eficiencia durante la validaciÃ³n = 0.16483516483516483+-0.28550288036849625\n",
            "f1-score  0.14636980302711047  +-  0.25351993553680424\n",
            "Eficiencia durante el entrenamiento = 0.15873015873015872+-0.2749286996141075\n",
            "Eficiencia durante la validaciÃ³n = 0.15567765567765568+-0.26964160923691316\n",
            "f1-score  0.12446570422516796  +-  0.21558092351783123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "kh7gBn5p3_uD",
        "outputId": "3a52c543-0df3-4a53-82f4-be430e8d68b1"
      },
      "source": [
        "tablaAnalisisDiscriminanteCuadratico"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>regParam</th>\n",
              "      <th>Eficiencia en validacion</th>\n",
              "      <th>Intervalo de confianza</th>\n",
              "      <th>Intervalo de confianza de eficiencia</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>Intervalo de confianza de f1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1681929181929182</td>\n",
              "      <td></td>\n",
              "      <td>0.2913186797834101</td>\n",
              "      <td>0.15945546406053548</td>\n",
              "      <td>0.27618496529732056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.16697191697191696</td>\n",
              "      <td></td>\n",
              "      <td>0.2892038436325323</td>\n",
              "      <td>0.15449234495960465</td>\n",
              "      <td>0.26758859085049286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.6</td>\n",
              "      <td>0.16483516483516483</td>\n",
              "      <td></td>\n",
              "      <td>0.28550288036849625</td>\n",
              "      <td>0.14636980302711047</td>\n",
              "      <td>0.25351993553680424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.9</td>\n",
              "      <td>0.15567765567765568</td>\n",
              "      <td></td>\n",
              "      <td>0.26964160923691316</td>\n",
              "      <td>0.12446570422516796</td>\n",
              "      <td>0.21558092351783123</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   regParam  ... Intervalo de confianza de f1-score\n",
              "0       0.0  ...                0.27618496529732056\n",
              "1       0.3  ...                0.26758859085049286\n",
              "2       0.6  ...                0.25351993553680424\n",
              "3       0.9  ...                0.21558092351783123\n",
              "\n",
              "[4 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPuiXm89vK47"
      },
      "source": [
        "# 5. Gradient boosting tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC-puh7pvTHo",
        "outputId": "75cd8086-1159-4d00-e2bb-3828748eee9e"
      },
      "source": [
        "tablaBoostingTree = pd.DataFrame()\n",
        "tablaBoostingTree[\"n_estimators\"] = \"\"\n",
        "tablaBoostingTree[\"learning_rate\"] = \"\"\n",
        "tablaBoostingTree[\"max_depth\"] = \"\"\n",
        "tablaBoostingTree[\"Eficiencia en validacion\"] = \"\"\n",
        "tablaBoostingTree[\"Intervalo de confianza\"] = \"\"\n",
        "\n",
        "\n",
        "n_estimators=[300,500,1000,]\n",
        "learning_rate=[0.3,0.5,0.7]\n",
        "max_depth=[1,3,5]\n",
        "pos=0\n",
        "for i in range(3):\n",
        "  for z in range(3):\n",
        "    for k in range(3):\n",
        "      clf = GradientBoostingClassifier(n_estimators=n_estimators[z], learning_rate=learning_rate[k],max_depth=max_depth[i], random_state=0)\n",
        "      Folds = 4\n",
        "      EficienciaTrain = np.zeros(Folds)\n",
        "      EficienciaVal = np.zeros(Folds)\n",
        "      skf = StratifiedKFold(n_splits=Folds)\n",
        "      f1 = np.zeros(Folds)\n",
        "      j = 0\n",
        "      for train, test in skf.split(X_escalado, integerY):\n",
        "          Xtrain = X_escalado[train,:]\n",
        "          Ytrain = integerY[train]\n",
        "          Xtest = X_escalado[test,:]\n",
        "          Ytest = integerY[test]\n",
        "          Ytest=Ytest.reshape(len(Ytest))\n",
        "          Ytrain=Ytrain.reshape(len(Ytrain))\n",
        "          clf.fit(Xtrain, Ytrain)\n",
        "          Ytrain_pred = clf.predict(Xtrain)\n",
        "          Yest = clf.predict(Xtest)\n",
        "          #Evaluamos las predicciones del modelo con los datos de test\n",
        "          EficienciaTrain[j] = metrics.accuracy_score(Ytrain, Ytrain_pred)\n",
        "          EficienciaVal[j] = metrics.accuracy_score(Ytest, Yest)\n",
        "          f1[j]=metrics.f1_score(Ytest, Yest,average='weighted') \n",
        "          j += 1  \n",
        "      print('Eficiencia durante el entrenamiento = ' + str(np.mean(EficienciaTrain)) + '+-' + str(np.std(EficienciaTrain)))\n",
        "      print('Eficiencia durante la validaciÃ³n = ' + str(np.mean(EficienciaVal)) + '+-' + str(np.std(EficienciaVal)))\n",
        "      print(\"f1-score \",str(np.mean(f1)),\" +- \",np.std(f1))\n",
        "      tablaBoostingTree=completarTablaBoostingTree(pos,tablaBoostingTree,np.mean(EficienciaVal),np.std(EficienciaVal),n_estimators[z],learning_rate[k],max_depth[i],np.mean(f1),np.std(f1))\n",
        "      pos+=1"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eficiencia durante el entrenamiento = 0.781949531949532+-0.007237143631291538\n",
            "Eficiencia durante la validaciÃ³n = 0.7448107448107448+-0.038213632123391496\n",
            "f1-score  0.7283591874359728  +-  0.03676154284602545\n",
            "Eficiencia durante el entrenamiento = 0.7969067969067969+-0.011601296551680277\n",
            "Eficiencia durante la validaciÃ³n = 0.7387057387057387+-0.03797883031745002\n",
            "f1-score  0.7259814617676325  +-  0.03718059952287511\n",
            "Eficiencia durante el entrenamiento = 0.8137973137973138+-0.006489710508385034\n",
            "Eficiencia durante la validaciÃ³n = 0.7313797313797314+-0.03439454461783461\n",
            "f1-score  0.7215598488202637  +-  0.033004275169576575\n",
            "Eficiencia durante el entrenamiento = 0.7921245421245421+-0.00962009411491537\n",
            "Eficiencia durante la validaciÃ³n = 0.7417582417582418+-0.03873192167414382\n",
            "f1-score  0.7275948041443806  +-  0.03881565756410568\n",
            "Eficiencia durante el entrenamiento = 0.8161375661375662+-0.0058015406146309885\n",
            "Eficiencia durante la validaciÃ³n = 0.7356532356532357+-0.03143337157962394\n",
            "f1-score  0.7252039478361147  +-  0.031068274509417487\n",
            "Eficiencia durante el entrenamiento = 0.8357753357753359+-0.004254079153513798\n",
            "Eficiencia durante la validaciÃ³n = 0.7255799755799757+-0.030037312499071576\n",
            "f1-score  0.7174415745985367  +-  0.029365577780117228\n",
            "Eficiencia durante el entrenamiento = 0.8195970695970695+-0.004767076424551262\n",
            "Eficiencia durante la validaciÃ³n = 0.7310744810744811+-0.031277358987499136\n",
            "f1-score  0.7209011803572587  +-  0.031120998240496956\n",
            "Eficiencia durante el entrenamiento = 0.8448310948310949+-0.0027510186883589398\n",
            "Eficiencia durante la validaciÃ³n = 0.7271062271062272+-0.026715941986619116\n",
            "f1-score  0.7191758273012587  +-  0.026538672010077726\n",
            "Eficiencia durante el entrenamiento = 0.8673178673178674+-0.0019730076749761463\n",
            "Eficiencia durante la validaciÃ³n = 0.721916971916972+-0.020565354411112912\n",
            "f1-score  0.714961119732852  +-  0.021036038934216742\n",
            "Eficiencia durante el entrenamiento = 0.9992877492877493+-0.00033746690988559013\n",
            "Eficiencia durante la validaciÃ³n = 0.7338217338217339+-0.034351171908755064\n",
            "f1-score  0.729392315169478  +-  0.0352887929822425\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7298534798534799+-0.03232051581299316\n",
            "f1-score  0.725432164308225  +-  0.03256639203701662\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7216117216117216+-0.032223792968001576\n",
            "f1-score  0.7189958597705717  +-  0.03277426253254614\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7316849816849818+-0.03155024391383191\n",
            "f1-score  0.7274542211309406  +-  0.03203080934077861\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7286324786324787+-0.03844094066354073\n",
            "f1-score  0.7240749321777366  +-  0.039122118092732944\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7203907203907204+-0.029620291747761183\n",
            "f1-score  0.7176340077209917  +-  0.030092000461010928\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7341269841269842+-0.026278133433853382\n",
            "f1-score  0.7305022061482517  +-  0.02690632977039891\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7286324786324787+-0.0373192686442504\n",
            "f1-score  0.7247042283720065  +-  0.037761695826845826\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7228327228327228+-0.03080458073636841\n",
            "f1-score  0.7194503357991257  +-  0.030841531775212504\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7295482295482295+-0.03296137969888423\n",
            "f1-score  0.7244989270748473  +-  0.03286370624041559\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7368742368742369+-0.03414985658350915\n",
            "f1-score  0.7320221290414445  +-  0.034480546253225275\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7332112332112333+-0.029259479771237495\n",
            "f1-score  0.7283086539842345  +-  0.029864502754085285\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7310744810744811+-0.02544220551167872\n",
            "f1-score  0.7256685884190002  +-  0.02566226978829143\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7387057387057387+-0.030525030525030538\n",
            "f1-score  0.7338123039726434  +-  0.03090769267156549\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7332112332112333+-0.029259479771237495\n",
            "f1-score  0.7283086539842345  +-  0.029864502754085285\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7277167277167277+-0.026917469700914028\n",
            "f1-score  0.7226690339852678  +-  0.027261604743223342\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7387057387057387+-0.030525030525030538\n",
            "f1-score  0.7338123039726434  +-  0.03090769267156549\n",
            "Eficiencia durante el entrenamiento = 1.0+-0.0\n",
            "Eficiencia durante la validaciÃ³n = 0.7332112332112333+-0.029259479771237495\n",
            "f1-score  0.7283086539842345  +-  0.029864502754085285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "rBuP2kFAVkxi",
        "outputId": "0972de31-9b93-4451-c7a8-6e48d231dd19"
      },
      "source": [
        "tablaBoostingTree"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_estimators</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>max_depth</th>\n",
              "      <th>Eficiencia en validacion</th>\n",
              "      <th>Intervalo de confianza</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>Intervalo de confianza de f1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>300</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.7448107448107448</td>\n",
              "      <td>0.038213632123391496</td>\n",
              "      <td>0.7283591874359728</td>\n",
              "      <td>0.03676154284602545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>300</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.7387057387057387</td>\n",
              "      <td>0.03797883031745002</td>\n",
              "      <td>0.7259814617676325</td>\n",
              "      <td>0.03718059952287511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>300</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.7313797313797314</td>\n",
              "      <td>0.03439454461783461</td>\n",
              "      <td>0.7215598488202637</td>\n",
              "      <td>0.033004275169576575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>500</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.7417582417582418</td>\n",
              "      <td>0.03873192167414382</td>\n",
              "      <td>0.7275948041443806</td>\n",
              "      <td>0.03881565756410568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>500</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.7356532356532357</td>\n",
              "      <td>0.03143337157962394</td>\n",
              "      <td>0.7252039478361147</td>\n",
              "      <td>0.031068274509417487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>500</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.7255799755799757</td>\n",
              "      <td>0.030037312499071576</td>\n",
              "      <td>0.7174415745985367</td>\n",
              "      <td>0.029365577780117228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1000</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.7310744810744811</td>\n",
              "      <td>0.031277358987499136</td>\n",
              "      <td>0.7209011803572587</td>\n",
              "      <td>0.031120998240496956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.7271062271062272</td>\n",
              "      <td>0.026715941986619116</td>\n",
              "      <td>0.7191758273012587</td>\n",
              "      <td>0.026538672010077726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1000</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.721916971916972</td>\n",
              "      <td>0.020565354411112912</td>\n",
              "      <td>0.714961119732852</td>\n",
              "      <td>0.021036038934216742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>300</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.7338217338217339</td>\n",
              "      <td>0.034351171908755064</td>\n",
              "      <td>0.729392315169478</td>\n",
              "      <td>0.0352887929822425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.7298534798534799</td>\n",
              "      <td>0.03232051581299316</td>\n",
              "      <td>0.725432164308225</td>\n",
              "      <td>0.03256639203701662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>300</td>\n",
              "      <td>0.7</td>\n",
              "      <td>3</td>\n",
              "      <td>0.7216117216117216</td>\n",
              "      <td>0.032223792968001576</td>\n",
              "      <td>0.7189958597705717</td>\n",
              "      <td>0.03277426253254614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>500</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.7316849816849818</td>\n",
              "      <td>0.03155024391383191</td>\n",
              "      <td>0.7274542211309406</td>\n",
              "      <td>0.03203080934077861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>500</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.7286324786324787</td>\n",
              "      <td>0.03844094066354073</td>\n",
              "      <td>0.7240749321777366</td>\n",
              "      <td>0.039122118092732944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>500</td>\n",
              "      <td>0.7</td>\n",
              "      <td>3</td>\n",
              "      <td>0.7203907203907204</td>\n",
              "      <td>0.029620291747761183</td>\n",
              "      <td>0.7176340077209917</td>\n",
              "      <td>0.030092000461010928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1000</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.7341269841269842</td>\n",
              "      <td>0.026278133433853382</td>\n",
              "      <td>0.7305022061482517</td>\n",
              "      <td>0.02690632977039891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.7286324786324787</td>\n",
              "      <td>0.0373192686442504</td>\n",
              "      <td>0.7247042283720065</td>\n",
              "      <td>0.037761695826845826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1000</td>\n",
              "      <td>0.7</td>\n",
              "      <td>3</td>\n",
              "      <td>0.7228327228327228</td>\n",
              "      <td>0.03080458073636841</td>\n",
              "      <td>0.7194503357991257</td>\n",
              "      <td>0.030841531775212504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>300</td>\n",
              "      <td>0.3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.7295482295482295</td>\n",
              "      <td>0.03296137969888423</td>\n",
              "      <td>0.7244989270748473</td>\n",
              "      <td>0.03286370624041559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>300</td>\n",
              "      <td>0.5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.7368742368742369</td>\n",
              "      <td>0.03414985658350915</td>\n",
              "      <td>0.7320221290414445</td>\n",
              "      <td>0.034480546253225275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>300</td>\n",
              "      <td>0.7</td>\n",
              "      <td>5</td>\n",
              "      <td>0.7332112332112333</td>\n",
              "      <td>0.029259479771237495</td>\n",
              "      <td>0.7283086539842345</td>\n",
              "      <td>0.029864502754085285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>500</td>\n",
              "      <td>0.3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.7310744810744811</td>\n",
              "      <td>0.02544220551167872</td>\n",
              "      <td>0.7256685884190002</td>\n",
              "      <td>0.02566226978829143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>500</td>\n",
              "      <td>0.5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.7387057387057387</td>\n",
              "      <td>0.030525030525030538</td>\n",
              "      <td>0.7338123039726434</td>\n",
              "      <td>0.03090769267156549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>500</td>\n",
              "      <td>0.7</td>\n",
              "      <td>5</td>\n",
              "      <td>0.7332112332112333</td>\n",
              "      <td>0.029259479771237495</td>\n",
              "      <td>0.7283086539842345</td>\n",
              "      <td>0.029864502754085285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1000</td>\n",
              "      <td>0.3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.7277167277167277</td>\n",
              "      <td>0.026917469700914028</td>\n",
              "      <td>0.7226690339852678</td>\n",
              "      <td>0.027261604743223342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.7387057387057387</td>\n",
              "      <td>0.030525030525030538</td>\n",
              "      <td>0.7338123039726434</td>\n",
              "      <td>0.03090769267156549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1000</td>\n",
              "      <td>0.7</td>\n",
              "      <td>5</td>\n",
              "      <td>0.7332112332112333</td>\n",
              "      <td>0.029259479771237495</td>\n",
              "      <td>0.7283086539842345</td>\n",
              "      <td>0.029864502754085285</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   n_estimators  ... Intervalo de confianza de f1-score\n",
              "0           300  ...                0.03676154284602545\n",
              "1           300  ...                0.03718059952287511\n",
              "2           300  ...               0.033004275169576575\n",
              "3           500  ...                0.03881565756410568\n",
              "4           500  ...               0.031068274509417487\n",
              "5           500  ...               0.029365577780117228\n",
              "6          1000  ...               0.031120998240496956\n",
              "7          1000  ...               0.026538672010077726\n",
              "8          1000  ...               0.021036038934216742\n",
              "9           300  ...                 0.0352887929822425\n",
              "10          300  ...                0.03256639203701662\n",
              "11          300  ...                0.03277426253254614\n",
              "12          500  ...                0.03203080934077861\n",
              "13          500  ...               0.039122118092732944\n",
              "14          500  ...               0.030092000461010928\n",
              "15         1000  ...                0.02690632977039891\n",
              "16         1000  ...               0.037761695826845826\n",
              "17         1000  ...               0.030841531775212504\n",
              "18          300  ...                0.03286370624041559\n",
              "19          300  ...               0.034480546253225275\n",
              "20          300  ...               0.029864502754085285\n",
              "21          500  ...                0.02566226978829143\n",
              "22          500  ...                0.03090769267156549\n",
              "23          500  ...               0.029864502754085285\n",
              "24         1000  ...               0.027261604743223342\n",
              "25         1000  ...                0.03090769267156549\n",
              "26         1000  ...               0.029864502754085285\n",
              "\n",
              "[27 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    }
  ]
}